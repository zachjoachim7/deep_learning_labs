{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachjoachim7/deep_learning_labs/blob/main/DL_Lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzLAruSaXXcv"
      },
      "source": [
        "<a\n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab6.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cksgAH12XRjV"
      },
      "source": [
        "# Lab 6: Sequence-to-sequence models\n",
        "\n",
        "### Description:\n",
        "For this lab, you will code up the [char-rnn model of Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). This is a recurrent neural network that is trained probabilistically on sequences of characters, and that can then be used to sample new sequences that are like the original.\n",
        "\n",
        "This lab will help you develop several new skills, as well as understand some best practices needed for building large models. In addition, we'll be able to create networks that generate neat text!\n",
        "\n",
        "### Deliverable:\n",
        "- Fill in the code for the RNN (using PyTorch's built-in GRU).\n",
        "- Fill in the training loop\n",
        "- Fill in the evaluation loop. In this loop, rather than using a validation set, you will sample text from the RNN.\n",
        "- Implement your own GRU cell.\n",
        "- Train your RNN on a new domain of text (Star Wars, political speeches, etc. - have fun!)\n",
        "\n",
        "### Grading Standards:\n",
        "- 20% Implementation the RNN\n",
        "- 20% Implementation training loop\n",
        "- 20% Implementation of evaluation loop\n",
        "- 20% Implementation of your own GRU cell\n",
        "- 20% Training of your RNN on a domain of your choice\n",
        "\n",
        "### Tips:\n",
        "- Read through all the helper functions, run them, and make sure you understand what they are doing\n",
        "- At each stage, ask yourself: What should the dimensions of this tensor be? Should its data type be float or int? (int is called `long` in PyTorch)\n",
        "- Don't apply a softmax inside the RNN if you are using an nn.CrossEntropyLoss (this module already applies a softmax to its input).\n",
        "\n",
        "### Example Output:\n",
        "An example of my final samples are shown below (more detail in the\n",
        "final section of this writeup), after 150 passes through the data.\n",
        "Please generate about 15 samples for each dataset.\n",
        "\n",
        "<code>\n",
        "And ifte thin forgision forward thene over up to a fear not your\n",
        "And freitions, which is great God. Behold these are the loss sub\n",
        "And ache with the Lord hath bloes, which was done to the holy Gr\n",
        "And appeicis arm vinimonahites strong in name, to doth piseling\n",
        "And miniquithers these words, he commanded order not; neither sa\n",
        "And min for many would happine even to the earth, to said unto m\n",
        "And mie first be traditions? Behold, you, because it was a sound\n",
        "And from tike ended the Lamanites had administered, and I say bi\n",
        "</code>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2i_QpSsWG4c"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 0: Readings, data loading, and high level training\n",
        "\n",
        "---\n",
        "\n",
        "There is a tutorial here that will help build out scaffolding code, and get an understanding of using sequences in pytorch.\n",
        "\n",
        "* Read the following\n",
        "\n",
        "> * [Pytorch sequence-to-sequence tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) (You will be implementing the decoder, not the encoder, as we are not doing sequence-to-sequence translation.)\n",
        "> * [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l7bdZWxvJrsx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a38e0ee-26e7-4ba5-9c6f-5e24333119f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-16 16:37:44--  https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz\n",
            "Resolving piazza.com (piazza.com)... 23.22.156.213, 44.216.95.21, 35.174.203.227, ...\n",
            "Connecting to piazza.com (piazza.com)|23.22.156.213|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz [following]\n",
            "--2024-02-16 16:37:44--  https://cdn-uploads.piazza.com/attach/jlifkda6h0x5bk/hzosotq4zil49m/jn13x09arfeb/text_files.tar.gz\n",
            "Resolving cdn-uploads.piazza.com (cdn-uploads.piazza.com)... 13.249.141.26, 13.249.141.11, 13.249.141.91, ...\n",
            "Connecting to cdn-uploads.piazza.com (cdn-uploads.piazza.com)|13.249.141.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1533290 (1.5M) [application/x-gzip]\n",
            "Saving to: ‘./text_files.tar.gz’\n",
            "\n",
            "./text_files.tar.gz 100%[===================>]   1.46M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-02-16 16:37:44 (23.9 MB/s) - ‘./text_files.tar.gz’ saved [1533290/1533290]\n",
            "\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "file_len = 2579888\n"
          ]
        }
      ],
      "source": [
        "! wget -O ./text_files.tar.gz 'https://piazza.com/redirect/s3?bucket=uploads&prefix=attach%2Fjlifkda6h0x5bk%2Fhzosotq4zil49m%2Fjn13x09arfeb%2Ftext_files.tar.gz'\n",
        "! tar -xzf text_files.tar.gz\n",
        "! pip install unidecode\n",
        "! pip install torch\n",
        "\n",
        "import unidecode\n",
        "import string\n",
        "import random\n",
        "\n",
        "all_characters = string.printable\n",
        "n_characters = len(all_characters)\n",
        "file = unidecode.unidecode(open('./text_files/lotr.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TxBeKeNjJ0NQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a284b35-129e-4fc0-bc33-abebd28618a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all the while I sit and think \n",
            "of times there were before, \n",
            "\n",
            "I listen for returning feet \n",
            "and voices at the door. \n",
            "\n",
            "It was a cold grey day near the end of December. The East Wind was \n",
            "streaming through\n"
          ]
        }
      ],
      "source": [
        "chunk_len = 200\n",
        "\n",
        "def random_chunk():\n",
        "  start_index = random.randint(0, file_len - chunk_len)\n",
        "  end_index = start_index + chunk_len + 1\n",
        "  return file[start_index:end_index]\n",
        "\n",
        "print(random_chunk())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "On0_WitWJ99e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57487eae-b918-43dc-f363-74159159bc9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([10, 11, 12, 39, 40, 41])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Turn string into list of longs\n",
        "def char_tensor(string):\n",
        "  tensor = torch.zeros(len(string)).long()\n",
        "  for c in range(len(string)):\n",
        "      tensor[c] = all_characters.index(string[c])\n",
        "  return tensor\n",
        "\n",
        "print(char_tensor('abcDEF'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYJPTLcaYmfI"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: Creating your own GRU cell\n",
        "\n",
        "**(Come back to this later - its defined here so that the GRU will be defined before it is used)**\n",
        "\n",
        "---\n",
        "\n",
        "The cell that you used in Part 1 was a pre-defined Pytorch layer. Now, write your own GRU class using the same parameters as the built-in Pytorch class does.\n",
        "\n",
        "Please do not look at the documentation's code for the GRU cell definition. The answer is right there in the code, and in theory, you could just cut-and-paste it. This bit is on your honor!\n",
        "\n",
        "**TODO:**\n",
        "* Create a custom GRU cell\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aavAv50ZKQ-F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class GRU(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers):\n",
        "    super(GRU, self).__init__()\n",
        "    self.num_layers = num_layers\n",
        "    # W_ir and W_hr\n",
        "    self.W_ir = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hr = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    # W_iz and W_hz\n",
        "    self.W_iz = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hz = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    # W_in and W_hn\n",
        "    self.W_in = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    self.W_hn = nn.ModuleList([nn.Linear(input_size, hidden_size) for _ in range(num_layers)])\n",
        "    # Activation Functions\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    # Each layer does the following:\n",
        "    # r_t = sigmoid(W_ir*x_t + b_ir + W_hr*h_(t-1) + b_hr)\n",
        "    # z_t = sigmoid(W_iz*x_t + b_iz + W_hz*h_(t-1) + b_hz)\n",
        "    # n_t = tanh(W_in*x_t + b_in + r_t**(W_hn*h_(t-1) + b_hn))\n",
        "    # h_(t) = (1 - z_t)**n_t + z_t**h_(t-1)\n",
        "    # Where ** is hadamard product (not matrix multiplication, but elementwise multiplication)\n",
        "\n",
        "    hiddens = torch.tensor.zeros(hidden.shape).cuda()\n",
        "    for i in range(self.num_layers):\n",
        "      hidden_input = hidden[i]\n",
        "      r_t = self.sigmoid(self.W_ir[i](inputs) + self.W_hr[i](hidden_input))\n",
        "      z_t = self.sigmoid(self.W_iz[i](inputs) + self.W_hz[i](hidden_input))\n",
        "      n_t = self.tanh(self.W_in[i](inputs) + self.W_hn[i](hidden_input))\n",
        "      h_t = torch.mul((1-z_t), n_t) + torch.mul(z_t, hidden_input)\n",
        "      del r_t, z_t, n_t\n",
        "\n",
        "      hiddens[i] = h_t\n",
        "      outputs = h_t\n",
        "\n",
        "    return outputs, hiddens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtXdX-B_WiAY"
      },
      "source": [
        "---\n",
        "\n",
        "##  Part 1: Building a sequence to sequence model\n",
        "\n",
        "---\n",
        "\n",
        "Great! We have the data in a useable form. We can switch out which text file we are reading from, and trying to simulate.\n",
        "\n",
        "We now want to build out an RNN model, in this section, we will use all built in Pytorch pieces when building our RNN class.\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create an RNN class that extends from nn.Module.\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "d6tNdEnzWj5F"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
        "    super(RNN, self).__init__()\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_size = output_size\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # more stuff here...\n",
        "    self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "    self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
        "    self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "  def forward(self, input_char, hidden):\n",
        "    # by reviewing the documentation, construct a forward function that properly uses the output\n",
        "    # of the GRU\n",
        "\n",
        "    # stuff here\n",
        "    output = self.embedding(input_char)\n",
        "\n",
        "    if len(output.size()) == 1:\n",
        "      output = output.unsqueeze(0).unsqueeze(0)\n",
        "    elif len(output.size()) == 2:\n",
        "      output = output.unsqueeze(0)\n",
        "\n",
        "    output = F.relu(output)\n",
        "    output, hidden = self.gru(output, hidden)\n",
        "    out_decoded = self.out(output).squeeze(0)\n",
        "\n",
        "    return out_decoded, hidden\n",
        "\n",
        "  def init_hidden(self):\n",
        "    return torch.zeros(self.n_layers, 1, self.hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hrhXghEPKD-5"
      },
      "outputs": [],
      "source": [
        "def random_training_set():\n",
        "  chunk = random_chunk()\n",
        "  inp = char_tensor(chunk[:-1])\n",
        "  target = char_tensor(chunk[1:])\n",
        "  return inp, target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpiGObbBX0Mr"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 2: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "We now want to be able to train our network, and sample text after training.\n",
        "\n",
        "This function outlines how training a sequence style network goes.\n",
        "\n",
        "**TODO:**\n",
        "* Fill in the pieces.\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2ALC3Pf8Kbsi"
      },
      "outputs": [],
      "source": [
        "# NOTE: decoder_optimizer, decoder, and criterion will be defined below as global variables\n",
        "def train(inp, target):\n",
        "  ## initialize hidden layers, set up gradient and loss\n",
        "    # your code here\n",
        "  ## /\n",
        "\n",
        "  decoder_optimizer.zero_grad()\n",
        "  hidden = decoder.init_hidden()\n",
        "  loss = 0\n",
        "\n",
        "  # more stuff here...\n",
        "  for i in range(len(inp)):\n",
        "    y_hat, hidden = decoder(inp[i], hidden)\n",
        "    loss += criterion(y_hat, target[i].unsqueeze(0))\n",
        "  loss.backward()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EN06NUu3YRlz"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 3: Sample text and Training information\n",
        "\n",
        "---\n",
        "\n",
        "You can at this time, if you choose, also write out your train loop boilerplate that samples random sequences and trains your RNN. This will be helpful to have working before writing your own GRU class.\n",
        "\n",
        "If you are finished training, or during training, and you want to sample from the network you may consider using the following function. If your RNN model is instantiated as `decoder`then this will probabilistically sample a sequence of length `predict_len`\n",
        "\n",
        "**TODO:**\n",
        "* Fill out the evaluate function to generate text frome a primed string\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B-bp-OZ1KjNh"
      },
      "outputs": [],
      "source": [
        "def sample_outputs(output, temperature):\n",
        "    \"\"\"Takes in a vector of unnormalized probability weights and samples a character from the distribution\"\"\"\n",
        "    # As temperature approaches 0, this sampling function becomes argmax (no randomness)\n",
        "    # As temperature approaches infinity, this sampling function becomes a purely random choice\n",
        "    return torch.multinomial(torch.exp(output / temperature), 1)\n",
        "\n",
        "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
        "  ## initialize hidden state, initialize other useful variables\n",
        "    # your code here\n",
        "  ## /\n",
        "  hidden = decoder.init_hidden()\n",
        "\n",
        "  # This loop primes the output and the hidden layers with the char_tensor function\n",
        "    # defined above and the prime_str that is present in the function parameters.\n",
        "  for char in prime_str:\n",
        "    output, hidden = decoder(char_tensor(char), hidden)\n",
        "\n",
        "  output_list = []\n",
        "  # This loop is very similar to the training.\n",
        "  for i in range(predict_len):\n",
        "    # You take a 'predicted' character index\n",
        "    character_index = sample_outputs(output, temperature)\n",
        "    # Send that index into the decoder (which is just the RNN model), and gives us the actual input\n",
        "      # and the hidden.\n",
        "    output, hidden = decoder(character_index, hidden)\n",
        "    # Appends those values to a list that is returned with the primer.\n",
        "    output_list.append(all_characters[character_index])\n",
        "\n",
        "  return prime_str + ''.join(output_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du4AGA8PcFEW"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 4: (Create a GRU cell, requirements above)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFS2bpHSZEU6"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Part 5: Run it and generate some text!\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "* Create some cool output\n",
        "\n",
        "**DONE:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Assuming everything has gone well, you should be able to run the main function in the scaffold code, using either your custom GRU cell or the built in layer, and see output something like this. I trained on the “lotr.txt” dataset, using chunk_length=200, hidden_size=100 for 2000 epochs. These are the results, along with the prime string:\n",
        "\n",
        "---\n",
        "\n",
        " G:\n",
        "\n",
        " Gandalf was decrond.\n",
        "'All have lord you. Forward the road at least walk this is stuff, and\n",
        "went to the long grey housel-winding and kindled side was a sleep pleasuring, I do long\n",
        "row hrough. In  \n",
        "\n",
        " lo:\n",
        "\n",
        " lost death it.\n",
        "'The last of the gatherings and take you,' said Aragorn, shining out of the Gate.\n",
        "'Yes, as you there were remembaused to seen their pass, when? What\n",
        "said here, such seven an the sear\n",
        "\n",
        " lo:\n",
        "\n",
        " low, and frod to keepn\n",
        "Came of their most. But here priced doubtless to an Sam up is\n",
        "masters; he left hor as they are looked. And he could now the long to stout in the right fro horseless of\n",
        "the like\n",
        "\n",
        " I:\n",
        "\n",
        " I had been the\n",
        "in his eyes with the perushed to lest, if then only the ring and the legended\n",
        "of the less of the long they which as the\n",
        "enders of Orcovered and smood, and the p\n",
        "\n",
        " I:\n",
        "\n",
        " I they were not the lord of the hoomes.\n",
        "Home already well from the Elves. And he sat strength, and we\n",
        "housed out of the good of the days to the mountains from his perith.\n",
        "\n",
        "'Yess! Where though as if  \n",
        "\n",
        " Th:\n",
        "\n",
        " There yarden\n",
        "you would guard the hoor might. Far and then may was\n",
        "croties, too began to see the drumbred many line\n",
        "and was then hoard walk and they heart, and the chair of the\n",
        "Ents of way, might was\n",
        "\n",
        " G:\n",
        "\n",
        " Gandalf\n",
        "been lat of less the round of the stump; both and seemed to the trees and perished they\n",
        "lay are speered the less; and the wind the steep and have to she\n",
        "precious. There was in the oonly went\n",
        "\n",
        " wh:\n",
        "\n",
        " which went out of the door.\n",
        "Hull the King and of the The days of his brodo\n",
        "stumbler of the windard was a thing there, then it been shining langing\n",
        "to him poor land. They hands; though they seemed ou\n",
        "\n",
        " ra:\n",
        "\n",
        " rather,' have all the least deather\n",
        "down of the truven beginning to the house of sunk.\n",
        "'Nark shorts of the Eyes of the Gate your great nothing as Eret.\n",
        "'I wander trust horn, and there were not, it  \n",
        "\n",
        " I:\n",
        "\n",
        " I can have no mind\n",
        "together! Where don't may had one may little blung\n",
        "terrible to tales. And turn and Gandalf shall be not to as only the Cattring\n",
        "not stopped great the out them forms. On they she lo\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-nXFeCmdKodw"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "n_epochs = 5000\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xKfozqw-6eqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb1fd74-4f03-4aef-c3c2-00b689553a4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[111.39304280281067 (200 19%) 557.5963]\n",
            "Whed c\n",
            "Rriud alt ad. od wow\n",
            "meng incaand ttot beow ho tcere thot asd Lot cnor agrrhe sociut the teom,  \n",
            "\n",
            "[204.05555701255798 (400 39%) 446.0882]\n",
            "Whor sbamill the \n",
            "\n",
            "vead Ame caar oct was weris mond be ooes that ell and we said nos the gallafedeld \n",
            " \n",
            "\n",
            "[296.3060305118561 (600 59%) 397.2455]\n",
            "Whing you I \n",
            "light, hi haid and the bame. \n",
            "Whe lirn.' \n",
            "\n",
            "'pinibut shanred yut and the sall thim, the So \n",
            "\n",
            "[388.7120954990387 (800 79%) 361.0090]\n",
            "Whing pirsing on them liped peathas do to qleath a passe mittle in I both to dent \n",
            "a main the up are t \n",
            "\n",
            "[483.46568608283997 (1000 99%) 334.3880]\n",
            "Whey himsen up from \n",
            "bive treat and they reptands of they me to was were unore the broughts in the \n",
            "am \n",
            "\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 1001\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())\n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ee0so6aKJ5L8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58de68f-8691-4bc4-c223-00530a0b0dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " lo\n",
            " looking in their gisile to is \n",
            "that pistegs brope unought would \n",
            "fell all be neanturss far the in lear wooder in there e spe the some hands and the hide asy it to that into the can of The noors to grist \n",
            "\n",
            " lo\n",
            " lork, and that are \n",
            "all the ronuhed as \n",
            "and wimis are will \n",
            "sleak to that his well to they spemt, and mbat besiled \n",
            "the spower pate. \n",
            "\n",
            "Net shadcard and're them. his all be thas a mong to ever of qut the \n",
            "\n",
            " he\n",
            " he botty there meard of Hisped. \n",
            "\n",
            "But whem a shat it hands of the could \n",
            "for mide to the spoin \n",
            "store. \n",
            "\n",
            "Has away to it all think on Mess. Not in the caknes; and it. \n",
            "\n",
            "And is the \n",
            "meem. The wore be verk \n",
            "\n",
            " Th\n",
            " The Gudden of the Enping his stills geep of the Gond \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "lit of the \n",
            "was the \n",
            "do this of the Et of the \n",
            "fear and store \n",
            "and nothly on the Elves of they move the Enes of the counse and my looked. 'For \n",
            "\n",
            " ra\n",
            " randing the brey. The ling and apping the End.' \n",
            "\n",
            "'He more that grats of the mester of the world in evains as has Frodo the Ellikis; by a ring and the cleading to the contelf to his this feame, and this \n",
            "\n",
            " Th\n",
            " The Has came shen the pale as hing the goen to before the Sranks of must upon the Fores. Af sounted where of they alls for to thouge \n",
            "and let do now, and they was stood of the Frodo said. 'I been. 'They \n",
            "\n",
            " I \n",
            " I Lores of the one and the swoors of use the Cide, To blate of the plooking of knamons as before. And the East and \n",
            "felp so doon hore to had midded to the more of Sam. \n",
            "'We past. I that we bet last to t \n",
            "\n",
            " G\n",
            " Gores, in \n",
            "can.' At we For is sut case of The collum \n",
            "oveributs in the \n",
            "all \n",
            "to his neise that \n",
            "were to we all his upeds at and king the onluft rise was and staicheth the string of Them too as is in hi \n",
            "\n",
            " I \n",
            " I gley foriribe the Said bear brave. He rid only \n",
            "for the could befue; When the bornotem to \n",
            "a pil of fight on of Thear \n",
            "bepovered thones. His are not tright tree mens of Gond, and theak ain the \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Sl \n",
            "\n",
            " I \n",
            " I light, and vood. Hooked was not insay wurned his for all the \n",
            "and them one on. And the Forethe stood a clowly retileve with the was \n",
            "\n",
            "stood of the Eneight for was took in the sevems \n",
            "at the Mounes. He \n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "#   all_characters.index(string[c])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJhgDc2IauPE"
      },
      "source": [
        "---\n",
        "\n",
        "## Part 6: Generate output on a different dataset\n",
        "\n",
        "---\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "* Choose a textual dataset. Here are some [text datasets](https://www.kaggle.com/datasets?tags=14104-text+data%2C13205-text+mining) from Kaggle\n",
        "\n",
        "* Generate some decent looking results and evaluate your model's performance (say what it did well / not so well)\n",
        "\n",
        "**DONE:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "n_characters = len(string.printable)\n",
        "file = unidecode.unidecode(open('./text_files/tiny_shakespeare.txt').read())\n",
        "file_len = len(file)\n",
        "print('file_len =', file_len)\n",
        "\n",
        "n_epochs = 1001\n",
        "print_every = 200\n",
        "plot_every = 10\n",
        "hidden_size = 200\n",
        "n_layers = 3\n",
        "lr = 0.001\n",
        "\n",
        "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
        "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "start = time.time()\n",
        "all_losses = []\n",
        "loss_avg = 0\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  loss_ = train(*random_training_set())\n",
        "  loss_avg += loss_\n",
        "\n",
        "  if epoch % print_every == 0:\n",
        "      print('[%s (%d %d%%) %.4f]' % (time.time() - start, epoch, epoch / n_epochs * 100, loss_))\n",
        "      print(evaluate('Wh', 100), '\\n')\n",
        "\n",
        "  if epoch % plot_every == 0:\n",
        "      all_losses.append(loss_avg / plot_every)\n",
        "      loss_avg = 0\n",
        "\n",
        "for i in range(10):\n",
        "  start_strings = [\" Th\", \" wh\", \" he\", \" I \", \" ca\", \" G\", \" lo\", \" ra\"]\n",
        "  start = random.randint(0,len(start_strings)-1)\n",
        "  print(start_strings[start])\n",
        "  print(evaluate(start_strings[start], 200), '\\n')"
      ],
      "metadata": {
        "id": "EtJeD0d1Myg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d6d10a8-c9a2-4307-fb98-22297aa4d3db"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file_len = 1115394\n",
            "[93.88283085823059 (200 19%) 494.6338]\n",
            "Whos hat miy thov wor,\n",
            "That ten mind ik hemerir ir time toy to kay on bes thal bou, bont mar ant you p \n",
            "\n",
            "[185.69856429100037 (400 39%) 408.5297]\n",
            "Why foit bere noe the in thang't is son ssargt lome.\n",
            "\n",
            "LINCINLE:\n",
            "CIARCINEA:\n",
            "Neas:\n",
            "Bod, tor totast steat \n",
            "\n",
            "[281.3085227012634 (600 59%) 409.3501]\n",
            "When the spomef\n",
            "Ouse with the a'luss afally\n",
            "Bith and heere.\n",
            "\n",
            "GRICHHoOd the shellood.\n",
            "\n",
            "YeRIPINIET:\n",
            "E'd! \n",
            "\n",
            "[382.4420771598816 (800 79%) 376.1064]\n",
            "Whis wither marest,\n",
            "What so samy, in the lip's\n",
            "Outh the forther'd stard honous it.\n",
            "\n",
            "MORVER:\n",
            "His his li \n",
            "\n",
            "[477.03542947769165 (1000 99%) 380.5674]\n",
            "Whow his can to shall,\n",
            "Sarebours him ever his boly me be and thou-hing but thank him thy truth than an \n",
            "\n",
            " ra\n",
            " raigs,\n",
            "And he hour betumn dean friet be all swe all high list, neqfen'd\n",
            "Wis and but me age bather,\n",
            "And but in kniair hone the lords, and have were, that thou put\n",
            "Thy by his nenven, purmide,\n",
            "And hory our \n",
            "\n",
            " lo\n",
            " lords.\n",
            "\n",
            "LUCCASS:\n",
            "Yet him, where hath my be greaw, would reving the now\n",
            "Thy panty the word, be. Cure on great with, me untrused now\n",
            "I had a liineing bust and with and alive me purviuns.\n",
            "\n",
            "MOMESTO:\n",
            "My hown \n",
            "\n",
            " ca\n",
            " cauns,\n",
            "Fill gray this nead falk soul, 's him.\n",
            "\n",
            "LUCIA:\n",
            "I his sould thee dold proult comming,\n",
            "And hert of his still.\n",
            "\n",
            "LARENA:\n",
            "A true the cullion shate heay how while.\n",
            "\n",
            "RONEO:\n",
            "A vile swill see that am and  \n",
            "\n",
            " G\n",
            " Gike in and will will, by the live\n",
            "pebadion of sheen a cours will shall in hours reay.\n",
            "\n",
            "CLANDELA:\n",
            "Share that cand will and thear is porse in it and thee me lardend;\n",
            "Yis ever of have the hath thy palard \n",
            "\n",
            " he\n",
            " hearth shall\n",
            "By be to hing stilf by you live:\n",
            "Cronce him, the parise, a but and the cayer here;\n",
            "Whand in what the chore comss that them the shiled creased, and these fariour, and prain. my live though n \n",
            "\n",
            " ra\n",
            " rasing,\n",
            "In of that with he besarufed,\n",
            "Will names and in love dost sir, and with, collus,\n",
            "The dister concess respent.\n",
            "\n",
            "GLOLEBUE:\n",
            "My stand here I will listing any thay then?\n",
            "\n",
            "OLANA:\n",
            "And swald on a lave, h \n",
            "\n",
            " G\n",
            " God to then;\n",
            "I'll libut think by speace ould nows\n",
            "shring pupy track here, and ny ware and dather.\n",
            "And this feep of their our beay to she by.\n",
            "\n",
            "FALIO:\n",
            "I Tay will be my hast your nock lords.\n",
            "My like you\n",
            "s \n",
            "\n",
            " I \n",
            " I have herister\n",
            "Seared frough all be of his braught.\n",
            "\n",
            "NRATI\n",
            "TARd BERDING:\n",
            "I live that houls with has he commer, and fashing his ose, my to then of the give on they curreng the lord,\n",
            "And fage these of th \n",
            "\n",
            " G\n",
            " Giged thing, and will no\n",
            "Send in thy fall take wish torn,\n",
            "Then he with Vack breath\n",
            "The sharving,\n",
            "My shrue now on becior perire reavel your halles,\n",
            "The know that dears Sind.\n",
            "\n",
            "BONTHRIUS:\n",
            "You that being t \n",
            "\n",
            " ra\n",
            " rast she lid,\n",
            "And us redisting in him, I prase of in if of Jucion:\n",
            "Shone your dowm's forth but blood have of the pere.\n",
            "\n",
            "JARMHAV:\n",
            "Srieven: he cours and but will from fathount'st, farious; mack and soul a \n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}