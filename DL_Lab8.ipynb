{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zachjoachim7/deep_learning_labs/blob/main/DL_Lab8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEwpv0GZ_CrT"
      },
      "source": [
        "<a\n",
        "href=\"https://colab.research.google.com/github/wingated/cs474_labs_f2019/blob/master/DL_Lab10.ipynb\"\n",
        "  target=\"_parent\">\n",
        "  <img\n",
        "    src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
        "    alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnoEhAVvBcMj"
      },
      "source": [
        "# Lab 8: Transfer Learning/Fine-Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBOvJdJfkXIL"
      },
      "source": [
        "## Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiuvTUWOjtBC"
      },
      "source": [
        "### Objective\n",
        "\n",
        "- Gain experience fine-tuning pre-trained models to domain-specific applications.\n",
        "\n",
        "### Deliverable\n",
        "\n",
        "For this lab you will submit an ipython notebook via learning suite. The bulk of the work is in modifying fine-tuning a pre-trained ResNet. Fine-tuning the GPT-2 language model is pretty easy. The provided code works as is; you will just have to swap in your own text dataset.\n",
        "\n",
        "### Grading\n",
        "\n",
        "- 35% Create a dataset class for your own dataset\n",
        "- 35% Create a network class that wraps a pretrained ResNet\n",
        "- 20% Implement unfreezing in the network class\n",
        "- 10% Fine-tune GPT-2 on your own dataset\n",
        "\n",
        "### Tips\n",
        "- Your life will be better if you download a dataset that already has the data in the expected format for ImageFolder (make sure to read the documentation!). The datasets recommended below are in the correct format.\n",
        "- Get the CNN working on the provided dataset (bird species classification) before swapping in your own.\n",
        "- For reference on freezing/unfreezing network weights, see [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c)\n",
        "- Check PyTorch's documentation to learn the difference between `requires_grad=False` and `requires_grad_(False)`.\n",
        "- For training GPT-2, first try the medium-size (355M parameter) model. If your Colab instance doesn't have enough GPU space, you may need to switch to the small-size (124M parameter) model, but the results will be less impressive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "dKzRORuLBNLR"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet152\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "import zipfile\n",
        "from google.colab import files\n",
        "import os\n",
        "import sys\n",
        "from PIL import Image, ImageOps\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4R3D8Mr8b54"
      },
      "source": [
        "## 1 Fine-tune a ResNet for image classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFoEeTYHDq2s"
      },
      "source": [
        "### 1.1 Find a dataset to fine-tune on, and make a Dataset class (1 hr.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z6g7a_Y84n0"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8NtFZRd5hcm"
      },
      "source": [
        "- Inherit from torch.utils.data.Dataset\n",
        "- Use a [torchvision.datasets.ImageFolder](https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.ImageFolder)\n",
        "- Don't spend too long finding another dataset. Some suggestions that you are free to use:\n",
        " - https://www.kaggle.com/jessicali9530/stanford-dogs-dataset\n",
        " - https://www.kaggle.com/puneet6060/intel-image-classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBigIUFTukeJ"
      },
      "source": [
        "#### Help for downloading kaggle datasets\n",
        "Downloading Kaggle datasets requires authentication, so you can't just download from a url. Here are some step-by-step instructions of how to get Kaggle datasets in Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X29UC6CvwfQ"
      },
      "source": [
        "1. Create an API key in Kaggle\n",
        "    - Click on profile photo\n",
        "    - Go to 'My Account'\n",
        "    - Scroll down to the API access section and click \"Create New API Token\"\n",
        "    - `kaggle.json` is now downloaded to your computer\n",
        "\n",
        "2. Upload the API key and install the Kaggle API client by running the next cell (run it again if it throws an error the first time). Also, `files.upload()` may not work in Firefox. One solution is to expand the Files banner (indicated by the '>' tab on the left side of the page) and use that to upload the key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Mhjc0pM7jOoZ",
        "outputId": "6fbe17ec-cfa5-4324-bce7-7de34b1ed5a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff6f04ed-2a72-4bc6-8d75-2f1a4ad56177\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ff6f04ed-2a72-4bc6-8d75-2f1a4ad56177\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "-rw-r--r-- 1 root root 68 Mar 10 01:47 kaggle.json\n"
          ]
        }
      ],
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "# Next, install the Kaggle API client.\n",
        "!pip install -q kaggle\n",
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json\n",
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGlIa4SIwEXB"
      },
      "source": [
        "3. Copy the desired dataset locally"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7HtB-XdIr1EE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c710ac4-3a54-4698-ff25-1e300e753d87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading intel-image-classification.zip to /content\n",
            " 99% 344M/346M [00:05<00:00, 26.6MB/s]\n",
            "100% 346M/346M [00:05<00:00, 67.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Example download command for dataset found here: https://www.kaggle.com/akash2907/bird-species-classification\n",
        "!kaggle datasets download -d puneet6060/intel-image-classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcz0JGXjxFGe"
      },
      "source": [
        "#### Make the Dataset class\n",
        "See the implementation below for reference, and implement a dataset class for the dataset you choose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lthPlsGeK4CX"
      },
      "outputs": [],
      "source": [
        "class BirdDataset(Dataset):\n",
        "    def __init__(self, zip_file='bird-species-classification.zip', size=256, train=True, upload=False):\n",
        "        super(BirdDataset, self).__init__()\n",
        "\n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            # Resize the images - originally they are high resolution. We could do this\n",
        "            # in the DataLoader, but it will read the full-resolution files from disk\n",
        "            # every time before resizing them, making training slow\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "\n",
        "        if train:\n",
        "            # The bird-species dataset mistakenly has a train_data folder inside of train_data\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'train_data', 'train_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'test_data', 'test_data'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=256):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset_folder)\n",
        "\n",
        "bird_data = BirdDataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5jHFdToeDtIF"
      },
      "outputs": [],
      "source": [
        "##############################\n",
        "# Implement your own Dataset #\n",
        "##############################\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, zip_file='./intel-image-classification.zip', size=64, train=True, upload=False):\n",
        "        super(CustomDataset, self).__init__()\n",
        "\n",
        "        self.train = train\n",
        "        extract_dir = os.path.splitext(zip_file)[0]\n",
        "        if not os.path.exists(extract_dir):\n",
        "            os.makedirs(extract_dir)\n",
        "            self.extract_zip(zip_file, extract_dir)\n",
        "            self.resize(extract_dir, size=size)\n",
        "\n",
        "        postfix = 'train' if train else 'test'\n",
        "\n",
        "        if train:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_train', 'seg_train'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "        else:\n",
        "            self.dataset_folder = datasets.ImageFolder(os.path.join(extract_dir, 'seg_test', 'seg_test'), transform=transforms.Compose([transforms.ToTensor()]))\n",
        "\n",
        "    def extract_zip(self, zip_file, extract_dir):\n",
        "        print(\"Extracting\", zip_file)\n",
        "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "\n",
        "    def resize(self, path, size=64):\n",
        "        \"\"\"Resizes all images in place\"\"\"\n",
        "        print(\"Resizing images\")\n",
        "        dirs = os.walk(path)\n",
        "        for root, dirs, files in os.walk(path):\n",
        "            for item in files:\n",
        "                name = os.path.join(root, item)\n",
        "                if os.path.isfile(name):\n",
        "                    im = Image.open(name)\n",
        "                    im = ImageOps.fit(im, (size, size))\n",
        "                    im.save(name[:-3] + 'bmp', 'BMP')\n",
        "                    os.remove(name)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.dataset_folder[i]\n",
        "\n",
        "    def __len__(self):\n",
        "      return 500\n",
        "\n",
        "intel_data = CustomDataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vJVbYcAJAf2"
      },
      "source": [
        "### 1.2 Wrap a pretrained ResNet in an `nn.Module` (30 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOzGDND9FD1"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLvmDHbl9IyG"
      },
      "source": [
        "- Make a model class that inherits from `nn.Module`\n",
        "- Wrap a pretrained ResNet and swap out the last layer of that network with a layer that maps to the number of classes in your new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOtl8z8G9wbr"
      },
      "source": [
        "#### Make your model class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AY-XU4Mwas0j"
      },
      "outputs": [],
      "source": [
        "class CustomResNet(nn.Module):\n",
        "    def __init__(self, num_classes, start_frozen=False):\n",
        "        super(CustomResNet, self).__init__()\n",
        "\n",
        "        # Part 1.2\n",
        "        # Load the model - make sure it is pre-trained\n",
        "        self.resnet_model = resnet152(pretrained=True)\n",
        "\n",
        "        # Part 1.4\n",
        "        if start_frozen:\n",
        "            # Turn off all gradients of the resnet\n",
        "            for p in self.resnet_model.parameters():\n",
        "              p.requires_grad = False\n",
        "\n",
        "        # Part 1.2\n",
        "        # Look at the code of torchvision.models.resnet152 to find the name of the attribute to override (the last layer of the resnet)\n",
        "        # Override the last layer of the neural network to map to the correct number of classes. Note that this new layer has requires_grad = True\n",
        "        # It is called fc\n",
        "        self.resnet_model.fc = nn.Linear(self.resnet_model.fc.in_features, num_classes)\n",
        "\n",
        "    def unfreeze(self, n_layers):\n",
        "        # Part 1.4\n",
        "        # Turn on gradients for the last n_layers\n",
        "        params = list(self.resnet_model.parameters())\n",
        "        # Do last n_layers, not n_layers onward.\n",
        "        param = params[len(params) - n_layers:]\n",
        "        for p in param:\n",
        "          p.requires_grad = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Part 1.2\n",
        "        # Pass x through the resnet\n",
        "        return self.resnet_model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Krh0eYy18R9"
      },
      "source": [
        "### 1.3 Read through and run this training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "yOGrrw2gbIPf"
      },
      "outputs": [],
      "source": [
        "def accuracy(y_hat, y_truth):\n",
        "    \"\"\"Gets average accuracy of a vector of predictions\"\"\"\n",
        "\n",
        "    preds = torch.argmax(y_hat, dim=1)\n",
        "    acc = torch.mean((preds == y_truth).float())\n",
        "    return acc\n",
        "\n",
        "def evaluate(model, objective, val_loader, device):\n",
        "    \"\"\"Gets average accuracy and loss for the validation set\"\"\"\n",
        "\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "    batches = 0\n",
        "    # model.eval() so that batchnorm and dropout work in eval mode\n",
        "    model.eval()\n",
        "    # torch.no_grad() to turn off computation graph creation. This allows for temporal\n",
        "    # and spatial complexity improvements, which allows for larger validation batch\n",
        "    # sizes so it’s recommended\n",
        "    with torch.no_grad():\n",
        "        for x, y_truth in val_loader:\n",
        "\n",
        "            batches += 1\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "            y_hat = model(x)\n",
        "            val_loss = objective(y_hat, y_truth)\n",
        "            val_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            val_accs.append(val_acc)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return torch.sum(torch.Tensor(val_losses)) / batches, torch.sum(torch.Tensor(val_accs)) / batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "VKESMcKi2E_f"
      },
      "outputs": [],
      "source": [
        "def train(start_frozen=False, model_unfreeze=0):\n",
        "    \"\"\"Fine-tunes a CNN\n",
        "    Args:\n",
        "        start_frozen (bool): whether to start with the network weights frozen.\n",
        "        model_unfreeze (int): the maximum number of network layers to unfreeze\n",
        "    \"\"\"\n",
        "    epochs = 20\n",
        "    # Start with a very low learning rate\n",
        "    lr = .00005\n",
        "    val_every = 3\n",
        "    num_classes = 16\n",
        "    batch_size = 32\n",
        "    device = torch.device('cuda:0')\n",
        "\n",
        "    # Data\n",
        "    # TODO: Use your own dataset\n",
        "    train_dataset = CustomDataset(upload=True, train=True)\n",
        "    val_dataset = CustomDataset(upload=True, train=False)\n",
        "    train_loader = DataLoader(train_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "    val_loader = DataLoader(val_dataset,\n",
        "                              shuffle=True,\n",
        "                              num_workers=8,\n",
        "                              batch_size=batch_size)\n",
        "\n",
        "    # Model\n",
        "    model = CustomResNet(num_classes, start_frozen=start_frozen).to(device)\n",
        "\n",
        "    # Objective\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    # Optimizer\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-1)\n",
        "\n",
        "    # Progress bar\n",
        "    pbar = tqdm(total=len(train_loader) * epochs)\n",
        "\n",
        "    train_losses = []\n",
        "    train_accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    cnt = 0\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Implement model unfreezing\n",
        "        if epoch < model_unfreeze:\n",
        "            # Part 1.4\n",
        "            # Unfreeze the last layers, one more each epoch\n",
        "            model.unfreeze(epoch+1)\n",
        "\n",
        "        for x, y_truth in train_loader:\n",
        "\n",
        "            x, y_truth = x.to(device), y_truth.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            y_hat = model(x)\n",
        "            train_loss = objective(y_hat, y_truth)\n",
        "            train_acc = accuracy(y_hat, y_truth)\n",
        "\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_accs.append(train_acc.item())\n",
        "            train_losses.append(train_loss.item())\n",
        "\n",
        "            if cnt % val_every == 0:\n",
        "                val_loss, val_acc = evaluate(model, objective, val_loader, device)\n",
        "                val_losses.append(val_loss)\n",
        "                val_accs.append(val_acc)\n",
        "\n",
        "            pbar.set_description('train loss:{:.4f}, train accuracy:{:.4f}.'.format(train_loss.item(), train_acc))\n",
        "            pbar.update(1)\n",
        "            cnt += 1\n",
        "\n",
        "    pbar.close()\n",
        "    plt.subplot(121)\n",
        "    plt.plot(np.arange(len(train_accs)), train_accs, label='Train Accuracy')\n",
        "    plt.plot(np.arange(len(train_accs), step=val_every), val_accs, label='Val Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(122)\n",
        "    plt.plot(np.arange(len(train_losses)), train_losses, label='Train Loss')\n",
        "    plt.plot(np.arange(len(train_losses), step=val_every), val_losses, label='Val Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fvnxeLotchiH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fd15f753-c501-4b25-fa6b-1ce4bd36d246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "\n",
            "  0%|          | 0/320 [00:00<?, ?it/s]\u001b[A\n",
            "train loss:2.6777, train accuracy:0.0000.:   0%|          | 0/320 [00:02<?, ?it/s]\u001b[A\n",
            "train loss:2.6777, train accuracy:0.0000.:   0%|          | 1/320 [00:02<11:56,  2.25s/it]\u001b[A\n",
            "train loss:2.5816, train accuracy:0.1875.:   0%|          | 1/320 [00:02<11:56,  2.25s/it]\u001b[A\n",
            "train loss:2.5816, train accuracy:0.1875.:   1%|          | 2/320 [00:02<05:20,  1.01s/it]\u001b[A\n",
            "train loss:2.4838, train accuracy:0.0938.:   1%|          | 2/320 [00:02<05:20,  1.01s/it]\u001b[A\n",
            "train loss:2.4838, train accuracy:0.0938.:   1%|          | 3/320 [00:02<03:12,  1.64it/s]\u001b[A\n",
            "train loss:2.3980, train accuracy:0.1875.:   1%|          | 3/320 [00:04<03:12,  1.64it/s]\u001b[A\n",
            "train loss:2.3980, train accuracy:0.1875.:   1%|▏         | 4/320 [00:04<05:16,  1.00s/it]\u001b[A\n",
            "train loss:2.3298, train accuracy:0.2812.:   1%|▏         | 4/320 [00:04<05:16,  1.00s/it]\u001b[A\n",
            "train loss:2.3298, train accuracy:0.2812.:   2%|▏         | 5/320 [00:04<03:39,  1.44it/s]\u001b[A\n",
            "train loss:2.2396, train accuracy:0.2500.:   2%|▏         | 5/320 [00:04<03:39,  1.44it/s]\u001b[A\n",
            "train loss:2.2396, train accuracy:0.2500.:   2%|▏         | 6/320 [00:04<02:40,  1.96it/s]\u001b[A\n",
            "train loss:2.1413, train accuracy:0.4062.:   2%|▏         | 6/320 [00:06<02:40,  1.96it/s]\u001b[A\n",
            "train loss:2.1413, train accuracy:0.4062.:   2%|▏         | 7/320 [00:06<05:04,  1.03it/s]\u001b[A\n",
            "train loss:2.0484, train accuracy:0.5625.:   2%|▏         | 7/320 [00:06<05:04,  1.03it/s]\u001b[A\n",
            "train loss:2.0484, train accuracy:0.5625.:   2%|▎         | 8/320 [00:06<03:40,  1.42it/s]\u001b[A\n",
            "train loss:1.9508, train accuracy:0.6562.:   2%|▎         | 8/320 [00:06<03:40,  1.42it/s]\u001b[A\n",
            "train loss:1.9508, train accuracy:0.6562.:   3%|▎         | 9/320 [00:06<02:43,  1.90it/s]\u001b[A\n",
            "train loss:1.8771, train accuracy:0.7188.:   3%|▎         | 9/320 [00:08<02:43,  1.90it/s]\u001b[A\n",
            "train loss:1.8771, train accuracy:0.7188.:   3%|▎         | 10/320 [00:08<04:11,  1.23it/s]\u001b[A\n",
            "train loss:1.7957, train accuracy:0.8438.:   3%|▎         | 10/320 [00:08<04:11,  1.23it/s]\u001b[A\n",
            "train loss:1.7957, train accuracy:0.8438.:   3%|▎         | 11/320 [00:08<03:07,  1.65it/s]\u001b[A\n",
            "train loss:1.7281, train accuracy:0.9062.:   3%|▎         | 11/320 [00:08<03:07,  1.65it/s]\u001b[A\n",
            "train loss:1.7281, train accuracy:0.9062.:   4%|▍         | 12/320 [00:08<02:22,  2.16it/s]\u001b[A\n",
            "train loss:1.6236, train accuracy:0.9062.:   4%|▍         | 12/320 [00:09<02:22,  2.16it/s]\u001b[A\n",
            "train loss:1.6236, train accuracy:0.9062.:   4%|▍         | 13/320 [00:09<03:52,  1.32it/s]\u001b[A\n",
            "train loss:1.5661, train accuracy:0.9375.:   4%|▍         | 13/320 [00:09<03:52,  1.32it/s]\u001b[A\n",
            "train loss:1.5661, train accuracy:0.9375.:   4%|▍         | 14/320 [00:09<02:54,  1.75it/s]\u001b[A\n",
            "train loss:1.4859, train accuracy:0.9375.:   4%|▍         | 14/320 [00:10<02:54,  1.75it/s]\u001b[A\n",
            "train loss:1.4859, train accuracy:0.9375.:   5%|▍         | 15/320 [00:10<02:14,  2.27it/s]\u001b[A\n",
            "train loss:1.3960, train accuracy:1.0000.:   5%|▍         | 15/320 [00:11<02:14,  2.27it/s]\u001b[A\n",
            "train loss:1.3960, train accuracy:1.0000.:   5%|▌         | 16/320 [00:11<03:46,  1.34it/s]\u001b[A\n",
            "train loss:1.2987, train accuracy:1.0000.:   5%|▌         | 16/320 [00:12<03:46,  1.34it/s]\u001b[A\n",
            "train loss:1.2987, train accuracy:1.0000.:   5%|▌         | 17/320 [00:12<04:08,  1.22it/s]\u001b[A\n",
            "train loss:1.2219, train accuracy:1.0000.:   5%|▌         | 17/320 [00:12<04:08,  1.22it/s]\u001b[A\n",
            "train loss:1.2219, train accuracy:1.0000.:   6%|▌         | 18/320 [00:12<03:05,  1.63it/s]\u001b[A\n",
            "train loss:1.1654, train accuracy:1.0000.:   6%|▌         | 18/320 [00:14<03:05,  1.63it/s]\u001b[A\n",
            "train loss:1.1654, train accuracy:1.0000.:   6%|▌         | 19/320 [00:14<04:20,  1.15it/s]\u001b[A\n",
            "train loss:1.1134, train accuracy:1.0000.:   6%|▌         | 19/320 [00:14<04:20,  1.15it/s]\u001b[A\n",
            "train loss:1.1134, train accuracy:1.0000.:   6%|▋         | 20/320 [00:14<03:14,  1.54it/s]\u001b[A\n",
            "train loss:1.0376, train accuracy:1.0000.:   6%|▋         | 20/320 [00:14<03:14,  1.54it/s]\u001b[A\n",
            "train loss:1.0376, train accuracy:1.0000.:   7%|▋         | 21/320 [00:14<02:28,  2.01it/s]\u001b[A\n",
            "train loss:0.9804, train accuracy:1.0000.:   7%|▋         | 21/320 [00:15<02:28,  2.01it/s]\u001b[A\n",
            "train loss:0.9804, train accuracy:1.0000.:   7%|▋         | 22/320 [00:15<03:50,  1.29it/s]\u001b[A\n",
            "train loss:0.9252, train accuracy:1.0000.:   7%|▋         | 22/320 [00:15<03:50,  1.29it/s]\u001b[A\n",
            "train loss:0.9252, train accuracy:1.0000.:   7%|▋         | 23/320 [00:15<02:54,  1.70it/s]\u001b[A\n",
            "train loss:0.8622, train accuracy:1.0000.:   7%|▋         | 23/320 [00:16<02:54,  1.70it/s]\u001b[A\n",
            "train loss:0.8622, train accuracy:1.0000.:   8%|▊         | 24/320 [00:16<02:15,  2.18it/s]\u001b[A\n",
            "train loss:0.8306, train accuracy:1.0000.:   8%|▊         | 24/320 [00:18<02:15,  2.18it/s]\u001b[A\n",
            "train loss:0.8306, train accuracy:1.0000.:   8%|▊         | 25/320 [00:18<04:51,  1.01it/s]\u001b[A\n",
            "train loss:0.7807, train accuracy:1.0000.:   8%|▊         | 25/320 [00:18<04:51,  1.01it/s]\u001b[A\n",
            "train loss:0.7807, train accuracy:1.0000.:   8%|▊         | 26/320 [00:18<03:37,  1.35it/s]\u001b[A\n",
            "train loss:0.7248, train accuracy:1.0000.:   8%|▊         | 26/320 [00:18<03:37,  1.35it/s]\u001b[A\n",
            "train loss:0.7248, train accuracy:1.0000.:   8%|▊         | 27/320 [00:18<02:44,  1.78it/s]\u001b[A\n",
            "train loss:0.6832, train accuracy:1.0000.:   8%|▊         | 27/320 [00:20<02:44,  1.78it/s]\u001b[A\n",
            "train loss:0.6832, train accuracy:1.0000.:   9%|▉         | 28/320 [00:20<04:02,  1.20it/s]\u001b[A\n",
            "train loss:0.6623, train accuracy:1.0000.:   9%|▉         | 28/320 [00:20<04:02,  1.20it/s]\u001b[A\n",
            "train loss:0.6623, train accuracy:1.0000.:   9%|▉         | 29/320 [00:20<03:01,  1.60it/s]\u001b[A\n",
            "train loss:0.6032, train accuracy:1.0000.:   9%|▉         | 29/320 [00:20<03:01,  1.60it/s]\u001b[A\n",
            "train loss:0.6032, train accuracy:1.0000.:   9%|▉         | 30/320 [00:20<02:18,  2.09it/s]\u001b[A\n",
            "train loss:0.5673, train accuracy:1.0000.:   9%|▉         | 30/320 [00:21<02:18,  2.09it/s]\u001b[A\n",
            "train loss:0.5673, train accuracy:1.0000.:  10%|▉         | 31/320 [00:21<03:42,  1.30it/s]\u001b[A\n",
            "train loss:0.5304, train accuracy:1.0000.:  10%|▉         | 31/320 [00:21<03:42,  1.30it/s]\u001b[A\n",
            "train loss:0.5304, train accuracy:1.0000.:  10%|█         | 32/320 [00:21<02:46,  1.72it/s]\u001b[A\n",
            "train loss:0.4865, train accuracy:1.0000.:  10%|█         | 32/320 [00:23<02:46,  1.72it/s]\u001b[A\n",
            "train loss:0.4865, train accuracy:1.0000.:  10%|█         | 33/320 [00:23<03:58,  1.20it/s]\u001b[A\n",
            "train loss:0.4523, train accuracy:1.0000.:  10%|█         | 33/320 [00:25<03:58,  1.20it/s]\u001b[A\n",
            "train loss:0.4523, train accuracy:1.0000.:  11%|█         | 34/320 [00:25<06:05,  1.28s/it]\u001b[A\n",
            "train loss:0.4274, train accuracy:1.0000.:  11%|█         | 34/320 [00:25<06:05,  1.28s/it]\u001b[A\n",
            "train loss:0.4274, train accuracy:1.0000.:  11%|█         | 35/320 [00:25<04:26,  1.07it/s]\u001b[A\n",
            "train loss:0.4060, train accuracy:1.0000.:  11%|█         | 35/320 [00:25<04:26,  1.07it/s]\u001b[A\n",
            "train loss:0.4060, train accuracy:1.0000.:  11%|█▏        | 36/320 [00:25<03:17,  1.44it/s]\u001b[A\n",
            "train loss:0.3904, train accuracy:1.0000.:  11%|█▏        | 36/320 [00:27<03:17,  1.44it/s]\u001b[A\n",
            "train loss:0.3904, train accuracy:1.0000.:  12%|█▏        | 37/320 [00:27<04:22,  1.08it/s]\u001b[A\n",
            "train loss:0.3532, train accuracy:1.0000.:  12%|█▏        | 37/320 [00:27<04:22,  1.08it/s]\u001b[A\n",
            "train loss:0.3532, train accuracy:1.0000.:  12%|█▏        | 38/320 [00:27<03:14,  1.45it/s]\u001b[A\n",
            "train loss:0.3467, train accuracy:1.0000.:  12%|█▏        | 38/320 [00:27<03:14,  1.45it/s]\u001b[A\n",
            "train loss:0.3467, train accuracy:1.0000.:  12%|█▏        | 39/320 [00:27<02:27,  1.91it/s]\u001b[A\n",
            "train loss:0.3274, train accuracy:1.0000.:  12%|█▏        | 39/320 [00:29<02:27,  1.91it/s]\u001b[A\n",
            "train loss:0.3274, train accuracy:1.0000.:  12%|█▎        | 40/320 [00:29<04:02,  1.15it/s]\u001b[A\n",
            "train loss:0.3109, train accuracy:1.0000.:  12%|█▎        | 40/320 [00:29<04:02,  1.15it/s]\u001b[A\n",
            "train loss:0.3109, train accuracy:1.0000.:  13%|█▎        | 41/320 [00:29<03:02,  1.53it/s]\u001b[A\n",
            "train loss:0.2924, train accuracy:1.0000.:  13%|█▎        | 41/320 [00:29<03:02,  1.53it/s]\u001b[A\n",
            "train loss:0.2924, train accuracy:1.0000.:  13%|█▎        | 42/320 [00:29<02:20,  1.98it/s]\u001b[A\n",
            "train loss:0.2704, train accuracy:1.0000.:  13%|█▎        | 42/320 [00:31<02:20,  1.98it/s]\u001b[A\n",
            "train loss:0.2704, train accuracy:1.0000.:  13%|█▎        | 43/320 [00:31<04:13,  1.09it/s]\u001b[A\n",
            "train loss:0.2667, train accuracy:1.0000.:  13%|█▎        | 43/320 [00:31<04:13,  1.09it/s]\u001b[A\n",
            "train loss:0.2667, train accuracy:1.0000.:  14%|█▍        | 44/320 [00:31<03:08,  1.47it/s]\u001b[A\n",
            "train loss:0.2509, train accuracy:1.0000.:  14%|█▍        | 44/320 [00:31<03:08,  1.47it/s]\u001b[A\n",
            "train loss:0.2509, train accuracy:1.0000.:  14%|█▍        | 45/320 [00:31<02:22,  1.93it/s]\u001b[A\n",
            "train loss:0.2396, train accuracy:1.0000.:  14%|█▍        | 45/320 [00:33<02:22,  1.93it/s]\u001b[A\n",
            "train loss:0.2396, train accuracy:1.0000.:  14%|█▍        | 46/320 [00:33<03:39,  1.25it/s]\u001b[A\n",
            "train loss:0.2265, train accuracy:1.0000.:  14%|█▍        | 46/320 [00:33<03:39,  1.25it/s]\u001b[A\n",
            "train loss:0.2265, train accuracy:1.0000.:  15%|█▍        | 47/320 [00:33<02:44,  1.66it/s]\u001b[A\n",
            "train loss:0.2203, train accuracy:1.0000.:  15%|█▍        | 47/320 [00:33<02:44,  1.66it/s]\u001b[A\n",
            "train loss:0.2203, train accuracy:1.0000.:  15%|█▌        | 48/320 [00:33<02:04,  2.18it/s]\u001b[A\n",
            "train loss:0.2050, train accuracy:1.0000.:  15%|█▌        | 48/320 [00:35<02:04,  2.18it/s]\u001b[A\n",
            "train loss:0.2050, train accuracy:1.0000.:  15%|█▌        | 49/320 [00:35<04:41,  1.04s/it]\u001b[A\n",
            "train loss:0.1931, train accuracy:1.0000.:  15%|█▌        | 49/320 [00:36<04:41,  1.04s/it]\u001b[A\n",
            "train loss:0.1931, train accuracy:1.0000.:  16%|█▌        | 50/320 [00:36<03:27,  1.30it/s]\u001b[A\n",
            "train loss:0.1947, train accuracy:1.0000.:  16%|█▌        | 50/320 [00:36<03:27,  1.30it/s]\u001b[A\n",
            "train loss:0.1947, train accuracy:1.0000.:  16%|█▌        | 51/320 [00:36<02:35,  1.73it/s]\u001b[A\n",
            "train loss:0.1750, train accuracy:1.0000.:  16%|█▌        | 51/320 [00:37<02:35,  1.73it/s]\u001b[A\n",
            "train loss:0.1750, train accuracy:1.0000.:  16%|█▋        | 52/320 [00:37<03:46,  1.18it/s]\u001b[A\n",
            "train loss:0.1724, train accuracy:1.0000.:  16%|█▋        | 52/320 [00:37<03:46,  1.18it/s]\u001b[A\n",
            "train loss:0.1724, train accuracy:1.0000.:  17%|█▋        | 53/320 [00:37<02:49,  1.57it/s]\u001b[A\n",
            "train loss:0.1608, train accuracy:1.0000.:  17%|█▋        | 53/320 [00:37<02:49,  1.57it/s]\u001b[A\n",
            "train loss:0.1608, train accuracy:1.0000.:  17%|█▋        | 54/320 [00:37<02:09,  2.06it/s]\u001b[A\n",
            "train loss:0.1585, train accuracy:1.0000.:  17%|█▋        | 54/320 [00:39<02:09,  2.06it/s]\u001b[A\n",
            "train loss:0.1585, train accuracy:1.0000.:  17%|█▋        | 55/320 [00:39<03:28,  1.27it/s]\u001b[A\n",
            "train loss:0.1588, train accuracy:1.0000.:  17%|█▋        | 55/320 [00:39<03:28,  1.27it/s]\u001b[A\n",
            "train loss:0.1588, train accuracy:1.0000.:  18%|█▊        | 56/320 [00:39<02:36,  1.69it/s]\u001b[A\n",
            "train loss:0.1539, train accuracy:1.0000.:  18%|█▊        | 56/320 [00:39<02:36,  1.69it/s]\u001b[A\n",
            "train loss:0.1539, train accuracy:1.0000.:  18%|█▊        | 57/320 [00:39<01:59,  2.19it/s]\u001b[A\n",
            "train loss:0.1552, train accuracy:1.0000.:  18%|█▊        | 57/320 [00:41<01:59,  2.19it/s]\u001b[A\n",
            "train loss:0.1552, train accuracy:1.0000.:  18%|█▊        | 58/320 [00:41<03:25,  1.27it/s]\u001b[A\n",
            "train loss:0.1439, train accuracy:1.0000.:  18%|█▊        | 58/320 [00:41<03:25,  1.27it/s]\u001b[A\n",
            "train loss:0.1439, train accuracy:1.0000.:  18%|█▊        | 59/320 [00:41<02:36,  1.67it/s]\u001b[A\n",
            "train loss:0.1383, train accuracy:1.0000.:  18%|█▊        | 59/320 [00:41<02:36,  1.67it/s]\u001b[A\n",
            "train loss:0.1383, train accuracy:1.0000.:  19%|█▉        | 60/320 [00:41<02:00,  2.15it/s]\u001b[A\n",
            "train loss:0.1322, train accuracy:1.0000.:  19%|█▉        | 60/320 [00:43<02:00,  2.15it/s]\u001b[A\n",
            "train loss:0.1322, train accuracy:1.0000.:  19%|█▉        | 61/320 [00:43<04:28,  1.04s/it]\u001b[A\n",
            "train loss:0.1309, train accuracy:1.0000.:  19%|█▉        | 61/320 [00:44<04:28,  1.04s/it]\u001b[A\n",
            "train loss:0.1309, train accuracy:1.0000.:  19%|█▉        | 62/320 [00:44<03:20,  1.29it/s]\u001b[A\n",
            "train loss:0.1290, train accuracy:1.0000.:  19%|█▉        | 62/320 [00:44<03:20,  1.29it/s]\u001b[A\n",
            "train loss:0.1290, train accuracy:1.0000.:  20%|█▉        | 63/320 [00:44<02:32,  1.69it/s]\u001b[A\n",
            "train loss:0.1243, train accuracy:1.0000.:  20%|█▉        | 63/320 [00:46<02:32,  1.69it/s]\u001b[A\n",
            "train loss:0.1243, train accuracy:1.0000.:  20%|██        | 64/320 [00:46<04:07,  1.03it/s]\u001b[A\n",
            "train loss:0.1178, train accuracy:1.0000.:  20%|██        | 64/320 [00:47<04:07,  1.03it/s]\u001b[A\n",
            "train loss:0.1178, train accuracy:1.0000.:  20%|██        | 65/320 [00:47<04:08,  1.03it/s]\u001b[A\n",
            "train loss:0.1123, train accuracy:1.0000.:  20%|██        | 65/320 [00:47<04:08,  1.03it/s]\u001b[A\n",
            "train loss:0.1123, train accuracy:1.0000.:  21%|██        | 66/320 [00:47<03:03,  1.38it/s]\u001b[A\n",
            "train loss:0.1202, train accuracy:1.0000.:  21%|██        | 66/320 [00:48<03:03,  1.38it/s]\u001b[A\n",
            "train loss:0.1202, train accuracy:1.0000.:  21%|██        | 67/320 [00:48<04:06,  1.02it/s]\u001b[A\n",
            "train loss:0.1070, train accuracy:1.0000.:  21%|██        | 67/320 [00:48<04:06,  1.02it/s]\u001b[A\n",
            "train loss:0.1070, train accuracy:1.0000.:  21%|██▏       | 68/320 [00:49<03:02,  1.38it/s]\u001b[A\n",
            "train loss:0.1100, train accuracy:1.0000.:  21%|██▏       | 68/320 [00:49<03:02,  1.38it/s]\u001b[A\n",
            "train loss:0.1100, train accuracy:1.0000.:  22%|██▏       | 69/320 [00:49<02:17,  1.82it/s]\u001b[A\n",
            "train loss:0.1009, train accuracy:1.0000.:  22%|██▏       | 69/320 [00:50<02:17,  1.82it/s]\u001b[A\n",
            "train loss:0.1009, train accuracy:1.0000.:  22%|██▏       | 70/320 [00:50<03:26,  1.21it/s]\u001b[A\n",
            "train loss:0.1006, train accuracy:1.0000.:  22%|██▏       | 70/320 [00:50<03:26,  1.21it/s]\u001b[A\n",
            "train loss:0.1006, train accuracy:1.0000.:  22%|██▏       | 71/320 [00:50<02:34,  1.61it/s]\u001b[A\n",
            "train loss:0.1009, train accuracy:1.0000.:  22%|██▏       | 71/320 [00:50<02:34,  1.61it/s]\u001b[A\n",
            "train loss:0.1009, train accuracy:1.0000.:  22%|██▎       | 72/320 [00:50<01:57,  2.11it/s]\u001b[A\n",
            "train loss:0.0987, train accuracy:1.0000.:  22%|██▎       | 72/320 [00:52<01:57,  2.11it/s]\u001b[A\n",
            "train loss:0.0987, train accuracy:1.0000.:  23%|██▎       | 73/320 [00:52<03:11,  1.29it/s]\u001b[A\n",
            "train loss:0.0972, train accuracy:1.0000.:  23%|██▎       | 73/320 [00:52<03:11,  1.29it/s]\u001b[A\n",
            "train loss:0.0972, train accuracy:1.0000.:  23%|██▎       | 74/320 [00:52<02:24,  1.71it/s]\u001b[A\n",
            "train loss:0.0969, train accuracy:1.0000.:  23%|██▎       | 74/320 [00:52<02:24,  1.71it/s]\u001b[A\n",
            "train loss:0.0969, train accuracy:1.0000.:  23%|██▎       | 75/320 [00:52<01:50,  2.22it/s]\u001b[A\n",
            "train loss:0.0902, train accuracy:1.0000.:  23%|██▎       | 75/320 [00:54<01:50,  2.22it/s]\u001b[A\n",
            "train loss:0.0902, train accuracy:1.0000.:  24%|██▍       | 76/320 [00:54<03:30,  1.16it/s]\u001b[A\n",
            "train loss:0.0843, train accuracy:1.0000.:  24%|██▍       | 76/320 [00:54<03:30,  1.16it/s]\u001b[A\n",
            "train loss:0.0843, train accuracy:1.0000.:  24%|██▍       | 77/320 [00:54<02:37,  1.54it/s]\u001b[A\n",
            "train loss:0.0898, train accuracy:1.0000.:  24%|██▍       | 77/320 [00:54<02:37,  1.54it/s]\u001b[A\n",
            "train loss:0.0898, train accuracy:1.0000.:  24%|██▍       | 78/320 [00:54<02:00,  2.00it/s]\u001b[A\n",
            "train loss:0.0869, train accuracy:1.0000.:  24%|██▍       | 78/320 [00:57<02:00,  2.00it/s]\u001b[A\n",
            "train loss:0.0869, train accuracy:1.0000.:  25%|██▍       | 79/320 [00:57<05:04,  1.26s/it]\u001b[A\n",
            "train loss:0.0857, train accuracy:1.0000.:  25%|██▍       | 79/320 [00:57<05:04,  1.26s/it]\u001b[A\n",
            "train loss:0.0857, train accuracy:1.0000.:  25%|██▌       | 80/320 [00:57<03:41,  1.09it/s]\u001b[A\n",
            "train loss:0.0811, train accuracy:1.0000.:  25%|██▌       | 80/320 [00:58<03:41,  1.09it/s]\u001b[A\n",
            "train loss:0.0811, train accuracy:1.0000.:  25%|██▌       | 81/320 [00:58<03:47,  1.05it/s]\u001b[A\n",
            "train loss:0.0806, train accuracy:1.0000.:  25%|██▌       | 81/320 [01:00<03:47,  1.05it/s]\u001b[A\n",
            "train loss:0.0806, train accuracy:1.0000.:  26%|██▌       | 82/320 [01:00<04:25,  1.11s/it]\u001b[A\n",
            "train loss:0.0823, train accuracy:1.0000.:  26%|██▌       | 82/320 [01:00<04:25,  1.11s/it]\u001b[A\n",
            "train loss:0.0823, train accuracy:1.0000.:  26%|██▌       | 83/320 [01:00<03:15,  1.21it/s]\u001b[A\n",
            "train loss:0.0830, train accuracy:1.0000.:  26%|██▌       | 83/320 [01:00<03:15,  1.21it/s]\u001b[A\n",
            "train loss:0.0830, train accuracy:1.0000.:  26%|██▋       | 84/320 [01:00<02:26,  1.61it/s]\u001b[A\n",
            "train loss:0.0798, train accuracy:1.0000.:  26%|██▋       | 84/320 [01:02<02:26,  1.61it/s]\u001b[A\n",
            "train loss:0.0798, train accuracy:1.0000.:  27%|██▋       | 85/320 [01:02<03:25,  1.14it/s]\u001b[A\n",
            "train loss:0.0767, train accuracy:1.0000.:  27%|██▋       | 85/320 [01:02<03:25,  1.14it/s]\u001b[A\n",
            "train loss:0.0767, train accuracy:1.0000.:  27%|██▋       | 86/320 [01:02<02:33,  1.53it/s]\u001b[A\n",
            "train loss:0.0757, train accuracy:1.0000.:  27%|██▋       | 86/320 [01:02<02:33,  1.53it/s]\u001b[A\n",
            "train loss:0.0757, train accuracy:1.0000.:  27%|██▋       | 87/320 [01:02<01:56,  2.00it/s]\u001b[A\n",
            "train loss:0.0737, train accuracy:1.0000.:  27%|██▋       | 87/320 [01:03<01:56,  2.00it/s]\u001b[A\n",
            "train loss:0.0737, train accuracy:1.0000.:  28%|██▊       | 88/320 [01:03<03:03,  1.26it/s]\u001b[A\n",
            "train loss:0.0747, train accuracy:1.0000.:  28%|██▊       | 88/320 [01:04<03:03,  1.26it/s]\u001b[A\n",
            "train loss:0.0747, train accuracy:1.0000.:  28%|██▊       | 89/320 [01:04<02:17,  1.68it/s]\u001b[A\n",
            "train loss:0.0765, train accuracy:1.0000.:  28%|██▊       | 89/320 [01:04<02:17,  1.68it/s]\u001b[A\n",
            "train loss:0.0765, train accuracy:1.0000.:  28%|██▊       | 90/320 [01:04<01:45,  2.18it/s]\u001b[A\n",
            "train loss:0.0704, train accuracy:1.0000.:  28%|██▊       | 90/320 [01:05<01:45,  2.18it/s]\u001b[A\n",
            "train loss:0.0704, train accuracy:1.0000.:  28%|██▊       | 91/320 [01:05<02:53,  1.32it/s]\u001b[A\n",
            "train loss:0.0718, train accuracy:1.0000.:  28%|██▊       | 91/320 [01:05<02:53,  1.32it/s]\u001b[A\n",
            "train loss:0.0718, train accuracy:1.0000.:  29%|██▉       | 92/320 [01:05<02:10,  1.75it/s]\u001b[A\n",
            "train loss:0.0712, train accuracy:1.0000.:  29%|██▉       | 92/320 [01:05<02:10,  1.75it/s]\u001b[A\n",
            "train loss:0.0712, train accuracy:1.0000.:  29%|██▉       | 93/320 [01:05<01:40,  2.25it/s]\u001b[A\n",
            "train loss:0.0734, train accuracy:1.0000.:  29%|██▉       | 93/320 [01:07<01:40,  2.25it/s]\u001b[A\n",
            "train loss:0.0734, train accuracy:1.0000.:  29%|██▉       | 94/320 [01:07<02:59,  1.26it/s]\u001b[A\n",
            "train loss:0.0763, train accuracy:1.0000.:  29%|██▉       | 94/320 [01:07<02:59,  1.26it/s]\u001b[A\n",
            "train loss:0.0763, train accuracy:1.0000.:  30%|██▉       | 95/320 [01:07<02:16,  1.65it/s]\u001b[A\n",
            "train loss:0.0737, train accuracy:1.0000.:  30%|██▉       | 95/320 [01:07<02:16,  1.65it/s]\u001b[A\n",
            "train loss:0.0737, train accuracy:1.0000.:  30%|███       | 96/320 [01:07<01:43,  2.16it/s]\u001b[A\n",
            "train loss:0.0635, train accuracy:1.0000.:  30%|███       | 96/320 [01:10<01:43,  2.16it/s]\u001b[A\n",
            "train loss:0.0635, train accuracy:1.0000.:  30%|███       | 97/320 [01:10<04:25,  1.19s/it]\u001b[A\n",
            "train loss:0.0640, train accuracy:1.0000.:  30%|███       | 97/320 [01:10<04:25,  1.19s/it]\u001b[A\n",
            "train loss:0.0640, train accuracy:1.0000.:  31%|███       | 98/320 [01:10<03:14,  1.14it/s]\u001b[A\n",
            "train loss:0.0613, train accuracy:1.0000.:  31%|███       | 98/320 [01:11<03:14,  1.14it/s]\u001b[A\n",
            "train loss:0.0613, train accuracy:1.0000.:  31%|███       | 99/320 [01:11<02:24,  1.53it/s]\u001b[A\n",
            "train loss:0.0632, train accuracy:1.0000.:  31%|███       | 99/320 [01:12<02:24,  1.53it/s]\u001b[A\n",
            "train loss:0.0632, train accuracy:1.0000.:  31%|███▏      | 100/320 [01:12<03:17,  1.11it/s]\u001b[A\n",
            "train loss:0.0637, train accuracy:1.0000.:  31%|███▏      | 100/320 [01:12<03:17,  1.11it/s]\u001b[A\n",
            "train loss:0.0637, train accuracy:1.0000.:  32%|███▏      | 101/320 [01:12<02:27,  1.49it/s]\u001b[A\n",
            "train loss:0.0631, train accuracy:1.0000.:  32%|███▏      | 101/320 [01:12<02:27,  1.49it/s]\u001b[A\n",
            "train loss:0.0631, train accuracy:1.0000.:  32%|███▏      | 102/320 [01:12<01:51,  1.95it/s]\u001b[A\n",
            "train loss:0.0621, train accuracy:1.0000.:  32%|███▏      | 102/320 [01:14<01:51,  1.95it/s]\u001b[A\n",
            "train loss:0.0621, train accuracy:1.0000.:  32%|███▏      | 103/320 [01:14<03:03,  1.18it/s]\u001b[A\n",
            "train loss:0.0588, train accuracy:1.0000.:  32%|███▏      | 103/320 [01:14<03:03,  1.18it/s]\u001b[A\n",
            "train loss:0.0588, train accuracy:1.0000.:  32%|███▎      | 104/320 [01:14<02:31,  1.42it/s]\u001b[A\n",
            "train loss:0.0623, train accuracy:1.0000.:  32%|███▎      | 104/320 [01:14<02:31,  1.42it/s]\u001b[A\n",
            "train loss:0.0623, train accuracy:1.0000.:  33%|███▎      | 105/320 [01:14<01:58,  1.82it/s]\u001b[A\n",
            "train loss:0.0647, train accuracy:1.0000.:  33%|███▎      | 105/320 [01:16<01:58,  1.82it/s]\u001b[A\n",
            "train loss:0.0647, train accuracy:1.0000.:  33%|███▎      | 106/320 [01:16<03:28,  1.03it/s]\u001b[A\n",
            "train loss:0.0622, train accuracy:1.0000.:  33%|███▎      | 106/320 [01:17<03:28,  1.03it/s]\u001b[A\n",
            "train loss:0.0622, train accuracy:1.0000.:  33%|███▎      | 107/320 [01:17<02:48,  1.27it/s]\u001b[A\n",
            "train loss:0.0620, train accuracy:1.0000.:  33%|███▎      | 107/320 [01:17<02:48,  1.27it/s]\u001b[A\n",
            "train loss:0.0620, train accuracy:1.0000.:  34%|███▍      | 108/320 [01:17<02:13,  1.59it/s]\u001b[A\n",
            "train loss:0.0588, train accuracy:1.0000.:  34%|███▍      | 108/320 [01:19<02:13,  1.59it/s]\u001b[A\n",
            "train loss:0.0588, train accuracy:1.0000.:  34%|███▍      | 109/320 [01:19<03:12,  1.09it/s]\u001b[A\n",
            "train loss:0.0595, train accuracy:1.0000.:  34%|███▍      | 109/320 [01:19<03:12,  1.09it/s]\u001b[A\n",
            "train loss:0.0595, train accuracy:1.0000.:  34%|███▍      | 110/320 [01:19<02:24,  1.46it/s]\u001b[A\n",
            "train loss:0.0594, train accuracy:1.0000.:  34%|███▍      | 110/320 [01:19<02:24,  1.46it/s]\u001b[A\n",
            "train loss:0.0594, train accuracy:1.0000.:  35%|███▍      | 111/320 [01:19<01:50,  1.90it/s]\u001b[A\n",
            "train loss:0.0599, train accuracy:1.0000.:  35%|███▍      | 111/320 [01:21<01:50,  1.90it/s]\u001b[A\n",
            "train loss:0.0599, train accuracy:1.0000.:  35%|███▌      | 112/320 [01:21<03:31,  1.01s/it]\u001b[A\n",
            "train loss:0.0582, train accuracy:1.0000.:  35%|███▌      | 112/320 [01:22<03:31,  1.01s/it]\u001b[A\n",
            "train loss:0.0582, train accuracy:1.0000.:  35%|███▌      | 113/320 [01:22<03:32,  1.03s/it]\u001b[A\n",
            "train loss:0.0573, train accuracy:1.0000.:  35%|███▌      | 113/320 [01:22<03:32,  1.03s/it]\u001b[A\n",
            "train loss:0.0573, train accuracy:1.0000.:  36%|███▌      | 114/320 [01:22<02:37,  1.31it/s]\u001b[A\n",
            "train loss:0.0549, train accuracy:1.0000.:  36%|███▌      | 114/320 [01:24<02:37,  1.31it/s]\u001b[A\n",
            "train loss:0.0549, train accuracy:1.0000.:  36%|███▌      | 115/320 [01:24<03:21,  1.02it/s]\u001b[A\n",
            "train loss:0.0558, train accuracy:1.0000.:  36%|███▌      | 115/320 [01:24<03:21,  1.02it/s]\u001b[A\n",
            "train loss:0.0558, train accuracy:1.0000.:  36%|███▋      | 116/320 [01:24<02:29,  1.37it/s]\u001b[A\n",
            "train loss:0.0521, train accuracy:1.0000.:  36%|███▋      | 116/320 [01:24<02:29,  1.37it/s]\u001b[A\n",
            "train loss:0.0521, train accuracy:1.0000.:  37%|███▋      | 117/320 [01:24<01:52,  1.80it/s]\u001b[A\n",
            "train loss:0.0578, train accuracy:1.0000.:  37%|███▋      | 117/320 [01:26<01:52,  1.80it/s]\u001b[A\n",
            "train loss:0.0578, train accuracy:1.0000.:  37%|███▋      | 118/320 [01:26<02:47,  1.21it/s]\u001b[A\n",
            "train loss:0.0528, train accuracy:1.0000.:  37%|███▋      | 118/320 [01:26<02:47,  1.21it/s]\u001b[A\n",
            "train loss:0.0528, train accuracy:1.0000.:  37%|███▋      | 119/320 [01:26<02:05,  1.60it/s]\u001b[A\n",
            "train loss:0.0499, train accuracy:1.0000.:  37%|███▋      | 119/320 [01:26<02:05,  1.60it/s]\u001b[A\n",
            "train loss:0.0499, train accuracy:1.0000.:  38%|███▊      | 120/320 [01:26<01:35,  2.08it/s]\u001b[A\n",
            "train loss:0.0532, train accuracy:1.0000.:  38%|███▊      | 120/320 [01:27<01:35,  2.08it/s]\u001b[A\n",
            "train loss:0.0532, train accuracy:1.0000.:  38%|███▊      | 121/320 [01:27<02:35,  1.28it/s]\u001b[A\n",
            "train loss:0.0498, train accuracy:1.0000.:  38%|███▊      | 121/320 [01:27<02:35,  1.28it/s]\u001b[A\n",
            "train loss:0.0498, train accuracy:1.0000.:  38%|███▊      | 122/320 [01:27<01:56,  1.69it/s]\u001b[A\n",
            "train loss:0.0535, train accuracy:1.0000.:  38%|███▊      | 122/320 [01:28<01:56,  1.69it/s]\u001b[A\n",
            "train loss:0.0535, train accuracy:1.0000.:  38%|███▊      | 123/320 [01:28<01:29,  2.20it/s]\u001b[A\n",
            "train loss:0.0517, train accuracy:1.0000.:  38%|███▊      | 123/320 [01:29<01:29,  2.20it/s]\u001b[A\n",
            "train loss:0.0517, train accuracy:1.0000.:  39%|███▉      | 124/320 [01:29<02:28,  1.32it/s]\u001b[A\n",
            "train loss:0.0527, train accuracy:1.0000.:  39%|███▉      | 124/320 [01:29<02:28,  1.32it/s]\u001b[A\n",
            "train loss:0.0527, train accuracy:1.0000.:  39%|███▉      | 125/320 [01:29<01:51,  1.74it/s]\u001b[A\n",
            "train loss:0.0513, train accuracy:1.0000.:  39%|███▉      | 125/320 [01:29<01:51,  1.74it/s]\u001b[A\n",
            "train loss:0.0513, train accuracy:1.0000.:  39%|███▉      | 126/320 [01:29<01:25,  2.26it/s]\u001b[A\n",
            "train loss:0.0522, train accuracy:1.0000.:  39%|███▉      | 126/320 [01:31<01:25,  2.26it/s]\u001b[A\n",
            "train loss:0.0522, train accuracy:1.0000.:  40%|███▉      | 127/320 [01:31<02:25,  1.33it/s]\u001b[A\n",
            "train loss:0.0545, train accuracy:1.0000.:  40%|███▉      | 127/320 [01:31<02:25,  1.33it/s]\u001b[A\n",
            "train loss:0.0545, train accuracy:1.0000.:  40%|████      | 128/320 [01:31<01:48,  1.77it/s]\u001b[A\n",
            "train loss:0.0508, train accuracy:1.0000.:  40%|████      | 128/320 [01:32<01:48,  1.77it/s]\u001b[A\n",
            "train loss:0.0508, train accuracy:1.0000.:  40%|████      | 129/320 [01:32<02:42,  1.18it/s]\u001b[A\n",
            "train loss:0.0470, train accuracy:1.0000.:  40%|████      | 129/320 [01:34<02:42,  1.18it/s]\u001b[A\n",
            "train loss:0.0470, train accuracy:1.0000.:  41%|████      | 130/320 [01:34<03:32,  1.12s/it]\u001b[A\n",
            "train loss:0.0497, train accuracy:1.0000.:  41%|████      | 130/320 [01:34<03:32,  1.12s/it]\u001b[A\n",
            "train loss:0.0497, train accuracy:1.0000.:  41%|████      | 131/320 [01:34<02:36,  1.21it/s]\u001b[A\n",
            "train loss:0.0478, train accuracy:1.0000.:  41%|████      | 131/320 [01:35<02:36,  1.21it/s]\u001b[A\n",
            "train loss:0.0478, train accuracy:1.0000.:  41%|████▏     | 132/320 [01:35<01:56,  1.61it/s]\u001b[A\n",
            "train loss:0.0489, train accuracy:1.0000.:  41%|████▏     | 132/320 [01:36<01:56,  1.61it/s]\u001b[A\n",
            "train loss:0.0489, train accuracy:1.0000.:  42%|████▏     | 133/320 [01:36<02:45,  1.13it/s]\u001b[A\n",
            "train loss:0.0446, train accuracy:1.0000.:  42%|████▏     | 133/320 [01:36<02:45,  1.13it/s]\u001b[A\n",
            "train loss:0.0446, train accuracy:1.0000.:  42%|████▏     | 134/320 [01:36<02:03,  1.51it/s]\u001b[A\n",
            "train loss:0.0494, train accuracy:1.0000.:  42%|████▏     | 134/320 [01:36<02:03,  1.51it/s]\u001b[A\n",
            "train loss:0.0494, train accuracy:1.0000.:  42%|████▏     | 135/320 [01:36<01:33,  1.98it/s]\u001b[A\n",
            "train loss:0.0484, train accuracy:1.0000.:  42%|████▏     | 135/320 [01:38<01:33,  1.98it/s]\u001b[A\n",
            "train loss:0.0484, train accuracy:1.0000.:  42%|████▎     | 136/320 [01:38<02:27,  1.25it/s]\u001b[A\n",
            "train loss:0.0468, train accuracy:1.0000.:  42%|████▎     | 136/320 [01:38<02:27,  1.25it/s]\u001b[A\n",
            "train loss:0.0468, train accuracy:1.0000.:  43%|████▎     | 137/320 [01:38<01:50,  1.66it/s]\u001b[A\n",
            "train loss:0.0436, train accuracy:1.0000.:  43%|████▎     | 137/320 [01:38<01:50,  1.66it/s]\u001b[A\n",
            "train loss:0.0436, train accuracy:1.0000.:  43%|████▎     | 138/320 [01:38<01:24,  2.16it/s]\u001b[A\n",
            "train loss:0.0431, train accuracy:1.0000.:  43%|████▎     | 138/320 [01:40<01:24,  2.16it/s]\u001b[A\n",
            "train loss:0.0431, train accuracy:1.0000.:  43%|████▎     | 139/320 [01:40<02:17,  1.32it/s]\u001b[A\n",
            "train loss:0.0462, train accuracy:1.0000.:  43%|████▎     | 139/320 [01:40<02:17,  1.32it/s]\u001b[A\n",
            "train loss:0.0462, train accuracy:1.0000.:  44%|████▍     | 140/320 [01:40<01:43,  1.74it/s]\u001b[A\n",
            "train loss:0.0445, train accuracy:1.0000.:  44%|████▍     | 140/320 [01:40<01:43,  1.74it/s]\u001b[A\n",
            "train loss:0.0445, train accuracy:1.0000.:  44%|████▍     | 141/320 [01:40<01:19,  2.26it/s]\u001b[A\n",
            "train loss:0.0460, train accuracy:1.0000.:  44%|████▍     | 141/320 [01:41<01:19,  2.26it/s]\u001b[A\n",
            "train loss:0.0460, train accuracy:1.0000.:  44%|████▍     | 142/320 [01:41<02:13,  1.34it/s]\u001b[A\n",
            "train loss:0.0446, train accuracy:1.0000.:  44%|████▍     | 142/320 [01:41<02:13,  1.34it/s]\u001b[A\n",
            "train loss:0.0446, train accuracy:1.0000.:  45%|████▍     | 143/320 [01:41<01:40,  1.76it/s]\u001b[A\n",
            "train loss:0.0449, train accuracy:1.0000.:  45%|████▍     | 143/320 [01:42<01:40,  1.76it/s]\u001b[A\n",
            "train loss:0.0449, train accuracy:1.0000.:  45%|████▌     | 144/320 [01:42<01:16,  2.31it/s]\u001b[A\n",
            "train loss:0.0401, train accuracy:1.0000.:  45%|████▌     | 144/320 [01:44<01:16,  2.31it/s]\u001b[A\n",
            "train loss:0.0401, train accuracy:1.0000.:  45%|████▌     | 145/320 [01:44<03:08,  1.08s/it]\u001b[A\n",
            "train loss:0.0442, train accuracy:1.0000.:  45%|████▌     | 145/320 [01:44<03:08,  1.08s/it]\u001b[A\n",
            "train loss:0.0442, train accuracy:1.0000.:  46%|████▌     | 146/320 [01:44<02:20,  1.24it/s]\u001b[A\n",
            "train loss:0.0431, train accuracy:1.0000.:  46%|████▌     | 146/320 [01:44<02:20,  1.24it/s]\u001b[A\n",
            "train loss:0.0431, train accuracy:1.0000.:  46%|████▌     | 147/320 [01:44<01:45,  1.65it/s]\u001b[A\n",
            "train loss:0.0454, train accuracy:1.0000.:  46%|████▌     | 147/320 [01:46<01:45,  1.65it/s]\u001b[A\n",
            "train loss:0.0454, train accuracy:1.0000.:  46%|████▋     | 148/320 [01:46<02:53,  1.01s/it]\u001b[A\n",
            "train loss:0.0381, train accuracy:1.0000.:  46%|████▋     | 148/320 [01:47<02:53,  1.01s/it]\u001b[A\n",
            "train loss:0.0381, train accuracy:1.0000.:  47%|████▋     | 149/320 [01:47<02:08,  1.33it/s]\u001b[A\n",
            "train loss:0.0373, train accuracy:1.0000.:  47%|████▋     | 149/320 [01:47<02:08,  1.33it/s]\u001b[A\n",
            "train loss:0.0373, train accuracy:1.0000.:  47%|████▋     | 150/320 [01:47<01:36,  1.76it/s]\u001b[A\n",
            "train loss:0.0396, train accuracy:1.0000.:  47%|████▋     | 150/320 [01:48<01:36,  1.76it/s]\u001b[A\n",
            "train loss:0.0396, train accuracy:1.0000.:  47%|████▋     | 151/320 [01:48<02:29,  1.13it/s]\u001b[A\n",
            "train loss:0.0418, train accuracy:1.0000.:  47%|████▋     | 151/320 [01:48<02:29,  1.13it/s]\u001b[A\n",
            "train loss:0.0418, train accuracy:1.0000.:  48%|████▊     | 152/320 [01:48<01:51,  1.50it/s]\u001b[A\n",
            "train loss:0.0408, train accuracy:1.0000.:  48%|████▊     | 152/320 [01:49<01:51,  1.50it/s]\u001b[A\n",
            "train loss:0.0408, train accuracy:1.0000.:  48%|████▊     | 153/320 [01:49<01:24,  1.97it/s]\u001b[A\n",
            "train loss:0.0400, train accuracy:1.0000.:  48%|████▊     | 153/320 [01:50<01:24,  1.97it/s]\u001b[A\n",
            "train loss:0.0400, train accuracy:1.0000.:  48%|████▊     | 154/320 [01:50<02:12,  1.25it/s]\u001b[A\n",
            "train loss:0.0401, train accuracy:1.0000.:  48%|████▊     | 154/320 [01:50<02:12,  1.25it/s]\u001b[A\n",
            "train loss:0.0401, train accuracy:1.0000.:  48%|████▊     | 155/320 [01:50<01:39,  1.66it/s]\u001b[A\n",
            "train loss:0.0381, train accuracy:1.0000.:  48%|████▊     | 155/320 [01:50<01:39,  1.66it/s]\u001b[A\n",
            "train loss:0.0381, train accuracy:1.0000.:  49%|████▉     | 156/320 [01:50<01:15,  2.16it/s]\u001b[A\n",
            "train loss:0.0407, train accuracy:1.0000.:  49%|████▉     | 156/320 [01:52<01:15,  2.16it/s]\u001b[A\n",
            "train loss:0.0407, train accuracy:1.0000.:  49%|████▉     | 157/320 [01:52<02:05,  1.30it/s]\u001b[A\n",
            "train loss:0.0396, train accuracy:1.0000.:  49%|████▉     | 157/320 [01:52<02:05,  1.30it/s]\u001b[A\n",
            "train loss:0.0396, train accuracy:1.0000.:  49%|████▉     | 158/320 [01:52<01:34,  1.72it/s]\u001b[A\n",
            "train loss:0.0401, train accuracy:1.0000.:  49%|████▉     | 158/320 [01:52<01:34,  1.72it/s]\u001b[A\n",
            "train loss:0.0401, train accuracy:1.0000.:  50%|████▉     | 159/320 [01:52<01:12,  2.22it/s]\u001b[A\n",
            "train loss:0.0419, train accuracy:1.0000.:  50%|████▉     | 159/320 [01:54<01:12,  2.22it/s]\u001b[A\n",
            "train loss:0.0419, train accuracy:1.0000.:  50%|█████     | 160/320 [01:54<02:00,  1.33it/s]\u001b[A\n",
            "train loss:0.0365, train accuracy:1.0000.:  50%|█████     | 160/320 [01:55<02:00,  1.33it/s]\u001b[A\n",
            "train loss:0.0365, train accuracy:1.0000.:  50%|█████     | 161/320 [01:55<02:12,  1.20it/s]\u001b[A\n",
            "train loss:0.0349, train accuracy:1.0000.:  50%|█████     | 161/320 [01:55<02:12,  1.20it/s]\u001b[A\n",
            "train loss:0.0349, train accuracy:1.0000.:  51%|█████     | 162/320 [01:55<01:38,  1.60it/s]\u001b[A\n",
            "train loss:0.0356, train accuracy:1.0000.:  51%|█████     | 162/320 [01:56<01:38,  1.60it/s]\u001b[A\n",
            "train loss:0.0356, train accuracy:1.0000.:  51%|█████     | 163/320 [01:56<02:24,  1.09it/s]\u001b[A\n",
            "train loss:0.0366, train accuracy:1.0000.:  51%|█████     | 163/320 [01:56<02:24,  1.09it/s]\u001b[A\n",
            "train loss:0.0366, train accuracy:1.0000.:  51%|█████▏    | 164/320 [01:57<01:48,  1.44it/s]\u001b[A\n",
            "train loss:0.0347, train accuracy:1.0000.:  51%|█████▏    | 164/320 [01:57<01:48,  1.44it/s]\u001b[A\n",
            "train loss:0.0347, train accuracy:1.0000.:  52%|█████▏    | 165/320 [01:57<01:22,  1.88it/s]\u001b[A\n",
            "train loss:0.0376, train accuracy:1.0000.:  52%|█████▏    | 165/320 [01:59<01:22,  1.88it/s]\u001b[A\n",
            "train loss:0.0376, train accuracy:1.0000.:  52%|█████▏    | 166/320 [01:59<02:28,  1.04it/s]\u001b[A\n",
            "train loss:0.0335, train accuracy:1.0000.:  52%|█████▏    | 166/320 [01:59<02:28,  1.04it/s]\u001b[A\n",
            "train loss:0.0335, train accuracy:1.0000.:  52%|█████▏    | 167/320 [01:59<01:49,  1.40it/s]\u001b[A\n",
            "train loss:0.0360, train accuracy:1.0000.:  52%|█████▏    | 167/320 [01:59<01:49,  1.40it/s]\u001b[A\n",
            "train loss:0.0360, train accuracy:1.0000.:  52%|█████▎    | 168/320 [01:59<01:22,  1.83it/s]\u001b[A\n",
            "train loss:0.0342, train accuracy:1.0000.:  52%|█████▎    | 168/320 [02:00<01:22,  1.83it/s]\u001b[A\n",
            "train loss:0.0342, train accuracy:1.0000.:  53%|█████▎    | 169/320 [02:00<02:03,  1.22it/s]\u001b[A\n",
            "train loss:0.0353, train accuracy:1.0000.:  53%|█████▎    | 169/320 [02:01<02:03,  1.22it/s]\u001b[A\n",
            "train loss:0.0353, train accuracy:1.0000.:  53%|█████▎    | 170/320 [02:01<01:32,  1.62it/s]\u001b[A\n",
            "train loss:0.0335, train accuracy:1.0000.:  53%|█████▎    | 170/320 [02:01<01:32,  1.62it/s]\u001b[A\n",
            "train loss:0.0335, train accuracy:1.0000.:  53%|█████▎    | 171/320 [02:01<01:10,  2.11it/s]\u001b[A\n",
            "train loss:0.0338, train accuracy:1.0000.:  53%|█████▎    | 171/320 [02:02<01:10,  2.11it/s]\u001b[A\n",
            "train loss:0.0338, train accuracy:1.0000.:  54%|█████▍    | 172/320 [02:02<01:54,  1.29it/s]\u001b[A\n",
            "train loss:0.0341, train accuracy:1.0000.:  54%|█████▍    | 172/320 [02:02<01:54,  1.29it/s]\u001b[A\n",
            "train loss:0.0341, train accuracy:1.0000.:  54%|█████▍    | 173/320 [02:02<01:26,  1.71it/s]\u001b[A\n",
            "train loss:0.0352, train accuracy:1.0000.:  54%|█████▍    | 173/320 [02:02<01:26,  1.71it/s]\u001b[A\n",
            "train loss:0.0352, train accuracy:1.0000.:  54%|█████▍    | 174/320 [02:02<01:05,  2.21it/s]\u001b[A\n",
            "train loss:0.0369, train accuracy:1.0000.:  54%|█████▍    | 174/320 [02:04<01:05,  2.21it/s]\u001b[A\n",
            "train loss:0.0369, train accuracy:1.0000.:  55%|█████▍    | 175/320 [02:04<01:51,  1.30it/s]\u001b[A\n",
            "train loss:0.0333, train accuracy:1.0000.:  55%|█████▍    | 175/320 [02:04<01:51,  1.30it/s]\u001b[A\n",
            "train loss:0.0333, train accuracy:1.0000.:  55%|█████▌    | 176/320 [02:04<01:22,  1.74it/s]\u001b[A\n",
            "train loss:0.0328, train accuracy:1.0000.:  55%|█████▌    | 176/320 [02:05<01:22,  1.74it/s]\u001b[A\n",
            "train loss:0.0328, train accuracy:1.0000.:  55%|█████▌    | 177/320 [02:05<01:45,  1.36it/s]\u001b[A\n",
            "train loss:0.0339, train accuracy:1.0000.:  55%|█████▌    | 177/320 [02:07<01:45,  1.36it/s]\u001b[A\n",
            "train loss:0.0339, train accuracy:1.0000.:  56%|█████▌    | 178/320 [02:07<02:17,  1.04it/s]\u001b[A\n",
            "train loss:0.0337, train accuracy:1.0000.:  56%|█████▌    | 178/320 [02:07<02:17,  1.04it/s]\u001b[A\n",
            "train loss:0.0337, train accuracy:1.0000.:  56%|█████▌    | 179/320 [02:07<01:41,  1.39it/s]\u001b[A\n",
            "train loss:0.0323, train accuracy:1.0000.:  56%|█████▌    | 179/320 [02:07<01:41,  1.39it/s]\u001b[A\n",
            "train loss:0.0323, train accuracy:1.0000.:  56%|█████▋    | 180/320 [02:07<01:16,  1.83it/s]\u001b[A\n",
            "train loss:0.0321, train accuracy:1.0000.:  56%|█████▋    | 180/320 [02:09<01:16,  1.83it/s]\u001b[A\n",
            "train loss:0.0321, train accuracy:1.0000.:  57%|█████▋    | 181/320 [02:09<01:58,  1.17it/s]\u001b[A\n",
            "train loss:0.0325, train accuracy:1.0000.:  57%|█████▋    | 181/320 [02:09<01:58,  1.17it/s]\u001b[A\n",
            "train loss:0.0325, train accuracy:1.0000.:  57%|█████▋    | 182/320 [02:09<01:28,  1.55it/s]\u001b[A\n",
            "train loss:0.0311, train accuracy:1.0000.:  57%|█████▋    | 182/320 [02:09<01:28,  1.55it/s]\u001b[A\n",
            "train loss:0.0311, train accuracy:1.0000.:  57%|█████▋    | 183/320 [02:09<01:08,  2.01it/s]\u001b[A\n",
            "train loss:0.0333, train accuracy:1.0000.:  57%|█████▋    | 183/320 [02:11<01:08,  2.01it/s]\u001b[A\n",
            "train loss:0.0333, train accuracy:1.0000.:  57%|█████▊    | 184/320 [02:11<02:12,  1.03it/s]\u001b[A\n",
            "train loss:0.0322, train accuracy:1.0000.:  57%|█████▊    | 184/320 [02:11<02:12,  1.03it/s]\u001b[A\n",
            "train loss:0.0322, train accuracy:1.0000.:  58%|█████▊    | 185/320 [02:11<01:37,  1.38it/s]\u001b[A\n",
            "train loss:0.0321, train accuracy:1.0000.:  58%|█████▊    | 185/320 [02:11<01:37,  1.38it/s]\u001b[A\n",
            "train loss:0.0321, train accuracy:1.0000.:  58%|█████▊    | 186/320 [02:11<01:13,  1.82it/s]\u001b[A\n",
            "train loss:0.0312, train accuracy:1.0000.:  58%|█████▊    | 186/320 [02:13<01:13,  1.82it/s]\u001b[A\n",
            "train loss:0.0312, train accuracy:1.0000.:  58%|█████▊    | 187/320 [02:13<01:50,  1.21it/s]\u001b[A\n",
            "train loss:0.0314, train accuracy:1.0000.:  58%|█████▊    | 187/320 [02:13<01:50,  1.21it/s]\u001b[A\n",
            "train loss:0.0314, train accuracy:1.0000.:  59%|█████▉    | 188/320 [02:13<01:22,  1.61it/s]\u001b[A\n",
            "train loss:0.0334, train accuracy:1.0000.:  59%|█████▉    | 188/320 [02:13<01:22,  1.61it/s]\u001b[A\n",
            "train loss:0.0334, train accuracy:1.0000.:  59%|█████▉    | 189/320 [02:13<01:02,  2.09it/s]\u001b[A\n",
            "train loss:0.0312, train accuracy:1.0000.:  59%|█████▉    | 189/320 [02:14<01:02,  2.09it/s]\u001b[A\n",
            "train loss:0.0312, train accuracy:1.0000.:  59%|█████▉    | 190/320 [02:14<01:41,  1.28it/s]\u001b[A\n",
            "train loss:0.0318, train accuracy:1.0000.:  59%|█████▉    | 190/320 [02:15<01:41,  1.28it/s]\u001b[A\n",
            "train loss:0.0318, train accuracy:1.0000.:  60%|█████▉    | 191/320 [02:15<01:16,  1.69it/s]\u001b[A\n",
            "train loss:0.0283, train accuracy:1.0000.:  60%|█████▉    | 191/320 [02:15<01:16,  1.69it/s]\u001b[A\n",
            "train loss:0.0283, train accuracy:1.0000.:  60%|██████    | 192/320 [02:15<00:57,  2.23it/s]\u001b[A\n",
            "train loss:0.0262, train accuracy:1.0000.:  60%|██████    | 192/320 [02:17<00:57,  2.23it/s]\u001b[A\n",
            "train loss:0.0262, train accuracy:1.0000.:  60%|██████    | 193/320 [02:17<02:22,  1.12s/it]\u001b[A\n",
            "train loss:0.0272, train accuracy:1.0000.:  60%|██████    | 193/320 [02:18<02:22,  1.12s/it]\u001b[A\n",
            "train loss:0.0272, train accuracy:1.0000.:  61%|██████    | 194/320 [02:18<01:44,  1.21it/s]\u001b[A\n",
            "train loss:0.0299, train accuracy:1.0000.:  61%|██████    | 194/320 [02:18<01:44,  1.21it/s]\u001b[A\n",
            "train loss:0.0299, train accuracy:1.0000.:  61%|██████    | 195/320 [02:18<01:18,  1.59it/s]\u001b[A\n",
            "train loss:0.0261, train accuracy:1.0000.:  61%|██████    | 195/320 [02:20<01:18,  1.59it/s]\u001b[A\n",
            "train loss:0.0261, train accuracy:1.0000.:  61%|██████▏   | 196/320 [02:20<02:08,  1.03s/it]\u001b[A\n",
            "train loss:0.0310, train accuracy:1.0000.:  61%|██████▏   | 196/320 [02:20<02:08,  1.03s/it]\u001b[A\n",
            "train loss:0.0310, train accuracy:1.0000.:  62%|██████▏   | 197/320 [02:20<01:34,  1.30it/s]\u001b[A\n",
            "train loss:0.0288, train accuracy:1.0000.:  62%|██████▏   | 197/320 [02:20<01:34,  1.30it/s]\u001b[A\n",
            "train loss:0.0288, train accuracy:1.0000.:  62%|██████▏   | 198/320 [02:20<01:10,  1.72it/s]\u001b[A\n",
            "train loss:0.0274, train accuracy:1.0000.:  62%|██████▏   | 198/320 [02:22<01:10,  1.72it/s]\u001b[A\n",
            "train loss:0.0274, train accuracy:1.0000.:  62%|██████▏   | 199/320 [02:22<01:55,  1.05it/s]\u001b[A\n",
            "train loss:0.0270, train accuracy:1.0000.:  62%|██████▏   | 199/320 [02:22<01:55,  1.05it/s]\u001b[A\n",
            "train loss:0.0270, train accuracy:1.0000.:  62%|██████▎   | 200/320 [02:22<01:26,  1.38it/s]\u001b[A\n",
            "train loss:0.0259, train accuracy:1.0000.:  62%|██████▎   | 200/320 [02:22<01:26,  1.38it/s]\u001b[A\n",
            "train loss:0.0259, train accuracy:1.0000.:  63%|██████▎   | 201/320 [02:22<01:05,  1.81it/s]\u001b[A\n",
            "train loss:0.0314, train accuracy:1.0000.:  63%|██████▎   | 201/320 [02:24<01:05,  1.81it/s]\u001b[A\n",
            "train loss:0.0314, train accuracy:1.0000.:  63%|██████▎   | 202/320 [02:24<01:48,  1.09it/s]\u001b[A\n",
            "train loss:0.0293, train accuracy:1.0000.:  63%|██████▎   | 202/320 [02:24<01:48,  1.09it/s]\u001b[A\n",
            "train loss:0.0293, train accuracy:1.0000.:  63%|██████▎   | 203/320 [02:24<01:20,  1.46it/s]\u001b[A\n",
            "train loss:0.0285, train accuracy:1.0000.:  63%|██████▎   | 203/320 [02:24<01:20,  1.46it/s]\u001b[A\n",
            "train loss:0.0285, train accuracy:1.0000.:  64%|██████▍   | 204/320 [02:24<01:00,  1.91it/s]\u001b[A\n",
            "train loss:0.0270, train accuracy:1.0000.:  64%|██████▍   | 204/320 [02:26<01:00,  1.91it/s]\u001b[A\n",
            "train loss:0.0270, train accuracy:1.0000.:  64%|██████▍   | 205/320 [02:26<01:33,  1.23it/s]\u001b[A\n",
            "train loss:0.0284, train accuracy:1.0000.:  64%|██████▍   | 205/320 [02:26<01:33,  1.23it/s]\u001b[A\n",
            "train loss:0.0284, train accuracy:1.0000.:  64%|██████▍   | 206/320 [02:26<01:09,  1.63it/s]\u001b[A\n",
            "train loss:0.0280, train accuracy:1.0000.:  64%|██████▍   | 206/320 [02:26<01:09,  1.63it/s]\u001b[A\n",
            "train loss:0.0280, train accuracy:1.0000.:  65%|██████▍   | 207/320 [02:26<00:53,  2.12it/s]\u001b[A\n",
            "train loss:0.0289, train accuracy:1.0000.:  65%|██████▍   | 207/320 [02:27<00:53,  2.12it/s]\u001b[A\n",
            "train loss:0.0289, train accuracy:1.0000.:  65%|██████▌   | 208/320 [02:27<01:26,  1.29it/s]\u001b[A\n",
            "train loss:0.0253, train accuracy:1.0000.:  65%|██████▌   | 208/320 [02:29<01:26,  1.29it/s]\u001b[A\n",
            "train loss:0.0253, train accuracy:1.0000.:  65%|██████▌   | 209/320 [02:29<01:35,  1.16it/s]\u001b[A\n",
            "train loss:0.0294, train accuracy:1.0000.:  65%|██████▌   | 209/320 [02:29<01:35,  1.16it/s]\u001b[A\n",
            "train loss:0.0294, train accuracy:1.0000.:  66%|██████▌   | 210/320 [02:29<01:11,  1.54it/s]\u001b[A\n",
            "train loss:0.0251, train accuracy:1.0000.:  66%|██████▌   | 210/320 [02:30<01:11,  1.54it/s]\u001b[A\n",
            "train loss:0.0251, train accuracy:1.0000.:  66%|██████▌   | 211/320 [02:30<01:38,  1.11it/s]\u001b[A\n",
            "train loss:0.0249, train accuracy:1.0000.:  66%|██████▌   | 211/320 [02:30<01:38,  1.11it/s]\u001b[A\n",
            "train loss:0.0249, train accuracy:1.0000.:  66%|██████▋   | 212/320 [02:30<01:13,  1.48it/s]\u001b[A\n",
            "train loss:0.0231, train accuracy:1.0000.:  66%|██████▋   | 212/320 [02:30<01:13,  1.48it/s]\u001b[A\n",
            "train loss:0.0231, train accuracy:1.0000.:  67%|██████▋   | 213/320 [02:30<00:55,  1.94it/s]\u001b[A\n",
            "train loss:0.0281, train accuracy:1.0000.:  67%|██████▋   | 213/320 [02:32<00:55,  1.94it/s]\u001b[A\n",
            "train loss:0.0281, train accuracy:1.0000.:  67%|██████▋   | 214/320 [02:32<01:25,  1.24it/s]\u001b[A\n",
            "train loss:0.0246, train accuracy:1.0000.:  67%|██████▋   | 214/320 [02:32<01:25,  1.24it/s]\u001b[A\n",
            "train loss:0.0246, train accuracy:1.0000.:  67%|██████▋   | 215/320 [02:32<01:04,  1.64it/s]\u001b[A\n",
            "train loss:0.0239, train accuracy:1.0000.:  67%|██████▋   | 215/320 [02:32<01:04,  1.64it/s]\u001b[A\n",
            "train loss:0.0239, train accuracy:1.0000.:  68%|██████▊   | 216/320 [02:32<00:49,  2.12it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  68%|██████▊   | 216/320 [02:34<00:49,  2.12it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  68%|██████▊   | 217/320 [02:34<01:26,  1.19it/s]\u001b[A\n",
            "train loss:0.0271, train accuracy:1.0000.:  68%|██████▊   | 217/320 [02:34<01:26,  1.19it/s]\u001b[A\n",
            "train loss:0.0271, train accuracy:1.0000.:  68%|██████▊   | 218/320 [02:34<01:05,  1.56it/s]\u001b[A\n",
            "train loss:0.0227, train accuracy:1.0000.:  68%|██████▊   | 218/320 [02:34<01:05,  1.56it/s]\u001b[A\n",
            "train loss:0.0227, train accuracy:1.0000.:  68%|██████▊   | 219/320 [02:34<00:49,  2.02it/s]\u001b[A\n",
            "train loss:0.0261, train accuracy:1.0000.:  68%|██████▊   | 219/320 [02:36<00:49,  2.02it/s]\u001b[A\n",
            "train loss:0.0261, train accuracy:1.0000.:  69%|██████▉   | 220/320 [02:36<01:31,  1.09it/s]\u001b[A\n",
            "train loss:0.0233, train accuracy:1.0000.:  69%|██████▉   | 220/320 [02:36<01:31,  1.09it/s]\u001b[A\n",
            "train loss:0.0233, train accuracy:1.0000.:  69%|██████▉   | 221/320 [02:36<01:07,  1.46it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  69%|██████▉   | 221/320 [02:36<01:07,  1.46it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  69%|██████▉   | 222/320 [02:36<00:51,  1.91it/s]\u001b[A\n",
            "train loss:0.0242, train accuracy:1.0000.:  69%|██████▉   | 222/320 [02:38<00:51,  1.91it/s]\u001b[A\n",
            "train loss:0.0242, train accuracy:1.0000.:  70%|██████▉   | 223/320 [02:38<01:18,  1.24it/s]\u001b[A\n",
            "train loss:0.0241, train accuracy:1.0000.:  70%|██████▉   | 223/320 [02:38<01:18,  1.24it/s]\u001b[A\n",
            "train loss:0.0241, train accuracy:1.0000.:  70%|███████   | 224/320 [02:38<00:57,  1.66it/s]\u001b[A\n",
            "train loss:0.0213, train accuracy:1.0000.:  70%|███████   | 224/320 [02:39<00:57,  1.66it/s]\u001b[A\n",
            "train loss:0.0213, train accuracy:1.0000.:  70%|███████   | 225/320 [02:39<01:10,  1.35it/s]\u001b[A\n",
            "train loss:0.0212, train accuracy:1.0000.:  70%|███████   | 225/320 [02:41<01:10,  1.35it/s]\u001b[A\n",
            "train loss:0.0212, train accuracy:1.0000.:  71%|███████   | 226/320 [02:41<01:30,  1.03it/s]\u001b[A\n",
            "train loss:0.0221, train accuracy:1.0000.:  71%|███████   | 226/320 [02:41<01:30,  1.03it/s]\u001b[A\n",
            "train loss:0.0221, train accuracy:1.0000.:  71%|███████   | 227/320 [02:41<01:07,  1.39it/s]\u001b[A\n",
            "train loss:0.0208, train accuracy:1.0000.:  71%|███████   | 227/320 [02:41<01:07,  1.39it/s]\u001b[A\n",
            "train loss:0.0208, train accuracy:1.0000.:  71%|███████▏  | 228/320 [02:41<00:50,  1.82it/s]\u001b[A\n",
            "train loss:0.0226, train accuracy:1.0000.:  71%|███████▏  | 228/320 [02:42<00:50,  1.82it/s]\u001b[A\n",
            "train loss:0.0226, train accuracy:1.0000.:  72%|███████▏  | 229/320 [02:42<01:15,  1.20it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  72%|███████▏  | 229/320 [02:43<01:15,  1.20it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  72%|███████▏  | 230/320 [02:43<00:56,  1.59it/s]\u001b[A\n",
            "train loss:0.0209, train accuracy:1.0000.:  72%|███████▏  | 230/320 [02:43<00:56,  1.59it/s]\u001b[A\n",
            "train loss:0.0209, train accuracy:1.0000.:  72%|███████▏  | 231/320 [02:43<00:43,  2.05it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  72%|███████▏  | 231/320 [02:44<00:43,  2.05it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  72%|███████▎  | 232/320 [02:44<01:09,  1.26it/s]\u001b[A\n",
            "train loss:0.0216, train accuracy:1.0000.:  72%|███████▎  | 232/320 [02:44<01:09,  1.26it/s]\u001b[A\n",
            "train loss:0.0216, train accuracy:1.0000.:  73%|███████▎  | 233/320 [02:44<00:51,  1.67it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  73%|███████▎  | 233/320 [02:45<00:51,  1.67it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  73%|███████▎  | 234/320 [02:45<00:39,  2.17it/s]\u001b[A\n",
            "train loss:0.0221, train accuracy:1.0000.:  73%|███████▎  | 234/320 [02:46<00:39,  2.17it/s]\u001b[A\n",
            "train loss:0.0221, train accuracy:1.0000.:  73%|███████▎  | 235/320 [02:46<01:09,  1.22it/s]\u001b[A\n",
            "train loss:0.0217, train accuracy:1.0000.:  73%|███████▎  | 235/320 [02:46<01:09,  1.22it/s]\u001b[A\n",
            "train loss:0.0217, train accuracy:1.0000.:  74%|███████▍  | 236/320 [02:46<00:52,  1.60it/s]\u001b[A\n",
            "train loss:0.0200, train accuracy:1.0000.:  74%|███████▍  | 236/320 [02:46<00:52,  1.60it/s]\u001b[A\n",
            "train loss:0.0200, train accuracy:1.0000.:  74%|███████▍  | 237/320 [02:46<00:39,  2.08it/s]\u001b[A\n",
            "train loss:0.0214, train accuracy:1.0000.:  74%|███████▍  | 237/320 [02:48<00:39,  2.08it/s]\u001b[A\n",
            "train loss:0.0214, train accuracy:1.0000.:  74%|███████▍  | 238/320 [02:48<01:15,  1.08it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  74%|███████▍  | 238/320 [02:49<01:15,  1.08it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  75%|███████▍  | 239/320 [02:49<00:56,  1.45it/s]\u001b[A\n",
            "train loss:0.0183, train accuracy:1.0000.:  75%|███████▍  | 239/320 [02:49<00:56,  1.45it/s]\u001b[A\n",
            "train loss:0.0183, train accuracy:1.0000.:  75%|███████▌  | 240/320 [02:49<00:41,  1.92it/s]\u001b[A\n",
            "train loss:0.0184, train accuracy:1.0000.:  75%|███████▌  | 240/320 [02:51<00:41,  1.92it/s]\u001b[A\n",
            "train loss:0.0184, train accuracy:1.0000.:  75%|███████▌  | 241/320 [02:51<01:25,  1.09s/it]\u001b[A\n",
            "train loss:0.0191, train accuracy:1.0000.:  75%|███████▌  | 241/320 [02:51<01:25,  1.09s/it]\u001b[A\n",
            "train loss:0.0191, train accuracy:1.0000.:  76%|███████▌  | 242/320 [02:51<01:02,  1.24it/s]\u001b[A\n",
            "train loss:0.0196, train accuracy:1.0000.:  76%|███████▌  | 242/320 [02:51<01:02,  1.24it/s]\u001b[A\n",
            "train loss:0.0196, train accuracy:1.0000.:  76%|███████▌  | 243/320 [02:51<00:46,  1.65it/s]\u001b[A\n",
            "train loss:0.0168, train accuracy:1.0000.:  76%|███████▌  | 243/320 [02:53<00:46,  1.65it/s]\u001b[A\n",
            "train loss:0.0168, train accuracy:1.0000.:  76%|███████▋  | 244/320 [02:53<01:06,  1.15it/s]\u001b[A\n",
            "train loss:0.0219, train accuracy:1.0000.:  76%|███████▋  | 244/320 [02:53<01:06,  1.15it/s]\u001b[A\n",
            "train loss:0.0219, train accuracy:1.0000.:  77%|███████▋  | 245/320 [02:53<00:48,  1.53it/s]\u001b[A\n",
            "train loss:0.0205, train accuracy:1.0000.:  77%|███████▋  | 245/320 [02:53<00:48,  1.53it/s]\u001b[A\n",
            "train loss:0.0205, train accuracy:1.0000.:  77%|███████▋  | 246/320 [02:53<00:37,  1.99it/s]\u001b[A\n",
            "train loss:0.0177, train accuracy:1.0000.:  77%|███████▋  | 246/320 [02:55<00:37,  1.99it/s]\u001b[A\n",
            "train loss:0.0177, train accuracy:1.0000.:  77%|███████▋  | 247/320 [02:55<00:58,  1.25it/s]\u001b[A\n",
            "train loss:0.0192, train accuracy:1.0000.:  77%|███████▋  | 247/320 [02:55<00:58,  1.25it/s]\u001b[A\n",
            "train loss:0.0192, train accuracy:1.0000.:  78%|███████▊  | 248/320 [02:55<00:43,  1.66it/s]\u001b[A\n",
            "train loss:0.0202, train accuracy:1.0000.:  78%|███████▊  | 248/320 [02:55<00:43,  1.66it/s]\u001b[A\n",
            "train loss:0.0202, train accuracy:1.0000.:  78%|███████▊  | 249/320 [02:55<00:32,  2.15it/s]\u001b[A\n",
            "train loss:0.0180, train accuracy:1.0000.:  78%|███████▊  | 249/320 [02:57<00:32,  2.15it/s]\u001b[A\n",
            "train loss:0.0180, train accuracy:1.0000.:  78%|███████▊  | 250/320 [02:57<00:54,  1.27it/s]\u001b[A\n",
            "train loss:0.0199, train accuracy:1.0000.:  78%|███████▊  | 250/320 [02:57<00:54,  1.27it/s]\u001b[A\n",
            "train loss:0.0199, train accuracy:1.0000.:  78%|███████▊  | 251/320 [02:57<00:40,  1.68it/s]\u001b[A\n",
            "train loss:0.0162, train accuracy:1.0000.:  78%|███████▊  | 251/320 [02:57<00:40,  1.68it/s]\u001b[A\n",
            "train loss:0.0162, train accuracy:1.0000.:  79%|███████▉  | 252/320 [02:57<00:31,  2.18it/s]\u001b[A\n",
            "train loss:0.0169, train accuracy:1.0000.:  79%|███████▉  | 252/320 [02:58<00:31,  2.18it/s]\u001b[A\n",
            "train loss:0.0169, train accuracy:1.0000.:  79%|███████▉  | 253/320 [02:58<00:53,  1.25it/s]\u001b[A\n",
            "train loss:0.0179, train accuracy:1.0000.:  79%|███████▉  | 253/320 [02:59<00:53,  1.25it/s]\u001b[A\n",
            "train loss:0.0179, train accuracy:1.0000.:  79%|███████▉  | 254/320 [02:59<00:40,  1.63it/s]\u001b[A\n",
            "train loss:0.0191, train accuracy:1.0000.:  79%|███████▉  | 254/320 [02:59<00:40,  1.63it/s]\u001b[A\n",
            "train loss:0.0191, train accuracy:1.0000.:  80%|███████▉  | 255/320 [02:59<00:30,  2.10it/s]\u001b[A\n",
            "train loss:0.0178, train accuracy:1.0000.:  80%|███████▉  | 255/320 [03:01<00:30,  2.10it/s]\u001b[A\n",
            "train loss:0.0178, train accuracy:1.0000.:  80%|████████  | 256/320 [03:01<01:00,  1.06it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  80%|████████  | 256/320 [03:02<01:00,  1.06it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  80%|████████  | 257/320 [03:02<01:01,  1.03it/s]\u001b[A\n",
            "train loss:0.0171, train accuracy:1.0000.:  80%|████████  | 257/320 [03:02<01:01,  1.03it/s]\u001b[A\n",
            "train loss:0.0171, train accuracy:1.0000.:  81%|████████  | 258/320 [03:02<00:44,  1.38it/s]\u001b[A\n",
            "train loss:0.0166, train accuracy:1.0000.:  81%|████████  | 258/320 [03:03<00:44,  1.38it/s]\u001b[A\n",
            "train loss:0.0166, train accuracy:1.0000.:  81%|████████  | 259/320 [03:03<00:58,  1.04it/s]\u001b[A\n",
            "train loss:0.0148, train accuracy:1.0000.:  81%|████████  | 259/320 [03:04<00:58,  1.04it/s]\u001b[A\n",
            "train loss:0.0148, train accuracy:1.0000.:  81%|████████▏ | 260/320 [03:04<00:42,  1.40it/s]\u001b[A\n",
            "train loss:0.0178, train accuracy:1.0000.:  81%|████████▏ | 260/320 [03:04<00:42,  1.40it/s]\u001b[A\n",
            "train loss:0.0178, train accuracy:1.0000.:  82%|████████▏ | 261/320 [03:04<00:32,  1.83it/s]\u001b[A\n",
            "train loss:0.0208, train accuracy:1.0000.:  82%|████████▏ | 261/320 [03:05<00:32,  1.83it/s]\u001b[A\n",
            "train loss:0.0208, train accuracy:1.0000.:  82%|████████▏ | 262/320 [03:05<00:49,  1.18it/s]\u001b[A\n",
            "train loss:0.0153, train accuracy:1.0000.:  82%|████████▏ | 262/320 [03:05<00:49,  1.18it/s]\u001b[A\n",
            "train loss:0.0153, train accuracy:1.0000.:  82%|████████▏ | 263/320 [03:05<00:36,  1.56it/s]\u001b[A\n",
            "train loss:0.0157, train accuracy:1.0000.:  82%|████████▏ | 263/320 [03:06<00:36,  1.56it/s]\u001b[A\n",
            "train loss:0.0157, train accuracy:1.0000.:  82%|████████▎ | 264/320 [03:06<00:27,  2.04it/s]\u001b[A\n",
            "train loss:0.0156, train accuracy:1.0000.:  82%|████████▎ | 264/320 [03:07<00:27,  2.04it/s]\u001b[A\n",
            "train loss:0.0156, train accuracy:1.0000.:  83%|████████▎ | 265/320 [03:07<00:43,  1.26it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  83%|████████▎ | 265/320 [03:07<00:43,  1.26it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  83%|████████▎ | 266/320 [03:07<00:32,  1.66it/s]\u001b[A\n",
            "train loss:0.0156, train accuracy:1.0000.:  83%|████████▎ | 266/320 [03:07<00:32,  1.66it/s]\u001b[A\n",
            "train loss:0.0156, train accuracy:1.0000.:  83%|████████▎ | 267/320 [03:07<00:24,  2.16it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  83%|████████▎ | 267/320 [03:09<00:24,  2.16it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  84%|████████▍ | 268/320 [03:09<00:40,  1.28it/s]\u001b[A\n",
            "train loss:0.0165, train accuracy:1.0000.:  84%|████████▍ | 268/320 [03:09<00:40,  1.28it/s]\u001b[A\n",
            "train loss:0.0165, train accuracy:1.0000.:  84%|████████▍ | 269/320 [03:09<00:30,  1.70it/s]\u001b[A\n",
            "train loss:0.0146, train accuracy:1.0000.:  84%|████████▍ | 269/320 [03:09<00:30,  1.70it/s]\u001b[A\n",
            "train loss:0.0146, train accuracy:1.0000.:  84%|████████▍ | 270/320 [03:09<00:22,  2.19it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  84%|████████▍ | 270/320 [03:11<00:22,  2.19it/s]\u001b[A\n",
            "train loss:0.0155, train accuracy:1.0000.:  85%|████████▍ | 271/320 [03:11<00:40,  1.21it/s]\u001b[A\n",
            "train loss:0.0150, train accuracy:1.0000.:  85%|████████▍ | 271/320 [03:11<00:40,  1.21it/s]\u001b[A\n",
            "train loss:0.0150, train accuracy:1.0000.:  85%|████████▌ | 272/320 [03:11<00:29,  1.60it/s]\u001b[A\n",
            "train loss:0.0140, train accuracy:1.0000.:  85%|████████▌ | 272/320 [03:13<00:29,  1.60it/s]\u001b[A\n",
            "train loss:0.0140, train accuracy:1.0000.:  85%|████████▌ | 273/320 [03:13<00:43,  1.07it/s]\u001b[A\n",
            "train loss:0.0136, train accuracy:1.0000.:  85%|████████▌ | 273/320 [03:14<00:43,  1.07it/s]\u001b[A\n",
            "train loss:0.0136, train accuracy:1.0000.:  86%|████████▌ | 274/320 [03:14<00:51,  1.11s/it]\u001b[A\n",
            "train loss:0.0135, train accuracy:1.0000.:  86%|████████▌ | 274/320 [03:14<00:51,  1.11s/it]\u001b[A\n",
            "train loss:0.0135, train accuracy:1.0000.:  86%|████████▌ | 275/320 [03:14<00:37,  1.21it/s]\u001b[A\n",
            "train loss:0.0129, train accuracy:1.0000.:  86%|████████▌ | 275/320 [03:15<00:37,  1.21it/s]\u001b[A\n",
            "train loss:0.0129, train accuracy:1.0000.:  86%|████████▋ | 276/320 [03:15<00:27,  1.61it/s]\u001b[A\n",
            "train loss:0.0137, train accuracy:1.0000.:  86%|████████▋ | 276/320 [03:16<00:27,  1.61it/s]\u001b[A\n",
            "train loss:0.0137, train accuracy:1.0000.:  87%|████████▋ | 277/320 [03:16<00:37,  1.14it/s]\u001b[A\n",
            "train loss:0.0137, train accuracy:1.0000.:  87%|████████▋ | 277/320 [03:16<00:37,  1.14it/s]\u001b[A\n",
            "train loss:0.0137, train accuracy:1.0000.:  87%|████████▋ | 278/320 [03:16<00:27,  1.51it/s]\u001b[A\n",
            "train loss:0.0138, train accuracy:1.0000.:  87%|████████▋ | 278/320 [03:16<00:27,  1.51it/s]\u001b[A\n",
            "train loss:0.0138, train accuracy:1.0000.:  87%|████████▋ | 279/320 [03:16<00:20,  1.97it/s]\u001b[A\n",
            "train loss:0.0136, train accuracy:1.0000.:  87%|████████▋ | 279/320 [03:18<00:20,  1.97it/s]\u001b[A\n",
            "train loss:0.0136, train accuracy:1.0000.:  88%|████████▊ | 280/320 [03:18<00:32,  1.24it/s]\u001b[A\n",
            "train loss:0.0129, train accuracy:1.0000.:  88%|████████▊ | 280/320 [03:18<00:32,  1.24it/s]\u001b[A\n",
            "train loss:0.0129, train accuracy:1.0000.:  88%|████████▊ | 281/320 [03:18<00:23,  1.64it/s]\u001b[A\n",
            "train loss:0.0117, train accuracy:1.0000.:  88%|████████▊ | 281/320 [03:18<00:23,  1.64it/s]\u001b[A\n",
            "train loss:0.0117, train accuracy:1.0000.:  88%|████████▊ | 282/320 [03:18<00:17,  2.13it/s]\u001b[A\n",
            "train loss:0.0134, train accuracy:1.0000.:  88%|████████▊ | 282/320 [03:20<00:17,  2.13it/s]\u001b[A\n",
            "train loss:0.0134, train accuracy:1.0000.:  88%|████████▊ | 283/320 [03:20<00:30,  1.22it/s]\u001b[A\n",
            "train loss:0.0124, train accuracy:1.0000.:  88%|████████▊ | 283/320 [03:20<00:30,  1.22it/s]\u001b[A\n",
            "train loss:0.0124, train accuracy:1.0000.:  89%|████████▉ | 284/320 [03:20<00:22,  1.62it/s]\u001b[A\n",
            "train loss:0.0143, train accuracy:1.0000.:  89%|████████▉ | 284/320 [03:20<00:22,  1.62it/s]\u001b[A\n",
            "train loss:0.0143, train accuracy:1.0000.:  89%|████████▉ | 285/320 [03:20<00:16,  2.10it/s]\u001b[A\n",
            "train loss:0.0164, train accuracy:1.0000.:  89%|████████▉ | 285/320 [03:22<00:16,  2.10it/s]\u001b[A\n",
            "train loss:0.0164, train accuracy:1.0000.:  89%|████████▉ | 286/320 [03:22<00:26,  1.28it/s]\u001b[A\n",
            "train loss:0.0123, train accuracy:1.0000.:  89%|████████▉ | 286/320 [03:22<00:26,  1.28it/s]\u001b[A\n",
            "train loss:0.0123, train accuracy:1.0000.:  90%|████████▉ | 287/320 [03:22<00:19,  1.69it/s]\u001b[A\n",
            "train loss:0.0117, train accuracy:1.0000.:  90%|████████▉ | 287/320 [03:22<00:19,  1.69it/s]\u001b[A\n",
            "train loss:0.0117, train accuracy:1.0000.:  90%|█████████ | 288/320 [03:22<00:14,  2.21it/s]\u001b[A\n",
            "train loss:0.0118, train accuracy:1.0000.:  90%|█████████ | 288/320 [03:25<00:14,  2.21it/s]\u001b[A\n",
            "train loss:0.0118, train accuracy:1.0000.:  90%|█████████ | 289/320 [03:25<00:39,  1.27s/it]\u001b[A\n",
            "train loss:0.0136, train accuracy:1.0000.:  90%|█████████ | 289/320 [03:25<00:39,  1.27s/it]\u001b[A\n",
            "train loss:0.0136, train accuracy:1.0000.:  91%|█████████ | 290/320 [03:25<00:28,  1.07it/s]\u001b[A\n",
            "train loss:0.0125, train accuracy:1.0000.:  91%|█████████ | 290/320 [03:25<00:28,  1.07it/s]\u001b[A\n",
            "train loss:0.0125, train accuracy:1.0000.:  91%|█████████ | 291/320 [03:25<00:20,  1.44it/s]\u001b[A\n",
            "train loss:0.0110, train accuracy:1.0000.:  91%|█████████ | 291/320 [03:27<00:20,  1.44it/s]\u001b[A\n",
            "train loss:0.0110, train accuracy:1.0000.:  91%|█████████▏| 292/320 [03:27<00:26,  1.06it/s]\u001b[A\n",
            "train loss:0.0125, train accuracy:1.0000.:  91%|█████████▏| 292/320 [03:27<00:26,  1.06it/s]\u001b[A\n",
            "train loss:0.0125, train accuracy:1.0000.:  92%|█████████▏| 293/320 [03:27<00:19,  1.40it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  92%|█████████▏| 293/320 [03:27<00:19,  1.40it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  92%|█████████▏| 294/320 [03:27<00:14,  1.83it/s]\u001b[A\n",
            "train loss:0.0119, train accuracy:1.0000.:  92%|█████████▏| 294/320 [03:29<00:14,  1.83it/s]\u001b[A\n",
            "train loss:0.0119, train accuracy:1.0000.:  92%|█████████▏| 295/320 [03:29<00:20,  1.20it/s]\u001b[A\n",
            "train loss:0.0114, train accuracy:1.0000.:  92%|█████████▏| 295/320 [03:29<00:20,  1.20it/s]\u001b[A\n",
            "train loss:0.0114, train accuracy:1.0000.:  92%|█████████▎| 296/320 [03:29<00:15,  1.60it/s]\u001b[A\n",
            "train loss:0.0134, train accuracy:1.0000.:  92%|█████████▎| 296/320 [03:29<00:15,  1.60it/s]\u001b[A\n",
            "train loss:0.0134, train accuracy:1.0000.:  93%|█████████▎| 297/320 [03:29<00:11,  2.08it/s]\u001b[A\n",
            "train loss:0.0122, train accuracy:1.0000.:  93%|█████████▎| 297/320 [03:30<00:11,  2.08it/s]\u001b[A\n",
            "train loss:0.0122, train accuracy:1.0000.:  93%|█████████▎| 298/320 [03:30<00:16,  1.29it/s]\u001b[A\n",
            "train loss:0.0107, train accuracy:1.0000.:  93%|█████████▎| 298/320 [03:31<00:16,  1.29it/s]\u001b[A\n",
            "train loss:0.0107, train accuracy:1.0000.:  93%|█████████▎| 299/320 [03:31<00:12,  1.71it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  93%|█████████▎| 299/320 [03:31<00:12,  1.71it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  94%|█████████▍| 300/320 [03:31<00:09,  2.21it/s]\u001b[A\n",
            "train loss:0.0103, train accuracy:1.0000.:  94%|█████████▍| 300/320 [03:32<00:09,  2.21it/s]\u001b[A\n",
            "train loss:0.0103, train accuracy:1.0000.:  94%|█████████▍| 301/320 [03:32<00:14,  1.31it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  94%|█████████▍| 301/320 [03:32<00:14,  1.31it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  94%|█████████▍| 302/320 [03:32<00:10,  1.73it/s]\u001b[A\n",
            "train loss:0.0111, train accuracy:1.0000.:  94%|█████████▍| 302/320 [03:32<00:10,  1.73it/s]\u001b[A\n",
            "train loss:0.0111, train accuracy:1.0000.:  95%|█████████▍| 303/320 [03:32<00:07,  2.22it/s]\u001b[A\n",
            "train loss:0.0116, train accuracy:1.0000.:  95%|█████████▍| 303/320 [03:34<00:07,  2.22it/s]\u001b[A\n",
            "train loss:0.0116, train accuracy:1.0000.:  95%|█████████▌| 304/320 [03:34<00:12,  1.31it/s]\u001b[A\n",
            "train loss:0.0102, train accuracy:1.0000.:  95%|█████████▌| 304/320 [03:35<00:12,  1.31it/s]\u001b[A\n",
            "train loss:0.0102, train accuracy:1.0000.:  95%|█████████▌| 305/320 [03:35<00:13,  1.15it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  95%|█████████▌| 305/320 [03:35<00:13,  1.15it/s]\u001b[A\n",
            "train loss:0.0108, train accuracy:1.0000.:  96%|█████████▌| 306/320 [03:35<00:09,  1.52it/s]\u001b[A\n",
            "train loss:0.0199, train accuracy:1.0000.:  96%|█████████▌| 306/320 [03:37<00:09,  1.52it/s]\u001b[A\n",
            "train loss:0.0199, train accuracy:1.0000.:  96%|█████████▌| 307/320 [03:37<00:14,  1.10s/it]\u001b[A\n",
            "train loss:0.0106, train accuracy:1.0000.:  96%|█████████▌| 307/320 [03:37<00:14,  1.10s/it]\u001b[A\n",
            "train loss:0.0106, train accuracy:1.0000.:  96%|█████████▋| 308/320 [03:37<00:09,  1.23it/s]\u001b[A\n",
            "train loss:0.0105, train accuracy:1.0000.:  96%|█████████▋| 308/320 [03:38<00:09,  1.23it/s]\u001b[A\n",
            "train loss:0.0105, train accuracy:1.0000.:  97%|█████████▋| 309/320 [03:38<00:06,  1.64it/s]\u001b[A\n",
            "train loss:0.0101, train accuracy:1.0000.:  97%|█████████▋| 309/320 [03:39<00:06,  1.64it/s]\u001b[A\n",
            "train loss:0.0101, train accuracy:1.0000.:  97%|█████████▋| 310/320 [03:39<00:08,  1.14it/s]\u001b[A\n",
            "train loss:0.0116, train accuracy:1.0000.:  97%|█████████▋| 310/320 [03:39<00:08,  1.14it/s]\u001b[A\n",
            "train loss:0.0116, train accuracy:1.0000.:  97%|█████████▋| 311/320 [03:39<00:05,  1.52it/s]\u001b[A\n",
            "train loss:0.0184, train accuracy:1.0000.:  97%|█████████▋| 311/320 [03:39<00:05,  1.52it/s]\u001b[A\n",
            "train loss:0.0184, train accuracy:1.0000.:  98%|█████████▊| 312/320 [03:39<00:04,  1.97it/s]\u001b[A\n",
            "train loss:0.0116, train accuracy:1.0000.:  98%|█████████▊| 312/320 [03:41<00:04,  1.97it/s]\u001b[A\n",
            "train loss:0.0116, train accuracy:1.0000.:  98%|█████████▊| 313/320 [03:41<00:05,  1.25it/s]\u001b[A\n",
            "train loss:0.0095, train accuracy:1.0000.:  98%|█████████▊| 313/320 [03:41<00:05,  1.25it/s]\u001b[A\n",
            "train loss:0.0095, train accuracy:1.0000.:  98%|█████████▊| 314/320 [03:41<00:03,  1.66it/s]\u001b[A\n",
            "train loss:0.0101, train accuracy:1.0000.:  98%|█████████▊| 314/320 [03:41<00:03,  1.66it/s]\u001b[A\n",
            "train loss:0.0101, train accuracy:1.0000.:  98%|█████████▊| 315/320 [03:41<00:02,  2.15it/s]\u001b[A\n",
            "train loss:0.0119, train accuracy:1.0000.:  98%|█████████▊| 315/320 [03:43<00:02,  2.15it/s]\u001b[A\n",
            "train loss:0.0119, train accuracy:1.0000.:  99%|█████████▉| 316/320 [03:43<00:03,  1.27it/s]\u001b[A\n",
            "train loss:0.0140, train accuracy:1.0000.:  99%|█████████▉| 316/320 [03:43<00:03,  1.27it/s]\u001b[A\n",
            "train loss:0.0140, train accuracy:1.0000.:  99%|█████████▉| 317/320 [03:43<00:01,  1.68it/s]\u001b[A\n",
            "train loss:0.0145, train accuracy:1.0000.:  99%|█████████▉| 317/320 [03:43<00:01,  1.68it/s]\u001b[A\n",
            "train loss:0.0145, train accuracy:1.0000.:  99%|█████████▉| 318/320 [03:43<00:00,  2.18it/s]\u001b[A\n",
            "train loss:0.0113, train accuracy:1.0000.:  99%|█████████▉| 318/320 [03:45<00:00,  2.18it/s]\u001b[A\n",
            "train loss:0.0113, train accuracy:1.0000.: 100%|█████████▉| 319/320 [03:45<00:00,  1.29it/s]\u001b[A\n",
            "train loss:0.0106, train accuracy:1.0000.: 100%|█████████▉| 319/320 [03:45<00:00,  1.29it/s]\u001b[A\n",
            "train loss:0.0106, train accuracy:1.0000.: 100%|██████████| 320/320 [03:45<00:00,  1.42it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "train(start_frozen=False, model_unfreeze=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEDv_-H7BvM0"
      },
      "source": [
        "### 1.4 Implement Unfreezing (1 hr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH5mQBaa-_0b"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_YmE1pe-6LF"
      },
      "source": [
        "Unfreezing is a technique that can be helpful when fine tuning a CNN for a more difficult task with a large amount of data.\n",
        "\n",
        "The idea is that if we allow the network to tweak the earliest layers immediately, before the last FCL has been trained at all, the earliest layers will forget all of the useful features that they learned in order  to provide features that are helpful for the (untrained) FCL.\n",
        "\n",
        "So, rather than training all of the model weights at once, we learn the last fully connected layer, then train that layer together with the second-to-last layer, gradually adding layers until we reach the first layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMKRI77_-8nc"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaUc8BTYC1bz"
      },
      "source": [
        "- Modify your model's parameters by setting the `requires_grad` attributes to `False`. (but keep `requires_grad = True` for the last layer).\n",
        "- Add a member function to you model class that allows the user to unfreeze weights in the training loop. See [this github gist](https://gist.github.com/jcjohnson/6e41e8512c17eae5da50aebef3378a4c) for reference.\n",
        "- Modify your training loop to add logic that calls the `unfreeze` function of the model class: unfreeze one layer (a convolutional layer or linear layer, not the sequential or bottleneck layers) every epoch.\n",
        "- Call your train function to fine-tune the ResNet on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBT5jgifC7Im"
      },
      "source": [
        "#### Call your train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "Mg9ySEO_BNDx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c543f8af-1812-46ec-edc0-1e022916a2bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/320 [00:00<?, ?it/s]\u001b[A\n",
            "train loss:2.9356, train accuracy:0.0312.:   0%|          | 0/320 [00:02<?, ?it/s]\u001b[A\n",
            "train loss:2.9356, train accuracy:0.0312.:   0%|          | 1/320 [00:02<12:08,  2.28s/it]\u001b[A\n",
            "train loss:2.8428, train accuracy:0.0312.:   0%|          | 1/320 [00:02<12:08,  2.28s/it]\u001b[A\n",
            "train loss:2.7497, train accuracy:0.0312.:   1%|          | 2/320 [00:02<12:06,  2.28s/it]\u001b[A\n",
            "train loss:2.6504, train accuracy:0.0625.:   1%|          | 3/320 [00:04<12:04,  2.28s/it]\u001b[A\n",
            "train loss:2.6504, train accuracy:0.0625.:   1%|▏         | 4/320 [00:04<04:45,  1.11it/s]\u001b[A\n",
            "train loss:2.5526, train accuracy:0.1562.:   1%|▏         | 4/320 [00:04<04:45,  1.11it/s]\u001b[A\n",
            "train loss:2.4746, train accuracy:0.1562.:   2%|▏         | 5/320 [00:04<04:44,  1.11it/s]\u001b[A\n",
            "train loss:2.4746, train accuracy:0.1562.:   2%|▏         | 6/320 [00:04<02:48,  1.87it/s]\u001b[A\n",
            "train loss:2.4033, train accuracy:0.3125.:   2%|▏         | 6/320 [00:06<02:48,  1.87it/s]\u001b[A\n",
            "train loss:2.4033, train accuracy:0.3125.:   2%|▏         | 7/320 [00:06<04:30,  1.16it/s]\u001b[A\n",
            "train loss:2.3367, train accuracy:0.3750.:   2%|▏         | 7/320 [00:06<04:30,  1.16it/s]\u001b[A\n",
            "train loss:2.2488, train accuracy:0.4375.:   2%|▎         | 8/320 [00:06<04:29,  1.16it/s]\u001b[A\n",
            "train loss:2.1667, train accuracy:0.5312.:   3%|▎         | 9/320 [00:07<04:28,  1.16it/s]\u001b[A\n",
            "train loss:2.1667, train accuracy:0.5312.:   3%|▎         | 10/320 [00:07<03:31,  1.47it/s]\u001b[A\n",
            "train loss:2.0818, train accuracy:0.6562.:   3%|▎         | 10/320 [00:07<03:31,  1.47it/s]\u001b[A\n",
            "train loss:1.9761, train accuracy:0.7500.:   3%|▎         | 11/320 [00:07<03:30,  1.47it/s]\u001b[A\n",
            "train loss:1.8847, train accuracy:0.8750.:   4%|▍         | 12/320 [00:09<03:29,  1.47it/s]\u001b[A\n",
            "train loss:1.8847, train accuracy:0.8750.:   4%|▍         | 13/320 [00:09<03:05,  1.66it/s]\u001b[A\n",
            "train loss:1.8210, train accuracy:0.8750.:   4%|▍         | 13/320 [00:09<03:05,  1.66it/s]\u001b[A\n",
            "train loss:1.7541, train accuracy:0.9375.:   4%|▍         | 14/320 [00:09<03:04,  1.66it/s]\u001b[A\n",
            "train loss:1.6575, train accuracy:0.9500.:   5%|▍         | 15/320 [00:10<03:03,  1.66it/s]\u001b[A\n",
            "train loss:1.6575, train accuracy:0.9500.:   5%|▌         | 16/320 [00:10<02:50,  1.78it/s]\u001b[A\n",
            "train loss:1.5936, train accuracy:1.0000.:   5%|▌         | 16/320 [00:11<02:50,  1.78it/s]\u001b[A\n",
            "train loss:1.5936, train accuracy:1.0000.:   5%|▌         | 17/320 [00:11<03:10,  1.59it/s]\u001b[A\n",
            "train loss:1.5020, train accuracy:1.0000.:   5%|▌         | 17/320 [00:11<03:10,  1.59it/s]\u001b[A\n",
            "train loss:1.4565, train accuracy:1.0000.:   6%|▌         | 18/320 [00:12<03:09,  1.59it/s]\u001b[A\n",
            "train loss:1.4565, train accuracy:1.0000.:   6%|▌         | 19/320 [00:13<03:18,  1.51it/s]\u001b[A\n",
            "train loss:1.3865, train accuracy:1.0000.:   6%|▌         | 19/320 [00:13<03:18,  1.51it/s]\u001b[A\n",
            "train loss:1.3228, train accuracy:1.0000.:   6%|▋         | 20/320 [00:13<03:18,  1.51it/s]\u001b[A\n",
            "train loss:1.2848, train accuracy:1.0000.:   7%|▋         | 21/320 [00:14<03:17,  1.51it/s]\u001b[A\n",
            "train loss:1.2848, train accuracy:1.0000.:   7%|▋         | 22/320 [00:14<03:07,  1.59it/s]\u001b[A\n",
            "train loss:1.2120, train accuracy:1.0000.:   7%|▋         | 22/320 [00:14<03:07,  1.59it/s]\u001b[A\n",
            "train loss:1.1232, train accuracy:1.0000.:   7%|▋         | 23/320 [00:14<03:07,  1.59it/s]\u001b[A\n",
            "train loss:1.0994, train accuracy:1.0000.:   8%|▊         | 24/320 [00:16<03:06,  1.59it/s]\u001b[A\n",
            "train loss:1.0994, train accuracy:1.0000.:   8%|▊         | 25/320 [00:16<02:59,  1.64it/s]\u001b[A\n",
            "train loss:1.0632, train accuracy:1.0000.:   8%|▊         | 25/320 [00:16<02:59,  1.64it/s]\u001b[A\n",
            "train loss:0.9913, train accuracy:1.0000.:   8%|▊         | 26/320 [00:16<02:59,  1.64it/s]\u001b[A\n",
            "train loss:0.9913, train accuracy:1.0000.:   8%|▊         | 27/320 [00:16<02:17,  2.13it/s]\u001b[A\n",
            "train loss:0.9598, train accuracy:1.0000.:   8%|▊         | 27/320 [00:18<02:17,  2.13it/s]\u001b[A\n",
            "train loss:0.9598, train accuracy:1.0000.:   9%|▉         | 28/320 [00:18<03:23,  1.43it/s]\u001b[A\n",
            "train loss:0.8998, train accuracy:1.0000.:   9%|▉         | 28/320 [00:18<03:23,  1.43it/s]\u001b[A\n",
            "train loss:0.8627, train accuracy:1.0000.:   9%|▉         | 29/320 [00:18<03:23,  1.43it/s]\u001b[A\n",
            "train loss:0.8096, train accuracy:1.0000.:   9%|▉         | 30/320 [00:20<03:22,  1.43it/s]\u001b[A\n",
            "train loss:0.8096, train accuracy:1.0000.:  10%|▉         | 31/320 [00:20<02:58,  1.62it/s]\u001b[A\n",
            "train loss:0.7509, train accuracy:1.0000.:  10%|▉         | 31/320 [00:20<02:58,  1.62it/s]\u001b[A\n",
            "train loss:0.7532, train accuracy:1.0000.:  10%|█         | 32/320 [00:21<02:58,  1.62it/s]\u001b[A\n",
            "train loss:0.7532, train accuracy:1.0000.:  10%|█         | 33/320 [00:21<02:48,  1.70it/s]\u001b[A\n",
            "train loss:0.7164, train accuracy:1.0000.:  10%|█         | 33/320 [00:22<02:48,  1.70it/s]\u001b[A\n",
            "train loss:0.7164, train accuracy:1.0000.:  11%|█         | 34/320 [00:22<03:32,  1.35it/s]\u001b[A\n",
            "train loss:0.6409, train accuracy:1.0000.:  11%|█         | 34/320 [00:22<03:32,  1.35it/s]\u001b[A\n",
            "train loss:0.6584, train accuracy:1.0000.:  11%|█         | 35/320 [00:22<03:31,  1.35it/s]\u001b[A\n",
            "train loss:0.6254, train accuracy:1.0000.:  11%|█▏        | 36/320 [00:24<03:30,  1.35it/s]\u001b[A\n",
            "train loss:0.6254, train accuracy:1.0000.:  12%|█▏        | 37/320 [00:24<03:15,  1.45it/s]\u001b[A\n",
            "train loss:0.5739, train accuracy:1.0000.:  12%|█▏        | 37/320 [00:24<03:15,  1.45it/s]\u001b[A\n",
            "train loss:0.5239, train accuracy:1.0000.:  12%|█▏        | 38/320 [00:24<03:14,  1.45it/s]\u001b[A\n",
            "train loss:0.5359, train accuracy:1.0000.:  12%|█▏        | 39/320 [00:25<03:14,  1.45it/s]\u001b[A\n",
            "train loss:0.5359, train accuracy:1.0000.:  12%|█▎        | 40/320 [00:25<02:54,  1.61it/s]\u001b[A\n",
            "train loss:0.5165, train accuracy:1.0000.:  12%|█▎        | 40/320 [00:25<02:54,  1.61it/s]\u001b[A\n",
            "train loss:0.4701, train accuracy:1.0000.:  13%|█▎        | 41/320 [00:26<02:53,  1.61it/s]\u001b[A\n",
            "train loss:0.4507, train accuracy:1.0000.:  13%|█▎        | 42/320 [00:27<02:52,  1.61it/s]\u001b[A\n",
            "train loss:0.4507, train accuracy:1.0000.:  13%|█▎        | 43/320 [00:27<02:40,  1.73it/s]\u001b[A\n",
            "train loss:0.4643, train accuracy:1.0000.:  13%|█▎        | 43/320 [00:27<02:40,  1.73it/s]\u001b[A\n",
            "train loss:0.4250, train accuracy:1.0000.:  14%|█▍        | 44/320 [00:27<02:39,  1.73it/s]\u001b[A\n",
            "train loss:0.4029, train accuracy:1.0000.:  14%|█▍        | 45/320 [00:29<02:39,  1.73it/s]\u001b[A\n",
            "train loss:0.4029, train accuracy:1.0000.:  14%|█▍        | 46/320 [00:29<02:40,  1.70it/s]\u001b[A\n",
            "train loss:0.3825, train accuracy:1.0000.:  14%|█▍        | 46/320 [00:29<02:40,  1.70it/s]\u001b[A\n",
            "train loss:0.3303, train accuracy:1.0000.:  15%|█▍        | 47/320 [00:29<02:40,  1.70it/s]\u001b[A\n",
            "train loss:0.3303, train accuracy:1.0000.:  15%|█▌        | 48/320 [00:29<02:05,  2.16it/s]\u001b[A\n",
            "train loss:0.3684, train accuracy:1.0000.:  15%|█▌        | 48/320 [00:32<02:05,  2.16it/s]\u001b[A\n",
            "train loss:0.3684, train accuracy:1.0000.:  15%|█▌        | 49/320 [00:32<03:40,  1.23it/s]\u001b[A\n",
            "train loss:0.3525, train accuracy:1.0000.:  15%|█▌        | 49/320 [00:32<03:40,  1.23it/s]\u001b[A\n",
            "train loss:0.3532, train accuracy:1.0000.:  16%|█▌        | 50/320 [00:32<03:39,  1.23it/s]\u001b[A\n",
            "train loss:0.3237, train accuracy:1.0000.:  16%|█▌        | 51/320 [00:33<03:38,  1.23it/s]\u001b[A\n",
            "train loss:0.3237, train accuracy:1.0000.:  16%|█▋        | 52/320 [00:33<03:06,  1.44it/s]\u001b[A\n",
            "train loss:0.3389, train accuracy:1.0000.:  16%|█▋        | 52/320 [00:33<03:06,  1.44it/s]\u001b[A\n",
            "train loss:0.3007, train accuracy:1.0000.:  17%|█▋        | 53/320 [00:33<03:05,  1.44it/s]\u001b[A\n",
            "train loss:0.2920, train accuracy:1.0000.:  17%|█▋        | 54/320 [00:35<03:04,  1.44it/s]\u001b[A\n",
            "train loss:0.2920, train accuracy:1.0000.:  17%|█▋        | 55/320 [00:35<02:45,  1.60it/s]\u001b[A\n",
            "train loss:0.2780, train accuracy:1.0000.:  17%|█▋        | 55/320 [00:35<02:45,  1.60it/s]\u001b[A\n",
            "train loss:0.2771, train accuracy:1.0000.:  18%|█▊        | 56/320 [00:35<02:45,  1.60it/s]\u001b[A\n",
            "train loss:0.2738, train accuracy:1.0000.:  18%|█▊        | 57/320 [00:36<02:44,  1.60it/s]\u001b[A\n",
            "train loss:0.2738, train accuracy:1.0000.:  18%|█▊        | 58/320 [00:36<02:33,  1.71it/s]\u001b[A\n",
            "train loss:0.2618, train accuracy:1.0000.:  18%|█▊        | 58/320 [00:36<02:33,  1.71it/s]\u001b[A\n",
            "train loss:0.2217, train accuracy:1.0000.:  18%|█▊        | 59/320 [00:36<02:32,  1.71it/s]\u001b[A\n",
            "train loss:0.2467, train accuracy:1.0000.:  19%|█▉        | 60/320 [00:38<02:32,  1.71it/s]\u001b[A\n",
            "train loss:0.2467, train accuracy:1.0000.:  19%|█▉        | 61/320 [00:38<02:23,  1.80it/s]\u001b[A\n",
            "train loss:0.2272, train accuracy:1.0000.:  19%|█▉        | 61/320 [00:38<02:23,  1.80it/s]\u001b[A\n",
            "train loss:0.2206, train accuracy:1.0000.:  19%|█▉        | 62/320 [00:38<02:23,  1.80it/s]\u001b[A\n",
            "train loss:0.2023, train accuracy:1.0000.:  20%|█▉        | 63/320 [00:39<02:22,  1.80it/s]\u001b[A\n",
            "train loss:0.2023, train accuracy:1.0000.:  20%|██        | 64/320 [00:39<02:16,  1.88it/s]\u001b[A\n",
            "train loss:0.2218, train accuracy:1.0000.:  20%|██        | 64/320 [00:40<02:16,  1.88it/s]\u001b[A\n",
            "train loss:0.2218, train accuracy:1.0000.:  20%|██        | 65/320 [00:40<02:31,  1.69it/s]\u001b[A\n",
            "train loss:0.1997, train accuracy:1.0000.:  20%|██        | 65/320 [00:40<02:31,  1.69it/s]\u001b[A\n",
            "train loss:0.2026, train accuracy:1.0000.:  21%|██        | 66/320 [00:42<02:30,  1.69it/s]\u001b[A\n",
            "train loss:0.2026, train accuracy:1.0000.:  21%|██        | 67/320 [00:42<03:05,  1.36it/s]\u001b[A\n",
            "train loss:0.2334, train accuracy:1.0000.:  21%|██        | 67/320 [00:42<03:05,  1.36it/s]\u001b[A\n",
            "train loss:0.1858, train accuracy:1.0000.:  21%|██▏       | 68/320 [00:43<03:04,  1.36it/s]\u001b[A\n",
            "train loss:0.1570, train accuracy:1.0000.:  22%|██▏       | 69/320 [00:44<03:04,  1.36it/s]\u001b[A\n",
            "train loss:0.1570, train accuracy:1.0000.:  22%|██▏       | 70/320 [00:44<02:46,  1.50it/s]\u001b[A\n",
            "train loss:0.1563, train accuracy:1.0000.:  22%|██▏       | 70/320 [00:44<02:46,  1.50it/s]\u001b[A\n",
            "train loss:0.1600, train accuracy:1.0000.:  22%|██▏       | 71/320 [00:44<02:45,  1.50it/s]\u001b[A\n",
            "train loss:0.1476, train accuracy:1.0000.:  22%|██▎       | 72/320 [00:46<02:45,  1.50it/s]\u001b[A\n",
            "train loss:0.1476, train accuracy:1.0000.:  23%|██▎       | 73/320 [00:46<02:35,  1.59it/s]\u001b[A\n",
            "train loss:0.1468, train accuracy:1.0000.:  23%|██▎       | 73/320 [00:46<02:35,  1.59it/s]\u001b[A\n",
            "train loss:0.1433, train accuracy:1.0000.:  23%|██▎       | 74/320 [00:46<02:35,  1.59it/s]\u001b[A\n",
            "train loss:0.1445, train accuracy:1.0000.:  23%|██▎       | 75/320 [00:47<02:34,  1.59it/s]\u001b[A\n",
            "train loss:0.1445, train accuracy:1.0000.:  24%|██▍       | 76/320 [00:47<02:23,  1.70it/s]\u001b[A\n",
            "train loss:0.1619, train accuracy:1.0000.:  24%|██▍       | 76/320 [00:47<02:23,  1.70it/s]\u001b[A\n",
            "train loss:0.1487, train accuracy:1.0000.:  24%|██▍       | 77/320 [00:47<02:23,  1.70it/s]\u001b[A\n",
            "train loss:0.1406, train accuracy:1.0000.:  24%|██▍       | 78/320 [00:49<02:22,  1.70it/s]\u001b[A\n",
            "train loss:0.1406, train accuracy:1.0000.:  25%|██▍       | 79/320 [00:49<02:14,  1.80it/s]\u001b[A\n",
            "train loss:0.1291, train accuracy:1.0000.:  25%|██▍       | 79/320 [00:49<02:14,  1.80it/s]\u001b[A\n",
            "train loss:0.1509, train accuracy:1.0000.:  25%|██▌       | 80/320 [00:50<02:13,  1.80it/s]\u001b[A\n",
            "train loss:0.1509, train accuracy:1.0000.:  25%|██▌       | 81/320 [00:50<02:10,  1.84it/s]\u001b[A\n",
            "train loss:0.1334, train accuracy:1.0000.:  25%|██▌       | 81/320 [00:51<02:10,  1.84it/s]\u001b[A\n",
            "train loss:0.1334, train accuracy:1.0000.:  26%|██▌       | 82/320 [00:51<02:41,  1.47it/s]\u001b[A\n",
            "train loss:0.1244, train accuracy:1.0000.:  26%|██▌       | 82/320 [00:51<02:41,  1.47it/s]\u001b[A\n",
            "train loss:0.1120, train accuracy:1.0000.:  26%|██▌       | 83/320 [00:51<02:41,  1.47it/s]\u001b[A\n",
            "train loss:0.1110, train accuracy:1.0000.:  26%|██▋       | 84/320 [00:53<02:40,  1.47it/s]\u001b[A\n",
            "train loss:0.1110, train accuracy:1.0000.:  27%|██▋       | 85/320 [00:53<02:27,  1.59it/s]\u001b[A\n",
            "train loss:0.1239, train accuracy:1.0000.:  27%|██▋       | 85/320 [00:53<02:27,  1.59it/s]\u001b[A\n",
            "train loss:0.1114, train accuracy:1.0000.:  27%|██▋       | 86/320 [00:53<02:26,  1.59it/s]\u001b[A\n",
            "train loss:0.1114, train accuracy:1.0000.:  27%|██▋       | 87/320 [00:53<01:50,  2.10it/s]\u001b[A\n",
            "train loss:0.1193, train accuracy:1.0000.:  27%|██▋       | 87/320 [00:55<01:50,  2.10it/s]\u001b[A\n",
            "train loss:0.1193, train accuracy:1.0000.:  28%|██▊       | 88/320 [00:55<02:46,  1.40it/s]\u001b[A\n",
            "train loss:0.1044, train accuracy:1.0000.:  28%|██▊       | 88/320 [00:55<02:46,  1.40it/s]\u001b[A\n",
            "train loss:0.1009, train accuracy:1.0000.:  28%|██▊       | 89/320 [00:55<02:45,  1.40it/s]\u001b[A\n",
            "train loss:0.1017, train accuracy:1.0000.:  28%|██▊       | 90/320 [00:56<02:44,  1.40it/s]\u001b[A\n",
            "train loss:0.1017, train accuracy:1.0000.:  28%|██▊       | 91/320 [00:56<02:23,  1.59it/s]\u001b[A\n",
            "train loss:0.0911, train accuracy:1.0000.:  28%|██▊       | 91/320 [00:56<02:23,  1.59it/s]\u001b[A\n",
            "train loss:0.1054, train accuracy:1.0000.:  29%|██▉       | 92/320 [00:57<02:23,  1.59it/s]\u001b[A\n",
            "train loss:0.1012, train accuracy:1.0000.:  29%|██▉       | 93/320 [00:58<02:22,  1.59it/s]\u001b[A\n",
            "train loss:0.1012, train accuracy:1.0000.:  29%|██▉       | 94/320 [00:58<02:11,  1.72it/s]\u001b[A\n",
            "train loss:0.0943, train accuracy:1.0000.:  29%|██▉       | 94/320 [00:58<02:11,  1.72it/s]\u001b[A\n",
            "train loss:0.0942, train accuracy:1.0000.:  30%|██▉       | 95/320 [00:58<02:10,  1.72it/s]\u001b[A\n",
            "train loss:0.1046, train accuracy:1.0000.:  30%|███       | 96/320 [01:00<02:10,  1.72it/s]\u001b[A\n",
            "train loss:0.1046, train accuracy:1.0000.:  30%|███       | 97/320 [01:01<02:29,  1.49it/s]\u001b[A\n",
            "train loss:0.0880, train accuracy:1.0000.:  30%|███       | 97/320 [01:01<02:29,  1.49it/s]\u001b[A\n",
            "train loss:0.1112, train accuracy:1.0000.:  31%|███       | 98/320 [01:01<02:29,  1.49it/s]\u001b[A\n",
            "train loss:0.0821, train accuracy:1.0000.:  31%|███       | 99/320 [01:02<02:28,  1.49it/s]\u001b[A\n",
            "train loss:0.0821, train accuracy:1.0000.:  31%|███▏      | 100/320 [01:02<02:16,  1.61it/s]\u001b[A\n",
            "train loss:0.0800, train accuracy:1.0000.:  31%|███▏      | 100/320 [01:02<02:16,  1.61it/s]\u001b[A\n",
            "train loss:0.0907, train accuracy:1.0000.:  32%|███▏      | 101/320 [01:02<02:15,  1.61it/s]\u001b[A\n",
            "train loss:0.0830, train accuracy:1.0000.:  32%|███▏      | 102/320 [01:04<02:15,  1.61it/s]\u001b[A\n",
            "train loss:0.0830, train accuracy:1.0000.:  32%|███▏      | 103/320 [01:04<02:06,  1.72it/s]\u001b[A\n",
            "train loss:0.0885, train accuracy:1.0000.:  32%|███▏      | 103/320 [01:04<02:06,  1.72it/s]\u001b[A\n",
            "train loss:0.0845, train accuracy:1.0000.:  32%|███▎      | 104/320 [01:04<02:05,  1.72it/s]\u001b[A\n",
            "train loss:0.0794, train accuracy:1.0000.:  33%|███▎      | 105/320 [01:05<02:05,  1.72it/s]\u001b[A\n",
            "train loss:0.0794, train accuracy:1.0000.:  33%|███▎      | 106/320 [01:05<02:01,  1.76it/s]\u001b[A\n",
            "train loss:0.0849, train accuracy:1.0000.:  33%|███▎      | 106/320 [01:05<02:01,  1.76it/s]\u001b[A\n",
            "train loss:0.0734, train accuracy:1.0000.:  33%|███▎      | 107/320 [01:05<02:01,  1.76it/s]\u001b[A\n",
            "train loss:0.0734, train accuracy:1.0000.:  34%|███▍      | 108/320 [01:05<01:35,  2.22it/s]\u001b[A\n",
            "train loss:0.0785, train accuracy:1.0000.:  34%|███▍      | 108/320 [01:07<01:35,  2.22it/s]\u001b[A\n",
            "train loss:0.0785, train accuracy:1.0000.:  34%|███▍      | 109/320 [01:07<02:21,  1.49it/s]\u001b[A\n",
            "train loss:0.0669, train accuracy:1.0000.:  34%|███▍      | 109/320 [01:07<02:21,  1.49it/s]\u001b[A\n",
            "train loss:0.0801, train accuracy:1.0000.:  34%|███▍      | 110/320 [01:07<02:20,  1.49it/s]\u001b[A\n",
            "train loss:0.0730, train accuracy:1.0000.:  35%|███▍      | 111/320 [01:09<02:20,  1.49it/s]\u001b[A\n",
            "train loss:0.0730, train accuracy:1.0000.:  35%|███▌      | 112/320 [01:09<02:05,  1.65it/s]\u001b[A\n",
            "train loss:0.0765, train accuracy:1.0000.:  35%|███▌      | 112/320 [01:10<02:05,  1.65it/s]\u001b[A\n",
            "train loss:0.0765, train accuracy:1.0000.:  35%|███▌      | 113/320 [01:10<02:14,  1.54it/s]\u001b[A\n",
            "train loss:0.0707, train accuracy:1.0000.:  35%|███▌      | 113/320 [01:10<02:14,  1.54it/s]\u001b[A\n",
            "train loss:0.0707, train accuracy:1.0000.:  36%|███▌      | 114/320 [01:10<01:54,  1.80it/s]\u001b[A\n",
            "train loss:0.0703, train accuracy:1.0000.:  36%|███▌      | 114/320 [01:11<01:54,  1.80it/s]\u001b[A\n",
            "train loss:0.0703, train accuracy:1.0000.:  36%|███▌      | 115/320 [01:11<02:34,  1.33it/s]\u001b[A\n",
            "train loss:0.0724, train accuracy:1.0000.:  36%|███▌      | 115/320 [01:11<02:34,  1.33it/s]\u001b[A\n",
            "train loss:0.0694, train accuracy:1.0000.:  36%|███▋      | 116/320 [01:11<02:33,  1.33it/s]\u001b[A\n",
            "train loss:0.0643, train accuracy:1.0000.:  37%|███▋      | 117/320 [01:13<02:32,  1.33it/s]\u001b[A\n",
            "train loss:0.0643, train accuracy:1.0000.:  37%|███▋      | 118/320 [01:13<02:08,  1.57it/s]\u001b[A\n",
            "train loss:0.0684, train accuracy:1.0000.:  37%|███▋      | 118/320 [01:13<02:08,  1.57it/s]\u001b[A\n",
            "train loss:0.0652, train accuracy:1.0000.:  37%|███▋      | 119/320 [01:13<02:07,  1.57it/s]\u001b[A\n",
            "train loss:0.0651, train accuracy:1.0000.:  38%|███▊      | 120/320 [01:14<02:07,  1.57it/s]\u001b[A\n",
            "train loss:0.0651, train accuracy:1.0000.:  38%|███▊      | 121/320 [01:14<02:00,  1.66it/s]\u001b[A\n",
            "train loss:0.0635, train accuracy:1.0000.:  38%|███▊      | 121/320 [01:15<02:00,  1.66it/s]\u001b[A\n",
            "train loss:0.0633, train accuracy:1.0000.:  38%|███▊      | 122/320 [01:15<01:59,  1.66it/s]\u001b[A\n",
            "train loss:0.0793, train accuracy:1.0000.:  38%|███▊      | 123/320 [01:16<01:58,  1.66it/s]\u001b[A\n",
            "train loss:0.0793, train accuracy:1.0000.:  39%|███▉      | 124/320 [01:16<01:51,  1.76it/s]\u001b[A\n",
            "train loss:0.0706, train accuracy:1.0000.:  39%|███▉      | 124/320 [01:16<01:51,  1.76it/s]\u001b[A\n",
            "train loss:0.0704, train accuracy:1.0000.:  39%|███▉      | 125/320 [01:16<01:51,  1.76it/s]\u001b[A\n",
            "train loss:0.0692, train accuracy:1.0000.:  39%|███▉      | 126/320 [01:18<01:50,  1.76it/s]\u001b[A\n",
            "train loss:0.0692, train accuracy:1.0000.:  40%|███▉      | 127/320 [01:18<01:48,  1.78it/s]\u001b[A\n",
            "train loss:0.0662, train accuracy:1.0000.:  40%|███▉      | 127/320 [01:18<01:48,  1.78it/s]\u001b[A\n",
            "train loss:0.0612, train accuracy:1.0000.:  40%|████      | 128/320 [01:19<01:47,  1.78it/s]\u001b[A\n",
            "train loss:0.0612, train accuracy:1.0000.:  40%|████      | 129/320 [01:19<01:56,  1.64it/s]\u001b[A\n",
            "train loss:0.0603, train accuracy:1.0000.:  40%|████      | 129/320 [01:21<01:56,  1.64it/s]\u001b[A\n",
            "train loss:0.0603, train accuracy:1.0000.:  41%|████      | 130/320 [01:21<02:25,  1.31it/s]\u001b[A\n",
            "train loss:0.0615, train accuracy:1.0000.:  41%|████      | 130/320 [01:21<02:25,  1.31it/s]\u001b[A\n",
            "train loss:0.0581, train accuracy:1.0000.:  41%|████      | 131/320 [01:21<02:24,  1.31it/s]\u001b[A\n",
            "train loss:0.0593, train accuracy:1.0000.:  41%|████▏     | 132/320 [01:22<02:23,  1.31it/s]\u001b[A\n",
            "train loss:0.0593, train accuracy:1.0000.:  42%|████▏     | 133/320 [01:22<02:03,  1.52it/s]\u001b[A\n",
            "train loss:0.0550, train accuracy:1.0000.:  42%|████▏     | 133/320 [01:22<02:03,  1.52it/s]\u001b[A\n",
            "train loss:0.0545, train accuracy:1.0000.:  42%|████▏     | 134/320 [01:22<02:02,  1.52it/s]\u001b[A\n",
            "train loss:0.0545, train accuracy:1.0000.:  42%|████▏     | 135/320 [01:22<01:31,  2.02it/s]\u001b[A\n",
            "train loss:0.0582, train accuracy:1.0000.:  42%|████▏     | 135/320 [01:24<01:31,  2.02it/s]\u001b[A\n",
            "train loss:0.0582, train accuracy:1.0000.:  42%|████▎     | 136/320 [01:24<02:06,  1.46it/s]\u001b[A\n",
            "train loss:0.0585, train accuracy:1.0000.:  42%|████▎     | 136/320 [01:24<02:06,  1.46it/s]\u001b[A\n",
            "train loss:0.0513, train accuracy:1.0000.:  43%|████▎     | 137/320 [01:24<02:05,  1.46it/s]\u001b[A\n",
            "train loss:0.0513, train accuracy:1.0000.:  43%|████▎     | 138/320 [01:24<01:28,  2.05it/s]\u001b[A\n",
            "train loss:0.0724, train accuracy:1.0000.:  43%|████▎     | 138/320 [01:26<01:28,  2.05it/s]\u001b[A\n",
            "train loss:0.0724, train accuracy:1.0000.:  43%|████▎     | 139/320 [01:26<02:18,  1.31it/s]\u001b[A\n",
            "train loss:0.0648, train accuracy:1.0000.:  43%|████▎     | 139/320 [01:26<02:18,  1.31it/s]\u001b[A\n",
            "train loss:0.0509, train accuracy:1.0000.:  44%|████▍     | 140/320 [01:26<02:17,  1.31it/s]\u001b[A\n",
            "train loss:0.0524, train accuracy:1.0000.:  44%|████▍     | 141/320 [01:28<02:17,  1.31it/s]\u001b[A\n",
            "train loss:0.0524, train accuracy:1.0000.:  44%|████▍     | 142/320 [01:28<01:55,  1.54it/s]\u001b[A\n",
            "train loss:0.0586, train accuracy:1.0000.:  44%|████▍     | 142/320 [01:28<01:55,  1.54it/s]\u001b[A\n",
            "train loss:0.0483, train accuracy:1.0000.:  45%|████▍     | 143/320 [01:28<01:54,  1.54it/s]\u001b[A\n",
            "train loss:0.0566, train accuracy:1.0000.:  45%|████▌     | 144/320 [01:30<01:54,  1.54it/s]\u001b[A\n",
            "train loss:0.0566, train accuracy:1.0000.:  45%|████▌     | 145/320 [01:30<02:09,  1.35it/s]\u001b[A\n",
            "train loss:0.0613, train accuracy:1.0000.:  45%|████▌     | 145/320 [01:30<02:09,  1.35it/s]\u001b[A\n",
            "train loss:0.0585, train accuracy:1.0000.:  46%|████▌     | 146/320 [01:31<02:09,  1.35it/s]\u001b[A\n",
            "train loss:0.0585, train accuracy:1.0000.:  46%|████▌     | 147/320 [01:31<01:36,  1.79it/s]\u001b[A\n",
            "train loss:0.0736, train accuracy:1.0000.:  46%|████▌     | 147/320 [01:32<01:36,  1.79it/s]\u001b[A\n",
            "train loss:0.0736, train accuracy:1.0000.:  46%|████▋     | 148/320 [01:32<02:15,  1.27it/s]\u001b[A\n",
            "train loss:0.0545, train accuracy:1.0000.:  46%|████▋     | 148/320 [01:33<02:15,  1.27it/s]\u001b[A\n",
            "train loss:0.0529, train accuracy:1.0000.:  47%|████▋     | 149/320 [01:33<02:14,  1.27it/s]\u001b[A\n",
            "train loss:0.0539, train accuracy:1.0000.:  47%|████▋     | 150/320 [01:34<02:14,  1.27it/s]\u001b[A\n",
            "train loss:0.0539, train accuracy:1.0000.:  47%|████▋     | 151/320 [01:34<01:53,  1.49it/s]\u001b[A\n",
            "train loss:0.0498, train accuracy:1.0000.:  47%|████▋     | 151/320 [01:34<01:53,  1.49it/s]\u001b[A\n",
            "train loss:0.0567, train accuracy:1.0000.:  48%|████▊     | 152/320 [01:34<01:52,  1.49it/s]\u001b[A\n",
            "train loss:0.0669, train accuracy:1.0000.:  48%|████▊     | 153/320 [01:35<01:52,  1.49it/s]\u001b[A\n",
            "train loss:0.0669, train accuracy:1.0000.:  48%|████▊     | 154/320 [01:35<01:40,  1.64it/s]\u001b[A\n",
            "train loss:0.0439, train accuracy:1.0000.:  48%|████▊     | 154/320 [01:36<01:40,  1.64it/s]\u001b[A\n",
            "train loss:0.0436, train accuracy:1.0000.:  48%|████▊     | 155/320 [01:36<01:40,  1.64it/s]\u001b[A\n",
            "train loss:0.0492, train accuracy:1.0000.:  49%|████▉     | 156/320 [01:37<01:39,  1.64it/s]\u001b[A\n",
            "train loss:0.0492, train accuracy:1.0000.:  49%|████▉     | 157/320 [01:37<01:33,  1.75it/s]\u001b[A\n",
            "train loss:0.0410, train accuracy:1.0000.:  49%|████▉     | 157/320 [01:37<01:33,  1.75it/s]\u001b[A\n",
            "train loss:0.0430, train accuracy:1.0000.:  49%|████▉     | 158/320 [01:37<01:32,  1.75it/s]\u001b[A\n",
            "train loss:0.0478, train accuracy:1.0000.:  50%|████▉     | 159/320 [01:38<01:31,  1.75it/s]\u001b[A\n",
            "train loss:0.0478, train accuracy:1.0000.:  50%|█████     | 160/320 [01:38<01:27,  1.83it/s]\u001b[A\n",
            "train loss:0.0482, train accuracy:1.0000.:  50%|█████     | 160/320 [01:39<01:27,  1.83it/s]\u001b[A\n",
            "train loss:0.0482, train accuracy:1.0000.:  50%|█████     | 161/320 [01:39<01:35,  1.67it/s]\u001b[A\n",
            "train loss:0.0491, train accuracy:1.0000.:  50%|█████     | 161/320 [01:40<01:35,  1.67it/s]\u001b[A\n",
            "train loss:0.0491, train accuracy:1.0000.:  51%|█████     | 162/320 [01:40<01:22,  1.92it/s]\u001b[A\n",
            "train loss:0.0501, train accuracy:1.0000.:  51%|█████     | 162/320 [01:41<01:22,  1.92it/s]\u001b[A\n",
            "train loss:0.0501, train accuracy:1.0000.:  51%|█████     | 163/320 [01:41<01:50,  1.42it/s]\u001b[A\n",
            "train loss:0.0456, train accuracy:1.0000.:  51%|█████     | 163/320 [01:41<01:50,  1.42it/s]\u001b[A\n",
            "train loss:0.0428, train accuracy:1.0000.:  51%|█████▏    | 164/320 [01:41<01:49,  1.42it/s]\u001b[A\n",
            "train loss:0.0427, train accuracy:1.0000.:  52%|█████▏    | 165/320 [01:43<01:49,  1.42it/s]\u001b[A\n",
            "train loss:0.0427, train accuracy:1.0000.:  52%|█████▏    | 166/320 [01:43<01:38,  1.57it/s]\u001b[A\n",
            "train loss:0.0453, train accuracy:1.0000.:  52%|█████▏    | 166/320 [01:43<01:38,  1.57it/s]\u001b[A\n",
            "train loss:0.0493, train accuracy:1.0000.:  52%|█████▏    | 167/320 [01:43<01:37,  1.57it/s]\u001b[A\n",
            "train loss:0.0493, train accuracy:1.0000.:  52%|█████▎    | 168/320 [01:43<01:10,  2.16it/s]\u001b[A\n",
            "train loss:0.0428, train accuracy:1.0000.:  52%|█████▎    | 168/320 [01:45<01:10,  2.16it/s]\u001b[A\n",
            "train loss:0.0428, train accuracy:1.0000.:  53%|█████▎    | 169/320 [01:45<01:53,  1.33it/s]\u001b[A\n",
            "train loss:0.0438, train accuracy:1.0000.:  53%|█████▎    | 169/320 [01:45<01:53,  1.33it/s]\u001b[A\n",
            "train loss:0.0419, train accuracy:1.0000.:  53%|█████▎    | 170/320 [01:45<01:52,  1.33it/s]\u001b[A\n",
            "train loss:0.0507, train accuracy:1.0000.:  53%|█████▎    | 171/320 [01:46<01:52,  1.33it/s]\u001b[A\n",
            "train loss:0.0507, train accuracy:1.0000.:  54%|█████▍    | 172/320 [01:46<01:35,  1.54it/s]\u001b[A\n",
            "train loss:0.0445, train accuracy:1.0000.:  54%|█████▍    | 172/320 [01:46<01:35,  1.54it/s]\u001b[A\n",
            "train loss:0.0473, train accuracy:1.0000.:  54%|█████▍    | 173/320 [01:47<01:35,  1.54it/s]\u001b[A\n",
            "train loss:0.0371, train accuracy:1.0000.:  54%|█████▍    | 174/320 [01:48<01:34,  1.54it/s]\u001b[A\n",
            "train loss:0.0371, train accuracy:1.0000.:  55%|█████▍    | 175/320 [01:48<01:25,  1.69it/s]\u001b[A\n",
            "train loss:0.0425, train accuracy:1.0000.:  55%|█████▍    | 175/320 [01:48<01:25,  1.69it/s]\u001b[A\n",
            "train loss:0.0501, train accuracy:1.0000.:  55%|█████▌    | 176/320 [01:49<01:25,  1.69it/s]\u001b[A\n",
            "train loss:0.0501, train accuracy:1.0000.:  55%|█████▌    | 177/320 [01:49<01:20,  1.77it/s]\u001b[A\n",
            "train loss:0.0430, train accuracy:1.0000.:  55%|█████▌    | 177/320 [01:50<01:20,  1.77it/s]\u001b[A\n",
            "train loss:0.0430, train accuracy:1.0000.:  56%|█████▌    | 178/320 [01:50<01:42,  1.38it/s]\u001b[A\n",
            "train loss:0.0427, train accuracy:1.0000.:  56%|█████▌    | 178/320 [01:51<01:42,  1.38it/s]\u001b[A\n",
            "train loss:0.0398, train accuracy:1.0000.:  56%|█████▌    | 179/320 [01:51<01:41,  1.38it/s]\u001b[A\n",
            "train loss:0.0355, train accuracy:1.0000.:  56%|█████▋    | 180/320 [01:52<01:41,  1.38it/s]\u001b[A\n",
            "train loss:0.0355, train accuracy:1.0000.:  57%|█████▋    | 181/320 [01:52<01:28,  1.58it/s]\u001b[A\n",
            "train loss:0.0362, train accuracy:1.0000.:  57%|█████▋    | 181/320 [01:52<01:28,  1.58it/s]\u001b[A\n",
            "train loss:0.0363, train accuracy:1.0000.:  57%|█████▋    | 182/320 [01:52<01:27,  1.58it/s]\u001b[A\n",
            "train loss:0.0352, train accuracy:1.0000.:  57%|█████▋    | 183/320 [01:53<01:26,  1.58it/s]\u001b[A\n",
            "train loss:0.0352, train accuracy:1.0000.:  57%|█████▊    | 184/320 [01:54<01:19,  1.70it/s]\u001b[A\n",
            "train loss:0.0458, train accuracy:1.0000.:  57%|█████▊    | 184/320 [01:54<01:19,  1.70it/s]\u001b[A\n",
            "train loss:0.0413, train accuracy:1.0000.:  58%|█████▊    | 185/320 [01:54<01:19,  1.70it/s]\u001b[A\n",
            "train loss:0.0500, train accuracy:1.0000.:  58%|█████▊    | 186/320 [01:55<01:18,  1.70it/s]\u001b[A\n",
            "train loss:0.0500, train accuracy:1.0000.:  58%|█████▊    | 187/320 [01:55<01:15,  1.77it/s]\u001b[A\n",
            "train loss:0.0401, train accuracy:1.0000.:  58%|█████▊    | 187/320 [01:55<01:15,  1.77it/s]\u001b[A\n",
            "train loss:0.0360, train accuracy:1.0000.:  59%|█████▉    | 188/320 [01:55<01:14,  1.77it/s]\u001b[A\n",
            "train loss:0.0360, train accuracy:1.0000.:  59%|█████▉    | 189/320 [01:55<00:57,  2.27it/s]\u001b[A\n",
            "train loss:0.0346, train accuracy:1.0000.:  59%|█████▉    | 189/320 [01:57<00:57,  2.27it/s]\u001b[A\n",
            "train loss:0.0346, train accuracy:1.0000.:  59%|█████▉    | 190/320 [01:57<01:28,  1.47it/s]\u001b[A\n",
            "train loss:0.0379, train accuracy:1.0000.:  59%|█████▉    | 190/320 [01:57<01:28,  1.47it/s]\u001b[A\n",
            "train loss:0.0315, train accuracy:1.0000.:  60%|█████▉    | 191/320 [01:57<01:27,  1.47it/s]\u001b[A\n",
            "train loss:0.0317, train accuracy:1.0000.:  60%|██████    | 192/320 [02:00<01:26,  1.47it/s]\u001b[A\n",
            "train loss:0.0317, train accuracy:1.0000.:  60%|██████    | 193/320 [02:00<01:34,  1.35it/s]\u001b[A\n",
            "train loss:0.0395, train accuracy:1.0000.:  60%|██████    | 193/320 [02:00<01:34,  1.35it/s]\u001b[A\n",
            "train loss:0.0316, train accuracy:1.0000.:  61%|██████    | 194/320 [02:00<01:33,  1.35it/s]\u001b[A\n",
            "train loss:0.0361, train accuracy:1.0000.:  61%|██████    | 195/320 [02:01<01:32,  1.35it/s]\u001b[A\n",
            "train loss:0.0361, train accuracy:1.0000.:  61%|██████▏   | 196/320 [02:01<01:20,  1.53it/s]\u001b[A\n",
            "train loss:0.0345, train accuracy:1.0000.:  61%|██████▏   | 196/320 [02:01<01:20,  1.53it/s]\u001b[A\n",
            "train loss:0.0379, train accuracy:1.0000.:  62%|██████▏   | 197/320 [02:01<01:20,  1.53it/s]\u001b[A\n",
            "train loss:0.0379, train accuracy:1.0000.:  62%|██████▏   | 198/320 [02:01<01:00,  2.00it/s]\u001b[A\n",
            "train loss:0.0367, train accuracy:1.0000.:  62%|██████▏   | 198/320 [02:03<01:00,  2.00it/s]\u001b[A\n",
            "train loss:0.0367, train accuracy:1.0000.:  62%|██████▏   | 199/320 [02:03<01:18,  1.54it/s]\u001b[A\n",
            "train loss:0.0349, train accuracy:1.0000.:  62%|██████▏   | 199/320 [02:03<01:18,  1.54it/s]\u001b[A\n",
            "train loss:0.0315, train accuracy:1.0000.:  62%|██████▎   | 200/320 [02:03<01:17,  1.54it/s]\u001b[A\n",
            "train loss:0.0324, train accuracy:1.0000.:  63%|██████▎   | 201/320 [02:04<01:17,  1.54it/s]\u001b[A\n",
            "train loss:0.0324, train accuracy:1.0000.:  63%|██████▎   | 202/320 [02:04<01:09,  1.70it/s]\u001b[A\n",
            "train loss:0.0356, train accuracy:1.0000.:  63%|██████▎   | 202/320 [02:04<01:09,  1.70it/s]\u001b[A\n",
            "train loss:0.0437, train accuracy:1.0000.:  63%|██████▎   | 203/320 [02:04<01:08,  1.70it/s]\u001b[A\n",
            "train loss:0.0300, train accuracy:1.0000.:  64%|██████▍   | 204/320 [02:06<01:08,  1.70it/s]\u001b[A\n",
            "train loss:0.0300, train accuracy:1.0000.:  64%|██████▍   | 205/320 [02:06<01:03,  1.80it/s]\u001b[A\n",
            "train loss:0.0336, train accuracy:1.0000.:  64%|██████▍   | 205/320 [02:06<01:03,  1.80it/s]\u001b[A\n",
            "train loss:0.0372, train accuracy:1.0000.:  64%|██████▍   | 206/320 [02:06<01:03,  1.80it/s]\u001b[A\n",
            "train loss:0.0336, train accuracy:1.0000.:  65%|██████▍   | 207/320 [02:07<01:02,  1.80it/s]\u001b[A\n",
            "train loss:0.0336, train accuracy:1.0000.:  65%|██████▌   | 208/320 [02:07<01:02,  1.80it/s]\u001b[A\n",
            "train loss:0.0398, train accuracy:1.0000.:  65%|██████▌   | 208/320 [02:09<01:02,  1.80it/s]\u001b[A\n",
            "train loss:0.0398, train accuracy:1.0000.:  65%|██████▌   | 209/320 [02:09<01:16,  1.46it/s]\u001b[A\n",
            "train loss:0.0335, train accuracy:1.0000.:  65%|██████▌   | 209/320 [02:09<01:16,  1.46it/s]\u001b[A\n",
            "train loss:0.0335, train accuracy:1.0000.:  66%|██████▌   | 210/320 [02:09<01:05,  1.69it/s]\u001b[A\n",
            "train loss:0.0307, train accuracy:1.0000.:  66%|██████▌   | 210/320 [02:11<01:05,  1.69it/s]\u001b[A\n",
            "train loss:0.0307, train accuracy:1.0000.:  66%|██████▌   | 211/320 [02:11<01:25,  1.28it/s]\u001b[A\n",
            "train loss:0.0352, train accuracy:1.0000.:  66%|██████▌   | 211/320 [02:11<01:25,  1.28it/s]\u001b[A\n",
            "train loss:0.0314, train accuracy:1.0000.:  66%|██████▋   | 212/320 [02:11<01:24,  1.28it/s]\u001b[A\n",
            "train loss:0.0314, train accuracy:1.0000.:  67%|██████▋   | 213/320 [02:11<00:56,  1.91it/s]\u001b[A\n",
            "train loss:0.0326, train accuracy:1.0000.:  67%|██████▋   | 213/320 [02:12<00:56,  1.91it/s]\u001b[A\n",
            "train loss:0.0326, train accuracy:1.0000.:  67%|██████▋   | 214/320 [02:12<01:15,  1.41it/s]\u001b[A\n",
            "train loss:0.0285, train accuracy:1.0000.:  67%|██████▋   | 214/320 [02:12<01:15,  1.41it/s]\u001b[A\n",
            "train loss:0.0278, train accuracy:1.0000.:  67%|██████▋   | 215/320 [02:12<01:14,  1.41it/s]\u001b[A\n",
            "train loss:0.0306, train accuracy:1.0000.:  68%|██████▊   | 216/320 [02:14<01:13,  1.41it/s]\u001b[A\n",
            "train loss:0.0306, train accuracy:1.0000.:  68%|██████▊   | 217/320 [02:14<01:03,  1.63it/s]\u001b[A\n",
            "train loss:0.0416, train accuracy:1.0000.:  68%|██████▊   | 217/320 [02:14<01:03,  1.63it/s]\u001b[A\n",
            "train loss:0.0378, train accuracy:1.0000.:  68%|██████▊   | 218/320 [02:14<01:02,  1.63it/s]\u001b[A\n",
            "train loss:0.0358, train accuracy:1.0000.:  68%|██████▊   | 219/320 [02:15<01:02,  1.63it/s]\u001b[A\n",
            "train loss:0.0358, train accuracy:1.0000.:  69%|██████▉   | 220/320 [02:15<00:58,  1.70it/s]\u001b[A\n",
            "train loss:0.0329, train accuracy:1.0000.:  69%|██████▉   | 220/320 [02:15<00:58,  1.70it/s]\u001b[A\n",
            "train loss:0.0340, train accuracy:1.0000.:  69%|██████▉   | 221/320 [02:15<00:58,  1.70it/s]\u001b[A\n",
            "train loss:0.0340, train accuracy:1.0000.:  69%|██████▉   | 222/320 [02:15<00:43,  2.27it/s]\u001b[A\n",
            "train loss:0.0343, train accuracy:1.0000.:  69%|██████▉   | 222/320 [02:17<00:43,  2.27it/s]\u001b[A\n",
            "train loss:0.0343, train accuracy:1.0000.:  70%|██████▉   | 223/320 [02:17<00:59,  1.64it/s]\u001b[A\n",
            "train loss:0.0331, train accuracy:1.0000.:  70%|██████▉   | 223/320 [02:17<00:59,  1.64it/s]\u001b[A\n",
            "train loss:0.0288, train accuracy:1.0000.:  70%|███████   | 224/320 [02:18<00:58,  1.64it/s]\u001b[A\n",
            "train loss:0.0288, train accuracy:1.0000.:  70%|███████   | 225/320 [02:18<00:53,  1.79it/s]\u001b[A\n",
            "train loss:0.0271, train accuracy:1.0000.:  70%|███████   | 225/320 [02:19<00:53,  1.79it/s]\u001b[A\n",
            "train loss:0.0271, train accuracy:1.0000.:  71%|███████   | 226/320 [02:19<01:11,  1.31it/s]\u001b[A\n",
            "train loss:0.0317, train accuracy:1.0000.:  71%|███████   | 226/320 [02:20<01:11,  1.31it/s]\u001b[A\n",
            "train loss:0.0329, train accuracy:1.0000.:  71%|███████   | 227/320 [02:20<01:11,  1.31it/s]\u001b[A\n",
            "train loss:0.0301, train accuracy:1.0000.:  71%|███████▏  | 228/320 [02:22<01:10,  1.31it/s]\u001b[A\n",
            "train loss:0.0301, train accuracy:1.0000.:  72%|███████▏  | 229/320 [02:22<01:07,  1.35it/s]\u001b[A\n",
            "train loss:0.0252, train accuracy:1.0000.:  72%|███████▏  | 229/320 [02:22<01:07,  1.35it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  72%|███████▏  | 230/320 [02:22<01:06,  1.35it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  72%|███████▏  | 231/320 [02:22<00:47,  1.86it/s]\u001b[A\n",
            "train loss:0.0319, train accuracy:1.0000.:  72%|███████▏  | 231/320 [02:23<00:47,  1.86it/s]\u001b[A\n",
            "train loss:0.0319, train accuracy:1.0000.:  72%|███████▎  | 232/320 [02:23<01:02,  1.40it/s]\u001b[A\n",
            "train loss:0.0273, train accuracy:1.0000.:  72%|███████▎  | 232/320 [02:23<01:02,  1.40it/s]\u001b[A\n",
            "train loss:0.0310, train accuracy:1.0000.:  73%|███████▎  | 233/320 [02:23<01:01,  1.40it/s]\u001b[A\n",
            "train loss:0.0355, train accuracy:1.0000.:  73%|███████▎  | 234/320 [02:25<01:01,  1.40it/s]\u001b[A\n",
            "train loss:0.0355, train accuracy:1.0000.:  73%|███████▎  | 235/320 [02:25<00:53,  1.60it/s]\u001b[A\n",
            "train loss:0.0297, train accuracy:1.0000.:  73%|███████▎  | 235/320 [02:25<00:53,  1.60it/s]\u001b[A\n",
            "train loss:0.0341, train accuracy:1.0000.:  74%|███████▍  | 236/320 [02:25<00:52,  1.60it/s]\u001b[A\n",
            "train loss:0.0325, train accuracy:1.0000.:  74%|███████▍  | 237/320 [02:26<00:51,  1.60it/s]\u001b[A\n",
            "train loss:0.0325, train accuracy:1.0000.:  74%|███████▍  | 238/320 [02:26<00:47,  1.74it/s]\u001b[A\n",
            "train loss:0.0273, train accuracy:1.0000.:  74%|███████▍  | 238/320 [02:26<00:47,  1.74it/s]\u001b[A\n",
            "train loss:0.0251, train accuracy:1.0000.:  75%|███████▍  | 239/320 [02:26<00:46,  1.74it/s]\u001b[A\n",
            "train loss:0.0290, train accuracy:1.0000.:  75%|███████▌  | 240/320 [02:29<00:46,  1.74it/s]\u001b[A\n",
            "train loss:0.0290, train accuracy:1.0000.:  75%|███████▌  | 241/320 [02:29<00:52,  1.50it/s]\u001b[A\n",
            "train loss:0.0271, train accuracy:1.0000.:  75%|███████▌  | 241/320 [02:29<00:52,  1.50it/s]\u001b[A\n",
            "train loss:0.0309, train accuracy:1.0000.:  76%|███████▌  | 242/320 [02:29<00:52,  1.50it/s]\u001b[A\n",
            "train loss:0.0301, train accuracy:1.0000.:  76%|███████▌  | 243/320 [02:30<00:51,  1.50it/s]\u001b[A\n",
            "train loss:0.0301, train accuracy:1.0000.:  76%|███████▋  | 244/320 [02:30<00:46,  1.64it/s]\u001b[A\n",
            "train loss:0.0288, train accuracy:1.0000.:  76%|███████▋  | 244/320 [02:30<00:46,  1.64it/s]\u001b[A\n",
            "train loss:0.0240, train accuracy:1.0000.:  77%|███████▋  | 245/320 [02:30<00:45,  1.64it/s]\u001b[A\n",
            "train loss:0.0275, train accuracy:1.0000.:  77%|███████▋  | 246/320 [02:32<00:45,  1.64it/s]\u001b[A\n",
            "train loss:0.0275, train accuracy:1.0000.:  77%|███████▋  | 247/320 [02:32<00:42,  1.73it/s]\u001b[A\n",
            "train loss:0.0343, train accuracy:1.0000.:  77%|███████▋  | 247/320 [02:32<00:42,  1.73it/s]\u001b[A\n",
            "train loss:0.0284, train accuracy:1.0000.:  78%|███████▊  | 248/320 [02:32<00:41,  1.73it/s]\u001b[A\n",
            "train loss:0.0263, train accuracy:1.0000.:  78%|███████▊  | 249/320 [02:34<00:41,  1.73it/s]\u001b[A\n",
            "train loss:0.0263, train accuracy:1.0000.:  78%|███████▊  | 250/320 [02:34<00:42,  1.65it/s]\u001b[A\n",
            "train loss:0.0239, train accuracy:1.0000.:  78%|███████▊  | 250/320 [02:34<00:42,  1.65it/s]\u001b[A\n",
            "train loss:0.0256, train accuracy:1.0000.:  78%|███████▊  | 251/320 [02:34<00:41,  1.65it/s]\u001b[A\n",
            "train loss:0.0256, train accuracy:1.0000.:  79%|███████▉  | 252/320 [02:34<00:32,  2.09it/s]\u001b[A\n",
            "train loss:0.0284, train accuracy:1.0000.:  79%|███████▉  | 252/320 [02:36<00:32,  2.09it/s]\u001b[A\n",
            "train loss:0.0284, train accuracy:1.0000.:  79%|███████▉  | 253/320 [02:36<00:43,  1.54it/s]\u001b[A\n",
            "train loss:0.0294, train accuracy:1.0000.:  79%|███████▉  | 253/320 [02:36<00:43,  1.54it/s]\u001b[A\n",
            "train loss:0.0272, train accuracy:1.0000.:  79%|███████▉  | 254/320 [02:36<00:42,  1.54it/s]\u001b[A\n",
            "train loss:0.0229, train accuracy:1.0000.:  80%|███████▉  | 255/320 [02:37<00:42,  1.54it/s]\u001b[A\n",
            "train loss:0.0229, train accuracy:1.0000.:  80%|████████  | 256/320 [02:37<00:38,  1.67it/s]\u001b[A\n",
            "train loss:0.0259, train accuracy:1.0000.:  80%|████████  | 256/320 [02:38<00:38,  1.67it/s]\u001b[A\n",
            "train loss:0.0259, train accuracy:1.0000.:  80%|████████  | 257/320 [02:38<00:40,  1.56it/s]\u001b[A\n",
            "train loss:0.0258, train accuracy:1.0000.:  80%|████████  | 257/320 [02:38<00:40,  1.56it/s]\u001b[A\n",
            "train loss:0.0258, train accuracy:1.0000.:  81%|████████  | 258/320 [02:38<00:34,  1.82it/s]\u001b[A\n",
            "train loss:0.0336, train accuracy:1.0000.:  81%|████████  | 258/320 [02:40<00:34,  1.82it/s]\u001b[A\n",
            "train loss:0.0336, train accuracy:1.0000.:  81%|████████  | 259/320 [02:40<00:44,  1.36it/s]\u001b[A\n",
            "train loss:0.0268, train accuracy:1.0000.:  81%|████████  | 259/320 [02:40<00:44,  1.36it/s]\u001b[A\n",
            "train loss:0.0259, train accuracy:1.0000.:  81%|████████▏ | 260/320 [02:40<00:44,  1.36it/s]\u001b[A\n",
            "train loss:0.0266, train accuracy:1.0000.:  82%|████████▏ | 261/320 [02:41<00:43,  1.36it/s]\u001b[A\n",
            "train loss:0.0266, train accuracy:1.0000.:  82%|████████▏ | 262/320 [02:41<00:36,  1.59it/s]\u001b[A\n",
            "train loss:0.0217, train accuracy:1.0000.:  82%|████████▏ | 262/320 [02:41<00:36,  1.59it/s]\u001b[A\n",
            "train loss:0.0338, train accuracy:1.0000.:  82%|████████▏ | 263/320 [02:41<00:35,  1.59it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  82%|████████▎ | 264/320 [02:43<00:35,  1.59it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  83%|████████▎ | 265/320 [02:43<00:31,  1.72it/s]\u001b[A\n",
            "train loss:0.0224, train accuracy:1.0000.:  83%|████████▎ | 265/320 [02:43<00:31,  1.72it/s]\u001b[A\n",
            "train loss:0.0319, train accuracy:1.0000.:  83%|████████▎ | 266/320 [02:43<00:31,  1.72it/s]\u001b[A\n",
            "train loss:0.0272, train accuracy:1.0000.:  83%|████████▎ | 267/320 [02:44<00:30,  1.72it/s]\u001b[A\n",
            "train loss:0.0272, train accuracy:1.0000.:  84%|████████▍ | 268/320 [02:44<00:28,  1.80it/s]\u001b[A\n",
            "train loss:0.0199, train accuracy:1.0000.:  84%|████████▍ | 268/320 [02:44<00:28,  1.80it/s]\u001b[A\n",
            "train loss:0.0263, train accuracy:1.0000.:  84%|████████▍ | 269/320 [02:44<00:28,  1.80it/s]\u001b[A\n",
            "train loss:0.0292, train accuracy:1.0000.:  84%|████████▍ | 270/320 [02:46<00:27,  1.80it/s]\u001b[A\n",
            "train loss:0.0292, train accuracy:1.0000.:  85%|████████▍ | 271/320 [02:46<00:28,  1.71it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  85%|████████▍ | 271/320 [02:46<00:28,  1.71it/s]\u001b[A\n",
            "train loss:0.0261, train accuracy:1.0000.:  85%|████████▌ | 272/320 [02:48<00:28,  1.71it/s]\u001b[A\n",
            "train loss:0.0261, train accuracy:1.0000.:  85%|████████▌ | 273/320 [02:48<00:29,  1.61it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  85%|████████▌ | 273/320 [02:49<00:29,  1.61it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  86%|████████▌ | 274/320 [02:49<00:34,  1.31it/s]\u001b[A\n",
            "train loss:0.0209, train accuracy:1.0000.:  86%|████████▌ | 274/320 [02:49<00:34,  1.31it/s]\u001b[A\n",
            "train loss:0.0228, train accuracy:1.0000.:  86%|████████▌ | 275/320 [02:49<00:34,  1.31it/s]\u001b[A\n",
            "train loss:0.0283, train accuracy:1.0000.:  86%|████████▋ | 276/320 [02:51<00:33,  1.31it/s]\u001b[A\n",
            "train loss:0.0283, train accuracy:1.0000.:  87%|████████▋ | 277/320 [02:51<00:28,  1.51it/s]\u001b[A\n",
            "train loss:0.0233, train accuracy:1.0000.:  87%|████████▋ | 277/320 [02:51<00:28,  1.51it/s]\u001b[A\n",
            "train loss:0.0260, train accuracy:1.0000.:  87%|████████▋ | 278/320 [02:51<00:27,  1.51it/s]\u001b[A\n",
            "train loss:0.0239, train accuracy:1.0000.:  87%|████████▋ | 279/320 [02:52<00:27,  1.51it/s]\u001b[A\n",
            "train loss:0.0239, train accuracy:1.0000.:  88%|████████▊ | 280/320 [02:52<00:24,  1.64it/s]\u001b[A\n",
            "train loss:0.0237, train accuracy:1.0000.:  88%|████████▊ | 280/320 [02:52<00:24,  1.64it/s]\u001b[A\n",
            "train loss:0.0236, train accuracy:1.0000.:  88%|████████▊ | 281/320 [02:52<00:23,  1.64it/s]\u001b[A\n",
            "train loss:0.0243, train accuracy:1.0000.:  88%|████████▊ | 282/320 [02:54<00:23,  1.64it/s]\u001b[A\n",
            "train loss:0.0243, train accuracy:1.0000.:  88%|████████▊ | 283/320 [02:54<00:21,  1.74it/s]\u001b[A\n",
            "train loss:0.0227, train accuracy:1.0000.:  88%|████████▊ | 283/320 [02:54<00:21,  1.74it/s]\u001b[A\n",
            "train loss:0.0221, train accuracy:1.0000.:  89%|████████▉ | 284/320 [02:54<00:20,  1.74it/s]\u001b[A\n",
            "train loss:0.0244, train accuracy:1.0000.:  89%|████████▉ | 285/320 [02:55<00:20,  1.74it/s]\u001b[A\n",
            "train loss:0.0244, train accuracy:1.0000.:  89%|████████▉ | 286/320 [02:55<00:18,  1.80it/s]\u001b[A\n",
            "train loss:0.0206, train accuracy:1.0000.:  89%|████████▉ | 286/320 [02:55<00:18,  1.80it/s]\u001b[A\n",
            "train loss:0.0216, train accuracy:1.0000.:  90%|████████▉ | 287/320 [02:55<00:18,  1.80it/s]\u001b[A\n",
            "train loss:0.0216, train accuracy:1.0000.:  90%|█████████ | 288/320 [02:55<00:13,  2.29it/s]\u001b[A\n",
            "train loss:0.0196, train accuracy:1.0000.:  90%|█████████ | 288/320 [02:58<00:13,  2.29it/s]\u001b[A\n",
            "train loss:0.0196, train accuracy:1.0000.:  90%|█████████ | 289/320 [02:58<00:23,  1.33it/s]\u001b[A\n",
            "train loss:0.0232, train accuracy:1.0000.:  90%|█████████ | 289/320 [02:58<00:23,  1.33it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  91%|█████████ | 290/320 [02:58<00:22,  1.33it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  91%|█████████ | 291/320 [02:58<00:15,  1.82it/s]\u001b[A\n",
            "train loss:0.0234, train accuracy:1.0000.:  91%|█████████ | 291/320 [03:00<00:15,  1.82it/s]\u001b[A\n",
            "train loss:0.0234, train accuracy:1.0000.:  91%|█████████▏| 292/320 [03:00<00:23,  1.18it/s]\u001b[A\n",
            "train loss:0.0245, train accuracy:1.0000.:  91%|█████████▏| 292/320 [03:00<00:23,  1.18it/s]\u001b[A\n",
            "train loss:0.0204, train accuracy:1.0000.:  92%|█████████▏| 293/320 [03:01<00:22,  1.18it/s]\u001b[A\n",
            "train loss:0.0204, train accuracy:1.0000.:  92%|█████████▏| 294/320 [03:01<00:15,  1.69it/s]\u001b[A\n",
            "train loss:0.0268, train accuracy:1.0000.:  92%|█████████▏| 294/320 [03:03<00:15,  1.69it/s]\u001b[A\n",
            "train loss:0.0268, train accuracy:1.0000.:  92%|█████████▏| 295/320 [03:03<00:22,  1.13it/s]\u001b[A\n",
            "train loss:0.0217, train accuracy:1.0000.:  92%|█████████▏| 295/320 [03:03<00:22,  1.13it/s]\u001b[A\n",
            "train loss:0.0223, train accuracy:1.0000.:  92%|█████████▎| 296/320 [03:03<00:21,  1.13it/s]\u001b[A\n",
            "train loss:0.0240, train accuracy:1.0000.:  93%|█████████▎| 297/320 [03:04<00:20,  1.13it/s]\u001b[A\n",
            "train loss:0.0240, train accuracy:1.0000.:  93%|█████████▎| 298/320 [03:04<00:15,  1.38it/s]\u001b[A\n",
            "train loss:0.0197, train accuracy:1.0000.:  93%|█████████▎| 298/320 [03:04<00:15,  1.38it/s]\u001b[A\n",
            "train loss:0.0234, train accuracy:1.0000.:  93%|█████████▎| 299/320 [03:04<00:15,  1.38it/s]\u001b[A\n",
            "train loss:0.0245, train accuracy:1.0000.:  94%|█████████▍| 300/320 [03:06<00:14,  1.38it/s]\u001b[A\n",
            "train loss:0.0245, train accuracy:1.0000.:  94%|█████████▍| 301/320 [03:06<00:12,  1.56it/s]\u001b[A\n",
            "train loss:0.0219, train accuracy:1.0000.:  94%|█████████▍| 301/320 [03:06<00:12,  1.56it/s]\u001b[A\n",
            "train loss:0.0208, train accuracy:1.0000.:  94%|█████████▍| 302/320 [03:06<00:11,  1.56it/s]\u001b[A\n",
            "train loss:0.0211, train accuracy:1.0000.:  95%|█████████▍| 303/320 [03:07<00:10,  1.56it/s]\u001b[A\n",
            "train loss:0.0211, train accuracy:1.0000.:  95%|█████████▌| 304/320 [03:07<00:09,  1.67it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  95%|█████████▌| 304/320 [03:08<00:09,  1.67it/s]\u001b[A\n",
            "train loss:0.0218, train accuracy:1.0000.:  95%|█████████▌| 305/320 [03:08<00:09,  1.52it/s]\u001b[A\n",
            "train loss:0.0242, train accuracy:1.0000.:  95%|█████████▌| 305/320 [03:08<00:09,  1.52it/s]\u001b[A\n",
            "train loss:0.0236, train accuracy:1.0000.:  96%|█████████▌| 306/320 [03:10<00:09,  1.52it/s]\u001b[A\n",
            "train loss:0.0236, train accuracy:1.0000.:  96%|█████████▌| 307/320 [03:10<00:08,  1.44it/s]\u001b[A\n",
            "train loss:0.0200, train accuracy:1.0000.:  96%|█████████▌| 307/320 [03:10<00:08,  1.44it/s]\u001b[A\n",
            "train loss:0.0223, train accuracy:1.0000.:  96%|█████████▋| 308/320 [03:10<00:08,  1.44it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  97%|█████████▋| 309/320 [03:11<00:07,  1.44it/s]\u001b[A\n",
            "train loss:0.0265, train accuracy:1.0000.:  97%|█████████▋| 310/320 [03:11<00:06,  1.60it/s]\u001b[A\n",
            "train loss:0.0209, train accuracy:1.0000.:  97%|█████████▋| 310/320 [03:12<00:06,  1.60it/s]\u001b[A\n",
            "train loss:0.0204, train accuracy:1.0000.:  97%|█████████▋| 311/320 [03:12<00:05,  1.60it/s]\u001b[A\n",
            "train loss:0.0204, train accuracy:1.0000.:  98%|█████████▊| 312/320 [03:12<00:03,  2.13it/s]\u001b[A\n",
            "train loss:0.0208, train accuracy:1.0000.:  98%|█████████▊| 312/320 [03:14<00:03,  2.13it/s]\u001b[A\n",
            "train loss:0.0208, train accuracy:1.0000.:  98%|█████████▊| 313/320 [03:14<00:05,  1.35it/s]\u001b[A\n",
            "train loss:0.0201, train accuracy:1.0000.:  98%|█████████▊| 313/320 [03:14<00:05,  1.35it/s]\u001b[A\n",
            "train loss:0.0185, train accuracy:1.0000.:  98%|█████████▊| 314/320 [03:14<00:04,  1.35it/s]\u001b[A\n",
            "train loss:0.0185, train accuracy:1.0000.:  98%|█████████▊| 315/320 [03:14<00:02,  1.90it/s]\u001b[A\n",
            "train loss:0.0212, train accuracy:1.0000.:  98%|█████████▊| 315/320 [03:15<00:02,  1.90it/s]\u001b[A\n",
            "train loss:0.0212, train accuracy:1.0000.:  99%|█████████▉| 316/320 [03:15<00:02,  1.44it/s]\u001b[A\n",
            "train loss:0.0244, train accuracy:1.0000.:  99%|█████████▉| 316/320 [03:15<00:02,  1.44it/s]\u001b[A\n",
            "train loss:0.0239, train accuracy:1.0000.:  99%|█████████▉| 317/320 [03:15<00:02,  1.44it/s]\u001b[A\n",
            "train loss:0.0186, train accuracy:1.0000.:  99%|█████████▉| 318/320 [03:17<00:01,  1.44it/s]\u001b[A\n",
            "train loss:0.0186, train accuracy:1.0000.: 100%|█████████▉| 319/320 [03:17<00:00,  1.58it/s]\u001b[A\n",
            "train loss:0.0184, train accuracy:1.0000.: 100%|██████████| 320/320 [03:17<00:00,  1.62it/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "############################\n",
        "# train with unfreezing here (should be a single call to your train function)\n",
        "############################\n",
        "train(start_frozen=True, model_unfreeze=6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZItH2lX7k4Yt"
      },
      "source": [
        "You may not see any improvement for your classification task, but unfreezing can help convergence for more difficult image classification tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAXHAUf3EEiE"
      },
      "source": [
        "## 2 Fine-tune a language model - (15 min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu9usOxtjFHL"
      },
      "source": [
        "In this section you will use the gpt-2-simple package [here](https://github.com/minimaxir/gpt-2-simple) to fine-tune the GPT-2 language model on a domain of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K7F19SPQo6U"
      },
      "source": [
        "### 2.1 Generate text from an the pretrained GPT-2 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YLXvK51RnuL"
      },
      "source": [
        "#### Run this code to generate text from a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "WDNOb_H5IRvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6717ae-af60-4713-acba-c21a0c069d4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.25.2)\n",
            "Requirement already satisfied: toposort in /usr/local/lib/python3.10/dist-packages (from gpt-2-simple) (1.10)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.10.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gpt-2-simple) (2024.2.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (3.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gpt-2-simple\n",
        "\n",
        "# the transformers package is built on top of Tensorflow, and the default TF version\n",
        "# for Colab will soon switch to 2.x. We remedy this with the following magic method\n",
        "#import tensorflow.compat.v1 as tf\n",
        "\n",
        "import gpt_2_simple as gpt2\n",
        "import os\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "6aRJ-c9uRMOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef12a5d-417d-4f92-8962-4c3e32956d46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading pretrained model models/124M/model.ckpt\n",
            "Facing off against a formidable opponent, the two sides settle for the last day of the ESL One Cologne 2014 hectic schedule.\n",
            "\n",
            "The two groups will play out in a series of group stages for the 2016 Summer Split. The three groups are:\n",
            "\n",
            "Group A: Winner's Bracket\n",
            "\n",
            "Group B: Group A Winners' Bracket\n",
            "\n",
            "Group C: Group A Next 2\n",
            "\n",
            "Preliminaries\n",
            "\n",
            "Group A: Novac vs Ladder 3\n",
            "\n",
            "Group B: Novac vs Ladder 4\n",
            "\n",
            "Group C: Novac vs Ladder 5\n",
            "\n",
            "Prizes\n",
            "\n",
            "$100,000 is available for Novac and Ladder, while the $20,000 prize pool for Novac and Ladder means that Contenders will win $50,000 of their prize pool\n",
            "\n",
            "Participants\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "Novac 0: The loser of the qualifying match will advance to the playoffs.\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Novac 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Ladder 3: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 3: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 3: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 3: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 3: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Prizes\n",
            "\n",
            "Ladder 2: The winner of the qualifying match will advance to the playoffs.\n",
            "\n",
            "\n",
            "Date:\n",
            "\n",
            "Dec. 5th\n",
            "\n",
            "Ticket Information\n",
            "\n",
            "Date:\n",
            "\n",
            "12:00pm PST - 11:00pm PST\n",
            "\n",
            "Ticket-Specific Information\n",
            "\n",
            "Signup:\n",
            "\n",
            "12:00pm PST - 12:00pm PST\n",
            "\n",
            "First Game\n",
            "\n",
            "12:00pm PST - 1:00pm PST\n",
            "\n",
            "Second Game\n",
            "\n",
            "12:00pm PST - 1:00pm PST\n",
            "\n",
            "Third Game\n",
            "\n",
            "12:00pm PST - 1:00pm PST\n",
            "\n",
            "Last Game\n",
            "\n",
            "12:00pm PST - 1:00pm PST\n",
            "\n",
            "Show Me The Match\n",
            "\n",
            "12:00pm PST - 12:00pm PST\n",
            "\n",
            "Show Me The Match-specific Information\n",
            "\n",
            "International Rules\n",
            "\n",
            "This game is not subject to any of the following rules:\n",
            "\n",
            "1. No conflicting event, matches, or it's third person.\n",
            "\n",
            "2. No online play.\n",
            "\n",
            "3. No paid publicity.\n",
            "\n",
            "4. No ads.\n",
            "\n",
            "5. No false advertising.<|endoftext|>The Canadian Press\n",
            "\n",
            "\n",
            "TORONTO -- The Ontario government says it has been listening to concerns voiced by some top lawyers over its handling of a top-secret file on the controversial offshore tax havens.\n",
            "\n",
            "A senior government official said the government is looking into the matter as part of its ongoing review of the offshore tax haven status of the $5.6 billion global financial centre.\n",
            "\n",
            "Manuel Valls, then chief of the office of the Cabinet Minister responsible for overseeing the case, said the government is looking into the matter separately from the Canada Revenue Agency and the Canadian Securities Administrators Association. The decision was made after the B.C. Supreme Court ruled Friday that the law was constitutional.\n",
            "\n",
            "\"We have heard from the other members and it's a matter that has been discussed internally,\" Valls said in an interview.\n",
            "\n",
            "The review is being conducted by the OPA during an interim government review of\n"
          ]
        }
      ],
      "source": [
        "# This line is necessary to be able to run a new tf session\n",
        "tf.compat.v1.reset_default_graph()\n",
        "# The medium-sized model. IF you run out of memory, try \"124M\" instead\n",
        "model_name = \"124M\"\n",
        "if not os.path.isdir(os.path.join(\"models\", model_name)):\n",
        "\tprint(f\"Downloading {model_name} model...\")\n",
        "\tgpt2.download_gpt2(model_name=model_name)   # model is saved into current directory under /models/124M/\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess, model_name=model_name)\n",
        "gpt2.generate(sess, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHmjSVf_FNHv"
      },
      "source": [
        "### 2.2 Download a text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPXJkNubFyY6"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWkuRjbcFzwb"
      },
      "source": [
        "- Use the provided functions to download your own text dataset\n",
        "- [Project Gutenberg](https://www.gutenberg.org/) is a nice starting point for raw text corpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD45m3IwF9hh"
      },
      "source": [
        "#### Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "ESltl2QM5nxw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce4c13d7-6f7e-43fa-ec79-84c1a76ae03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/data/text/996-0.txt': No such file or directory\n",
            "DL_Lab8_Part_2.txt\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "def extract_zip(zip_path, remove_finished=True):\n",
        "    print('Extracting {}'.format(zip_path))\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(zip_path.replace('.zip', ''))\n",
        "    if remove_finished:\n",
        "        os.remove(zip_path)\n",
        "\n",
        "def download_dataset(url, root='../data'):\n",
        "    if not os.path.exists(os.path.join(root, 'text')):\n",
        "        os.makedirs(os.path.join(root))\n",
        "        datasets.utils.download_url(url, root, 'text.zip', None)\n",
        "        extract_zip(os.path.join(root, 'text.zip'))\n",
        "    return os.path.join(root, 'text')\n",
        "\n",
        "##########################################\n",
        "# Set the url for your dataset here,\n",
        "# move the dataset to the desired location\n",
        "##########################################\n",
        "url = 'https://www.gutenberg.org/files/996/996-0.zip'\n",
        "download_dataset(url)\n",
        "!mv /data/text/996-0.txt /data/text/DL_Lab8_Part_2.txt\n",
        "!ls ../data/text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usQE-rSPZq_X"
      },
      "source": [
        "### 2.3 Fine-tune GPT-2 on your own dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoA0tZZCa_1k"
      },
      "source": [
        "#### TODO:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoU6ML1mbgjP"
      },
      "source": [
        "- Swap out the dataset parameter with the path to your dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pa5vFJ5EUjv"
      },
      "source": [
        "#### Train on your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "WuQ5snl4LuS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4ea058f-7d4a-4b65-bf2d-0ff2b876110b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:03<00:00,  3.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 621157 tokens\n",
            "Training...\n",
            "[1 | 6.46] loss=3.69 avg=3.69\n",
            "[2 | 8.55] loss=3.61 avg=3.65\n",
            "[3 | 10.63] loss=3.40 avg=3.57\n",
            "[4 | 12.72] loss=3.49 avg=3.55\n",
            "[5 | 14.82] loss=3.40 avg=3.52\n",
            "[6 | 16.91] loss=3.43 avg=3.50\n",
            "[7 | 19.02] loss=3.54 avg=3.51\n",
            "[8 | 21.12] loss=3.37 avg=3.49\n",
            "[9 | 23.22] loss=3.30 avg=3.47\n",
            "[10 | 25.32] loss=3.24 avg=3.44\n",
            "[11 | 27.43] loss=3.19 avg=3.42\n",
            "[12 | 29.54] loss=3.27 avg=3.40\n",
            "[13 | 31.66] loss=3.43 avg=3.41\n",
            "[14 | 33.78] loss=3.16 avg=3.39\n",
            "[15 | 35.90] loss=3.40 avg=3.39\n",
            "[16 | 38.02] loss=3.35 avg=3.39\n",
            "[17 | 40.17] loss=3.39 avg=3.39\n",
            "[18 | 42.31] loss=3.20 avg=3.37\n",
            "[19 | 44.44] loss=3.27 avg=3.37\n",
            "[20 | 46.57] loss=3.31 avg=3.37\n",
            "[21 | 48.71] loss=3.25 avg=3.36\n",
            "[22 | 50.84] loss=3.10 avg=3.35\n",
            "[23 | 52.98] loss=3.11 avg=3.33\n",
            "[24 | 55.12] loss=3.03 avg=3.32\n",
            "[25 | 57.26] loss=3.24 avg=3.32\n",
            "[26 | 59.41] loss=3.35 avg=3.32\n",
            "[27 | 61.57] loss=3.09 avg=3.31\n",
            "[28 | 63.72] loss=3.12 avg=3.30\n",
            "[29 | 65.87] loss=3.05 avg=3.29\n",
            "[30 | 68.02] loss=3.14 avg=3.29\n",
            "[31 | 70.17] loss=3.17 avg=3.28\n",
            "[32 | 72.32] loss=3.10 avg=3.27\n",
            "[33 | 74.47] loss=3.14 avg=3.27\n",
            "[34 | 76.62] loss=3.05 avg=3.26\n",
            "[35 | 78.79] loss=3.17 avg=3.26\n",
            "[36 | 80.95] loss=3.08 avg=3.25\n",
            "[37 | 83.12] loss=3.20 avg=3.25\n",
            "[38 | 85.29] loss=3.09 avg=3.25\n",
            "[39 | 87.45] loss=3.11 avg=3.24\n",
            "[40 | 89.66] loss=3.14 avg=3.24\n",
            "[41 | 91.90] loss=3.27 avg=3.24\n",
            "[42 | 94.09] loss=3.13 avg=3.24\n",
            "[43 | 96.27] loss=3.04 avg=3.23\n",
            "[44 | 98.45] loss=3.19 avg=3.23\n",
            "[45 | 100.63] loss=3.00 avg=3.22\n",
            "[46 | 102.81] loss=3.11 avg=3.22\n",
            "[47 | 104.99] loss=3.36 avg=3.22\n",
            "[48 | 107.16] loss=3.15 avg=3.22\n",
            "[49 | 109.34] loss=2.98 avg=3.22\n",
            "[50 | 111.52] loss=2.93 avg=3.21\n",
            "[51 | 113.70] loss=3.20 avg=3.21\n",
            "[52 | 115.89] loss=3.06 avg=3.20\n",
            "[53 | 118.08] loss=3.12 avg=3.20\n",
            "[54 | 120.32] loss=3.05 avg=3.20\n",
            "[55 | 122.53] loss=3.08 avg=3.20\n",
            "[56 | 124.73] loss=2.98 avg=3.19\n",
            "[57 | 126.92] loss=3.01 avg=3.19\n",
            "[58 | 129.15] loss=2.98 avg=3.18\n",
            "[59 | 131.37] loss=3.05 avg=3.18\n",
            "[60 | 133.56] loss=2.91 avg=3.17\n",
            "[61 | 135.77] loss=2.83 avg=3.17\n",
            "[62 | 137.99] loss=2.93 avg=3.16\n",
            "[63 | 140.20] loss=3.01 avg=3.16\n",
            "[64 | 142.40] loss=3.06 avg=3.16\n",
            "[65 | 144.60] loss=3.06 avg=3.15\n",
            "[66 | 146.81] loss=3.00 avg=3.15\n",
            "[67 | 149.01] loss=2.99 avg=3.15\n",
            "[68 | 151.22] loss=2.94 avg=3.14\n",
            "[69 | 153.43] loss=2.92 avg=3.14\n",
            "[70 | 155.64] loss=3.22 avg=3.14\n",
            "[71 | 157.84] loss=3.09 avg=3.14\n",
            "[72 | 160.05] loss=3.06 avg=3.14\n",
            "[73 | 162.27] loss=2.87 avg=3.13\n",
            "[74 | 164.48] loss=2.97 avg=3.13\n",
            "[75 | 166.69] loss=2.95 avg=3.13\n",
            "[76 | 168.90] loss=3.00 avg=3.12\n",
            "[77 | 171.12] loss=3.00 avg=3.12\n",
            "[78 | 173.33] loss=3.14 avg=3.12\n",
            "[79 | 175.55] loss=2.98 avg=3.12\n",
            "[80 | 177.77] loss=2.99 avg=3.12\n",
            "[81 | 179.99] loss=2.85 avg=3.11\n",
            "[82 | 182.20] loss=2.99 avg=3.11\n",
            "[83 | 184.43] loss=3.02 avg=3.11\n",
            "[84 | 186.65] loss=2.93 avg=3.10\n",
            "[85 | 188.88] loss=2.94 avg=3.10\n",
            "[86 | 191.10] loss=2.86 avg=3.10\n",
            "[87 | 193.33] loss=3.00 avg=3.10\n",
            "[88 | 195.55] loss=2.81 avg=3.09\n",
            "[89 | 197.78] loss=2.93 avg=3.09\n",
            "[90 | 200.01] loss=2.96 avg=3.09\n",
            "[91 | 202.23] loss=2.86 avg=3.08\n",
            "[92 | 204.47] loss=2.80 avg=3.08\n",
            "[93 | 206.70] loss=2.78 avg=3.07\n",
            "[94 | 208.94] loss=2.93 avg=3.07\n",
            "[95 | 211.18] loss=2.88 avg=3.07\n",
            "[96 | 213.42] loss=2.77 avg=3.06\n",
            "[97 | 215.67] loss=2.91 avg=3.06\n",
            "[98 | 217.92] loss=3.05 avg=3.06\n",
            "[99 | 220.18] loss=3.20 avg=3.06\n",
            "[100 | 222.43] loss=3.07 avg=3.06\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "c. “If it were not for that I should not have been able to see her;\n",
            "and if she were only a woman of her countenance ‘not very\n",
            "convenient in that case;’’ and though I know her well,’ not enough of\n",
            "the truth can be said of her; and all these she says, it must be so.’\n",
            "\n",
            "“Then I say,” said Don Quixote, “if God could make a woman in\n",
            "the same way as she is, for the good her soul will know that she is\n",
            "a woman of good health, and has the right to govern it; but what does the\n",
            "truth of the matter?’\n",
            "\n",
            "“As an oath to God,’ answered the Don, “I swear no part of\n",
            "those I have seen will go with her without her, for neither does the\n",
            "conversation between them. I have seen, as my father does, a great\n",
            "woman, who wears the most splendid clothes, and is called by many a beauty\n",
            "whose beauty and beauty has so far failed of beauty; and I\n",
            "worship her in many a way, as I have seen that her modesty is as\n",
            "good as that of the rest of mankind. Now, let me tell you, O lady of\n",
            "the house that is with thee, how they tell me how they have so great a\n",
            "father, that his daughter, and her son, and his eldest son, the\n",
            "wife, are all in love, and have made such a happy connection between\n",
            "their two senses.’\n",
            "\n",
            "Thus in short Don Quixote told the story of the affair of the\n",
            "woman\n",
            "Lena de Panza, the daughter of\n",
            "Barbaros, that the husband of his wife, and the husband of his\n",
            "wife, his father-in-law, had become so full of love and passion\n",
            "that they said, “Lena de Panza and his wife had been so great a\n",
            "muse and so generous on both feet that I could not bear to\n",
            "make amends.’\n",
            "\n",
            "And as Lena, having had a daughter by a woman that she considered\n",
            "unworthy, and he in no wise allowed it to be borne, could have given him up,\n",
            "having a daughter by a man of his own right, she said to him, “For\n",
            "the sake of your life, and also the happiness of mine, I am unable to\n",
            "govern my whole household as my father did before I was a lady;\n",
            "in short, the best I can ask for is the same to my daughters or\n",
            "myself.’\n",
            "\n",
            "And she that had given her daughter by such someone, the husband of his\n",
            "wife, was told she was only a woman, and his wife was his\n",
            "wife, whom she was bound to keep.\n",
            "\n",
            "A short time after this, the woman, whom the husband of her husband\n",
            "was very courteous, called to her by name, ‘I am Lenna of Zoraida,\n",
            "the wife of the rich Llanos, and I am his wife Lenny of Malga,\n",
            "and I am his heir to his son Rodrigo López.’’\n",
            "\n",
            "Lena replied, “What?’ ‘By God!’’ and ’Well, if the\n",
            "woman of hers is not a whore, her husband is a whore, because his\n",
            "wife does not obey the will of God, and if a good wife is not bound to obey\n",
            "God, her husband does not obey the will of the soul; and if, on the other\n",
            "hand, the wife of her husband is a dolt, her husband does not obey the will of\n",
            "the spirit of the flesh, and if he does not obey the will of the senses\n",
            "and the will of the intellect, he goes with a sigh’r into\n",
            "the abyss.’ Lona, in answer to the lady, ‘I am Lenna of Zoraida,\n",
            "and I am his wife Lenny of Malga, and I am his son Rodrigo, and\n",
            "I am his son to his son Rodrigo.’\n",
            "\n",
            "As for the lady, who now gave him up for what must be the\n",
            "best thing to do to make his wife happy, she said in a very plain\n",
            "sigh, “As for myself, let me tell you, I am your lady Ana Dulce, daughter\n",
            "of the rich El Toboso; in other words, if my master is a good person,\n",
            "there is no need for me to be so; for I have a daughter by the name\n",
            "of Lenna of Zoraida, for who could have any reason for caring for the\n",
            "wife, whom he so hated, who had no power or power to forgive him. I am\n",
            "Lena and I have four children together\n",
            "\n",
            "[101 | 236.98] loss=2.85 avg=3.06\n",
            "[102 | 239.24] loss=2.87 avg=3.06\n",
            "[103 | 241.51] loss=2.60 avg=3.05\n",
            "[104 | 243.79] loss=2.83 avg=3.05\n",
            "[105 | 246.06] loss=2.80 avg=3.04\n",
            "[106 | 248.33] loss=2.89 avg=3.04\n",
            "[107 | 250.59] loss=2.81 avg=3.04\n",
            "[108 | 252.84] loss=2.79 avg=3.03\n",
            "[109 | 255.10] loss=3.02 avg=3.03\n",
            "[110 | 257.37] loss=2.83 avg=3.03\n",
            "[111 | 259.63] loss=2.88 avg=3.03\n",
            "[112 | 261.89] loss=2.66 avg=3.02\n",
            "[113 | 264.15] loss=2.75 avg=3.02\n",
            "[114 | 266.40] loss=2.96 avg=3.02\n",
            "[115 | 268.66] loss=2.73 avg=3.01\n",
            "[116 | 270.92] loss=2.86 avg=3.01\n",
            "[117 | 273.18] loss=2.81 avg=3.01\n",
            "[118 | 275.43] loss=3.06 avg=3.01\n",
            "[119 | 277.69] loss=2.95 avg=3.01\n",
            "[120 | 279.94] loss=3.03 avg=3.01\n",
            "[121 | 282.19] loss=2.79 avg=3.00\n",
            "[122 | 284.44] loss=2.95 avg=3.00\n",
            "[123 | 286.68] loss=2.86 avg=3.00\n",
            "[124 | 288.93] loss=2.71 avg=3.00\n",
            "[125 | 291.18] loss=3.00 avg=3.00\n",
            "[126 | 293.43] loss=2.67 avg=2.99\n",
            "[127 | 295.68] loss=2.75 avg=2.99\n",
            "[128 | 297.93] loss=2.95 avg=2.99\n",
            "[129 | 300.18] loss=2.77 avg=2.99\n",
            "[130 | 302.43] loss=2.75 avg=2.98\n",
            "[131 | 304.69] loss=2.81 avg=2.98\n",
            "[132 | 306.95] loss=2.86 avg=2.98\n",
            "[133 | 309.21] loss=2.62 avg=2.97\n",
            "[134 | 311.46] loss=2.87 avg=2.97\n",
            "[135 | 313.71] loss=2.82 avg=2.97\n",
            "[136 | 315.96] loss=2.87 avg=2.97\n",
            "[137 | 318.22] loss=2.79 avg=2.97\n",
            "[138 | 320.47] loss=2.74 avg=2.96\n",
            "[139 | 322.73] loss=2.90 avg=2.96\n",
            "[140 | 324.99] loss=2.76 avg=2.96\n",
            "[141 | 327.25] loss=2.75 avg=2.96\n",
            "[142 | 329.51] loss=2.97 avg=2.96\n",
            "[143 | 331.76] loss=3.11 avg=2.96\n",
            "[144 | 334.02] loss=2.87 avg=2.96\n",
            "[145 | 336.27] loss=3.07 avg=2.96\n",
            "[146 | 338.53] loss=2.62 avg=2.96\n",
            "[147 | 340.79] loss=2.57 avg=2.95\n",
            "[148 | 343.05] loss=2.93 avg=2.95\n",
            "[149 | 345.30] loss=3.05 avg=2.95\n",
            "[150 | 347.54] loss=2.70 avg=2.95\n",
            "[151 | 349.79] loss=2.88 avg=2.95\n",
            "[152 | 352.06] loss=2.73 avg=2.94\n",
            "[153 | 354.32] loss=2.81 avg=2.94\n",
            "[154 | 356.57] loss=2.76 avg=2.94\n",
            "[155 | 358.82] loss=2.82 avg=2.94\n",
            "[156 | 361.08] loss=2.86 avg=2.94\n",
            "[157 | 363.34] loss=2.86 avg=2.94\n",
            "[158 | 365.61] loss=2.55 avg=2.93\n",
            "[159 | 367.86] loss=2.68 avg=2.93\n",
            "[160 | 370.12] loss=2.75 avg=2.93\n",
            "[161 | 372.38] loss=2.77 avg=2.92\n",
            "[162 | 374.63] loss=2.82 avg=2.92\n",
            "[163 | 376.89] loss=2.59 avg=2.92\n",
            "[164 | 379.16] loss=2.66 avg=2.92\n",
            "[165 | 381.41] loss=2.56 avg=2.91\n",
            "[166 | 383.67] loss=2.78 avg=2.91\n",
            "[167 | 385.92] loss=2.79 avg=2.91\n",
            "[168 | 388.19] loss=2.44 avg=2.90\n",
            "[169 | 390.45] loss=2.71 avg=2.90\n",
            "[170 | 392.70] loss=2.57 avg=2.90\n",
            "[171 | 394.97] loss=2.83 avg=2.90\n",
            "[172 | 397.23] loss=2.76 avg=2.89\n",
            "[173 | 399.49] loss=2.75 avg=2.89\n",
            "[174 | 401.75] loss=2.65 avg=2.89\n",
            "[175 | 404.01] loss=2.82 avg=2.89\n",
            "[176 | 406.27] loss=2.99 avg=2.89\n",
            "[177 | 408.53] loss=2.98 avg=2.89\n",
            "[178 | 410.78] loss=2.90 avg=2.89\n",
            "[179 | 413.04] loss=2.79 avg=2.89\n",
            "[180 | 415.30] loss=2.99 avg=2.89\n",
            "[181 | 417.56] loss=2.65 avg=2.89\n",
            "[182 | 419.82] loss=2.97 avg=2.89\n",
            "[183 | 422.08] loss=2.97 avg=2.89\n",
            "[184 | 424.35] loss=2.79 avg=2.89\n",
            "[185 | 426.60] loss=2.47 avg=2.88\n",
            "[186 | 428.86] loss=2.74 avg=2.88\n",
            "[187 | 431.13] loss=2.80 avg=2.88\n",
            "[188 | 433.38] loss=2.68 avg=2.88\n",
            "[189 | 435.64] loss=2.68 avg=2.88\n",
            "[190 | 437.90] loss=2.69 avg=2.87\n",
            "[191 | 440.16] loss=2.71 avg=2.87\n",
            "[192 | 442.41] loss=2.61 avg=2.87\n",
            "[193 | 444.68] loss=2.53 avg=2.87\n",
            "[194 | 446.94] loss=2.60 avg=2.86\n",
            "[195 | 449.20] loss=3.00 avg=2.86\n",
            "[196 | 451.46] loss=2.66 avg=2.86\n",
            "[197 | 453.72] loss=2.63 avg=2.86\n",
            "[198 | 455.98] loss=2.74 avg=2.86\n",
            "[199 | 458.24] loss=2.64 avg=2.85\n",
            "[200 | 460.51] loss=2.63 avg=2.85\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "In case you were wondering if I was from Greece\n",
            "nor from Egypt, I have been wandering the country and speaking\n",
            "the language of the people, which you see in my books;\n",
            "but as for this, and the reason for my being there, you cannot\n",
            "not understand my name, for there are no books with which\n",
            "you can learn my language and my name.”\n",
            "\n",
            "“That which I can tell you,” said the servant, “is\n",
            "the Greek alphabet, or at least your own own alphabet, and I\n",
            "think you understand it too.”\n",
            "\n",
            "“And what is it’s alphabet,” returned the master’s answer, “that is\n",
            "the one that I can tell you?”\n",
            "\n",
            "“That is no other than the C, ‘Cerulean’’’” said the servant; “and\n",
            "that which I can only tell you is ‘Empir,’’’’’’’ and ‘Empirism,’’’’’’’’’’ (to him he\n",
            "said ‘Empirism’’’’\n",
            "and ‘Empirism’’’’’ for ‘Empirism’,’’’’’’’’\n",
            "\n",
            "\n",
            "CHAPTER V.\n",
            "WHEREIN DO VIA NUMBERS OF OTHER GOODS BE\n",
            "CASTLE TOGETHER WITH WORTHLESS PROFITS OF NOT BEING DROLLINATED IN THE\n",
            "OTHER BOOKS THAT TREATED WITH SUCH GOOD KNOWLEDGMENTS?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "p44e.jpg (54K)\n",
            "\n",
            "Full Size\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "All the people of the city knew that the author that\n",
            "knows the books, Sancho Panza, came to know of the\n",
            "beginning and object of his fame, and it turned out for him that the author\n",
            "of the book\n",
            "called the author of the books, was of an ancient and noble order;\n",
            "but he who knows this has no time for gossip and has never\n",
            "thought of telling himself.\n",
            "\n",
            "\n",
            "\n",
            "p45a.jpg (155K)\n",
            "\n",
            "Full Size\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "This sage and great bookseller came to know and enjoy\n",
            "his fame through the ears of those who knew him, and,\n",
            "followed by one who knew Sancho Panza, he discovered more\n",
            "his books, and even more the truth. When Sancho saw he had a\n",
            "record of the books he left,\n",
            "he began to doubt whether he had ever read or understood\n",
            "any of them, and he began to wonder why this was the case. To\n",
            "his great great wonder he saw these relics of the past;\n",
            "but as time went on he came to perceive that none the less,\n",
            "and he saw that their owners carried them on horses, and some\n",
            "of them in the baggage of the pilgrims, as well as on their backs,\n",
            "they began to tremble and faint, and so having had a hard time\n",
            "they were brought to the ground and, seeing that they were not\n",
            "found alive, he made them give them up for burial, and in order\n",
            "to make burial comfortable the grave of the bookseller was\n",
            "buried, and so they laid it on the ground, and the bookseller put\n",
            "its weight upon it with every step, as if it were some precious stone\n",
            "stone which had been polished with one of these fine stones.\n",
            "\n",
            "\n",
            "p45b.jpg (168K)\n",
            "\n",
            "Full Size\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "On finding himself alone in the house of the bookseller\n",
            "and seeing the books that had been left behind\n",
            "behind him, and hearing of his madness he came into a most\n",
            "abhorrent plight, and as he was at a loss to find a way out of\n",
            "this intolerable plight he came to be extremely insensible and\n",
            "disappointed, not daring to venture further until,\n",
            "just as he needed to make peace with his master, he began to\n",
            "think of quitting the world, and so, if not to heaven in\n",
            "kind, at least to his own country, as it was the best of Christian\n",
            "desires. So he went with all speed across the country, as if\n",
            "there were no other country in the world, and so he made the best\n",
            "assent he could, and with such speed and speed that everyone on\n",
            "the road went up a step or two to reach his inn, and his\n",
            "master came out of the wood looking as if he were about to break\n",
            "out in a furious passion, for Don Quixote had already said,\n",
            "“To be at your ease I will not venture to say, Sancho, you\n",
            "don’t know anything at all, only that all is\n",
            "\n",
            "[201 | 473.57] loss=2.72 avg=2.85\n",
            "[202 | 475.82] loss=2.53 avg=2.85\n",
            "[203 | 478.07] loss=2.61 avg=2.84\n",
            "[204 | 480.33] loss=2.72 avg=2.84\n",
            "[205 | 482.58] loss=2.67 avg=2.84\n",
            "[206 | 484.84] loss=2.66 avg=2.84\n",
            "[207 | 487.10] loss=2.67 avg=2.84\n",
            "[208 | 489.35] loss=2.55 avg=2.83\n",
            "[209 | 491.60] loss=2.78 avg=2.83\n",
            "[210 | 493.85] loss=2.67 avg=2.83\n",
            "[211 | 496.11] loss=2.79 avg=2.83\n",
            "[212 | 498.37] loss=2.83 avg=2.83\n",
            "[213 | 500.62] loss=2.86 avg=2.83\n",
            "[214 | 502.88] loss=2.74 avg=2.83\n",
            "[215 | 505.13] loss=2.66 avg=2.83\n",
            "[216 | 507.40] loss=2.90 avg=2.83\n",
            "[217 | 509.65] loss=2.67 avg=2.83\n",
            "[218 | 511.90] loss=2.36 avg=2.82\n",
            "[219 | 514.16] loss=2.65 avg=2.82\n",
            "[220 | 516.41] loss=2.73 avg=2.82\n",
            "[221 | 518.67] loss=2.68 avg=2.82\n",
            "[222 | 520.92] loss=2.73 avg=2.82\n",
            "[223 | 523.18] loss=2.63 avg=2.81\n",
            "[224 | 525.43] loss=2.78 avg=2.81\n",
            "[225 | 527.68] loss=2.60 avg=2.81\n",
            "[226 | 529.94] loss=2.57 avg=2.81\n",
            "[227 | 532.20] loss=2.47 avg=2.80\n",
            "[228 | 534.45] loss=2.75 avg=2.80\n",
            "[229 | 536.71] loss=2.39 avg=2.80\n",
            "[230 | 538.97] loss=2.70 avg=2.80\n",
            "[231 | 541.22] loss=3.06 avg=2.80\n",
            "[232 | 543.49] loss=2.64 avg=2.80\n",
            "[233 | 545.75] loss=2.75 avg=2.80\n",
            "[234 | 548.00] loss=2.44 avg=2.80\n",
            "[235 | 550.25] loss=2.75 avg=2.79\n",
            "[236 | 552.50] loss=2.78 avg=2.79\n",
            "[237 | 554.76] loss=2.58 avg=2.79\n",
            "[238 | 557.03] loss=2.64 avg=2.79\n",
            "[239 | 559.28] loss=2.86 avg=2.79\n",
            "[240 | 561.53] loss=2.71 avg=2.79\n",
            "[241 | 563.78] loss=2.52 avg=2.79\n",
            "[242 | 566.03] loss=2.59 avg=2.79\n",
            "[243 | 568.29] loss=2.59 avg=2.78\n",
            "[244 | 570.55] loss=2.51 avg=2.78\n",
            "[245 | 572.81] loss=2.64 avg=2.78\n",
            "[246 | 575.06] loss=2.70 avg=2.78\n",
            "[247 | 577.32] loss=2.38 avg=2.77\n",
            "[248 | 579.58] loss=2.69 avg=2.77\n",
            "[249 | 581.83] loss=2.79 avg=2.77\n",
            "[250 | 584.09] loss=2.82 avg=2.77\n",
            "[251 | 586.34] loss=2.55 avg=2.77\n",
            "[252 | 588.60] loss=2.77 avg=2.77\n",
            "[253 | 590.86] loss=2.81 avg=2.77\n",
            "[254 | 593.11] loss=2.62 avg=2.77\n",
            "[255 | 595.37] loss=2.75 avg=2.77\n",
            "[256 | 597.62] loss=2.54 avg=2.77\n",
            "[257 | 599.88] loss=2.67 avg=2.77\n",
            "[258 | 602.13] loss=2.64 avg=2.76\n",
            "[259 | 604.40] loss=2.61 avg=2.76\n",
            "[260 | 606.65] loss=2.78 avg=2.76\n",
            "[261 | 608.91] loss=2.62 avg=2.76\n",
            "[262 | 611.17] loss=2.68 avg=2.76\n",
            "[263 | 613.43] loss=2.64 avg=2.76\n",
            "[264 | 615.69] loss=2.73 avg=2.76\n",
            "[265 | 617.95] loss=2.64 avg=2.76\n",
            "[266 | 620.20] loss=2.53 avg=2.76\n",
            "[267 | 622.47] loss=2.37 avg=2.75\n",
            "[268 | 624.72] loss=2.48 avg=2.75\n",
            "[269 | 626.99] loss=2.78 avg=2.75\n",
            "[270 | 629.25] loss=2.63 avg=2.75\n",
            "[271 | 631.51] loss=2.56 avg=2.75\n",
            "[272 | 633.77] loss=2.88 avg=2.75\n",
            "[273 | 636.03] loss=2.75 avg=2.75\n",
            "[274 | 638.30] loss=2.34 avg=2.74\n",
            "[275 | 640.55] loss=2.79 avg=2.74\n",
            "[276 | 642.82] loss=2.16 avg=2.74\n",
            "[277 | 645.08] loss=2.91 avg=2.74\n",
            "[278 | 647.33] loss=2.50 avg=2.74\n",
            "[279 | 649.60] loss=2.56 avg=2.73\n",
            "[280 | 651.86] loss=2.58 avg=2.73\n",
            "[281 | 654.12] loss=2.72 avg=2.73\n",
            "[282 | 656.38] loss=2.56 avg=2.73\n",
            "[283 | 658.64] loss=2.54 avg=2.73\n",
            "[284 | 660.90] loss=2.39 avg=2.72\n",
            "[285 | 663.17] loss=2.67 avg=2.72\n",
            "[286 | 665.42] loss=2.59 avg=2.72\n",
            "[287 | 667.69] loss=2.48 avg=2.72\n",
            "[288 | 669.95] loss=2.70 avg=2.72\n",
            "[289 | 672.20] loss=2.52 avg=2.72\n",
            "[290 | 674.47] loss=2.63 avg=2.72\n",
            "[291 | 676.73] loss=2.45 avg=2.71\n",
            "[292 | 679.00] loss=2.73 avg=2.71\n",
            "[293 | 681.25] loss=2.38 avg=2.71\n",
            "[294 | 683.51] loss=2.59 avg=2.71\n",
            "[295 | 685.78] loss=2.59 avg=2.71\n",
            "[296 | 688.05] loss=2.41 avg=2.71\n",
            "[297 | 690.31] loss=2.84 avg=2.71\n",
            "[298 | 692.56] loss=2.86 avg=2.71\n",
            "[299 | 694.83] loss=2.52 avg=2.71\n",
            "[300 | 697.08] loss=2.66 avg=2.71\n",
            "======== SAMPLE 1 ========\n",
            " how was this the best of\n",
            "this hour?”\n",
            "\n",
            "“It was the second,” said Sancho; “ and then with great composure and\n",
            "exhausting force, shaking off the dainty airs, he said, “There we are,\n",
            "three thousand times three thousand, sirs,\n",
            "in this field, so good an one as you see, not to think it could have\n",
            "been any other than this\n",
            "day; for one of the best gifts that can be given an\n",
            "enemy to an enemy state is that he gives you an opportunity of\n",
            "saying thank you.”\n",
            "\n",
            "“No one can know who I am,” returned the landlord; “though I can\n",
            "praise myself for a great master; and if your worship will forgive me\n",
            "any inconvenience, I will take care not to take any further damage.”\n",
            "\n",
            "“Well,” said Sancho, “as long as I have you in the room as I am,\n",
            "I’ll be more of an actor than a farmer, and even if I was, what\n",
            "it would be better if I had them in it—for there is nothing\n",
            "else more pleasing than seeing yourself as an actor in the drama\n",
            "of your profession, if that is what it may be.”\n",
            "\n",
            "“That I am disposed to agree to,” replied Don Quixote; and in short\n",
            "said Sancho, “and let my master and mistress stay, and let me\n",
            "see two others in the room; I wish them to have the pleasure of\n",
            "seeing me; if anyone wants to go up to their wives, either of them\n",
            "I will not let that happen to me; and let us take care to be\n",
            "good to one another; for the best and most profitable of the two professions\n",
            "is to play as you like; for you will never be left worse dressed\n",
            "than when you are engaged to become the bridegroom. And no more are\n",
            "going into the matter than with your servants, who must be servants\n",
            "of your highness; and if my wife will not be well, they are putting\n",
            "me in some dresser to get into her own house; a sort of a\n",
            "dresser’s chamber, with its own curtains, lights, pipes, and\n",
            "dressing-places, which in the house are like a great hall. But no\n",
            "observable thing would be more fitting and proper for me to do for\n",
            "thee, and if, as I am one of those who compose it, I think it is just\n",
            "for you—and we should be in the world for ever!”\n",
            "\n",
            "“Well, we should be in the world,” said Don Quixote.\n",
            "\n",
            "“I do not know what you say, Sancho, but in that way, at\n",
            "one time we did not have so many maidens as you think; and\n",
            "though you may think it would be better for you to have\n",
            "instead of a servant all your years in this house, your wives’\n",
            "wife will be sure to know, and you may be so much more than she\n",
            "would be if you had only had one; nor is it fit\n",
            "me to set myself back too much by going with less wear and tear on\n",
            "the part of a wife or husband; nor is it necessary or advisable\n",
            "to have some maidens in order to give you an excellent experience;\n",
            "I shall give you the pleasure of having me, even without marrying\n",
            "your mistress, in the house; if her pleasure is such that it\n",
            "contains more than the sum of two hundred pounds, you may suppose\n",
            "that you have been to have all that pleasure I have just now\n",
            "indicate, and even more than once more, if, as you are already saying,\n",
            "the beauty of the bridegroom is to be respected, and is not to be\n",
            "expected, as I have just now; all that is requisite for\n",
            "giving you pleasure is enough for you, and is an instant\n",
            "more sufficient than a thousand pounds; and it will be no wonder\n",
            "that your worship should come over to your house, not knowing\n",
            "how to govern yourself, and know how to meddle in others’ affairs;\n",
            "for the more I see that maidens are employed on you, the more I\n",
            "wont be able to keep in my mind the honour your worship\n",
            "should expect from me; but in all that I may afford for\n",
            "you that you may see more in your master’s hand than in his\n",
            "mouth.”\n",
            "\n",
            "“Sir,” said Don Quixote, “is it not no wonder that\n",
            "some people live there, when they take out a house, if\n",
            "any one of the maidens of your house is a servant, and if any of the\n",
            "others are? If you have more than two thousand two hundred,\n",
            "\n",
            "[301 | 710.22] loss=2.63 avg=2.71\n",
            "[302 | 712.49] loss=2.46 avg=2.70\n",
            "[303 | 714.76] loss=2.78 avg=2.70\n",
            "[304 | 717.02] loss=2.44 avg=2.70\n",
            "[305 | 719.27] loss=2.40 avg=2.70\n",
            "[306 | 721.53] loss=2.54 avg=2.70\n",
            "[307 | 723.80] loss=2.62 avg=2.69\n",
            "[308 | 726.05] loss=2.61 avg=2.69\n",
            "[309 | 728.31] loss=2.33 avg=2.69\n",
            "[310 | 730.56] loss=2.35 avg=2.69\n",
            "[311 | 732.82] loss=2.29 avg=2.68\n",
            "[312 | 735.08] loss=2.43 avg=2.68\n",
            "[313 | 737.34] loss=2.65 avg=2.68\n",
            "[314 | 739.60] loss=2.50 avg=2.68\n",
            "[315 | 741.87] loss=2.62 avg=2.68\n",
            "[316 | 744.13] loss=2.67 avg=2.68\n",
            "[317 | 746.40] loss=2.15 avg=2.67\n",
            "[318 | 748.66] loss=2.53 avg=2.67\n",
            "[319 | 750.91] loss=2.23 avg=2.67\n",
            "[320 | 753.17] loss=2.44 avg=2.66\n",
            "[321 | 755.42] loss=2.31 avg=2.66\n",
            "[322 | 757.69] loss=2.57 avg=2.66\n",
            "[323 | 759.94] loss=2.43 avg=2.66\n",
            "[324 | 762.19] loss=2.59 avg=2.66\n",
            "[325 | 764.45] loss=2.78 avg=2.66\n",
            "[326 | 766.71] loss=2.59 avg=2.66\n",
            "[327 | 768.96] loss=2.50 avg=2.65\n",
            "[328 | 771.23] loss=2.30 avg=2.65\n",
            "[329 | 773.48] loss=2.52 avg=2.65\n",
            "[330 | 775.74] loss=2.41 avg=2.65\n",
            "[331 | 777.99] loss=2.71 avg=2.65\n",
            "[332 | 780.26] loss=2.58 avg=2.65\n",
            "[333 | 782.52] loss=2.26 avg=2.64\n",
            "[334 | 784.77] loss=2.77 avg=2.64\n",
            "[335 | 787.03] loss=2.54 avg=2.64\n",
            "[336 | 789.29] loss=2.37 avg=2.64\n",
            "[337 | 791.55] loss=2.55 avg=2.64\n",
            "[338 | 793.81] loss=2.40 avg=2.64\n",
            "[339 | 796.07] loss=2.40 avg=2.63\n",
            "[340 | 798.33] loss=2.68 avg=2.63\n",
            "[341 | 800.58] loss=2.77 avg=2.64\n",
            "[342 | 802.84] loss=2.36 avg=2.63\n",
            "[343 | 805.10] loss=1.97 avg=2.63\n",
            "[344 | 807.35] loss=2.45 avg=2.62\n",
            "[345 | 809.61] loss=2.45 avg=2.62\n",
            "[346 | 811.86] loss=2.22 avg=2.62\n",
            "[347 | 814.12] loss=2.42 avg=2.62\n",
            "[348 | 816.38] loss=2.10 avg=2.61\n",
            "[349 | 818.63] loss=2.39 avg=2.61\n",
            "[350 | 820.90] loss=2.48 avg=2.61\n",
            "[351 | 823.15] loss=2.32 avg=2.60\n",
            "[352 | 825.41] loss=2.61 avg=2.60\n",
            "[353 | 827.66] loss=2.28 avg=2.60\n",
            "[354 | 829.92] loss=2.42 avg=2.60\n",
            "[355 | 832.18] loss=2.50 avg=2.60\n",
            "[356 | 834.43] loss=2.53 avg=2.60\n",
            "[357 | 836.68] loss=2.39 avg=2.60\n",
            "[358 | 838.94] loss=2.36 avg=2.59\n",
            "[359 | 841.20] loss=2.54 avg=2.59\n",
            "[360 | 843.46] loss=2.52 avg=2.59\n",
            "[361 | 845.72] loss=2.37 avg=2.59\n",
            "[362 | 847.98] loss=2.65 avg=2.59\n",
            "[363 | 850.24] loss=2.40 avg=2.59\n",
            "[364 | 852.50] loss=2.00 avg=2.58\n",
            "[365 | 854.76] loss=2.34 avg=2.58\n",
            "[366 | 857.02] loss=2.16 avg=2.58\n",
            "[367 | 859.27] loss=2.32 avg=2.57\n",
            "[368 | 861.53] loss=2.13 avg=2.57\n",
            "[369 | 863.79] loss=2.58 avg=2.57\n",
            "[370 | 866.05] loss=2.53 avg=2.57\n",
            "[371 | 868.31] loss=2.58 avg=2.57\n",
            "[372 | 870.57] loss=2.29 avg=2.57\n",
            "[373 | 872.83] loss=2.56 avg=2.57\n",
            "[374 | 875.09] loss=2.44 avg=2.56\n",
            "[375 | 877.34] loss=2.40 avg=2.56\n",
            "[376 | 879.60] loss=2.12 avg=2.56\n",
            "[377 | 881.86] loss=2.24 avg=2.55\n",
            "[378 | 884.12] loss=2.64 avg=2.56\n",
            "[379 | 886.37] loss=2.29 avg=2.55\n",
            "[380 | 888.64] loss=2.42 avg=2.55\n",
            "[381 | 890.90] loss=2.51 avg=2.55\n",
            "[382 | 893.16] loss=2.34 avg=2.55\n",
            "[383 | 895.42] loss=2.33 avg=2.55\n",
            "[384 | 897.67] loss=2.51 avg=2.55\n",
            "[385 | 899.92] loss=2.45 avg=2.55\n",
            "[386 | 902.18] loss=2.41 avg=2.54\n",
            "[387 | 904.44] loss=2.23 avg=2.54\n",
            "[388 | 906.69] loss=2.19 avg=2.54\n",
            "[389 | 908.95] loss=2.46 avg=2.54\n",
            "[390 | 911.21] loss=2.46 avg=2.54\n",
            "[391 | 913.48] loss=2.30 avg=2.53\n",
            "[392 | 915.74] loss=2.14 avg=2.53\n",
            "[393 | 918.00] loss=2.00 avg=2.52\n",
            "[394 | 920.26] loss=2.30 avg=2.52\n",
            "[395 | 922.51] loss=2.36 avg=2.52\n",
            "[396 | 924.78] loss=2.31 avg=2.52\n",
            "[397 | 927.04] loss=2.31 avg=2.52\n",
            "[398 | 929.29] loss=2.14 avg=2.51\n",
            "[399 | 931.55] loss=2.01 avg=2.51\n",
            "[400 | 933.81] loss=2.22 avg=2.50\n",
            "======== SAMPLE 1 ========\n",
            " for no other purpose. I have been the object of many misfortunes that are present in my\n",
            "life, but never in the life of any worthy and worthy man. My wife Teresa was my\n",
            "father’s mistress, and she had a lively desire, according to the\n",
            "tragedies of her example, to make me marry her; and she seems to have\n",
            "been content, if not a highly intelligent one, at least as well contented as\n",
            "her husband was. I feel very much moved and moved by these words of hers:\n",
            "\n",
            "“I have the greatest satisfaction, my lady, that your generosity with me\n",
            "has been beneficial, for if such be true as I believe it to be, I do not\n",
            "begrudge you assistance in any way; for I want nothing more than to give\n",
            "peace of mind to my soul in this age; for what peace or happiness may you\n",
            "desire to have, these are deeds that can only be done by you in\n",
            "good taste, and that you may strive to pursue, rather than to\n",
            "dishonour, those whom God is most pleased to make known in his ordinances;\n",
            "and besides I do not hesitate to undertake any venture that will make\n",
            "a happy end to my life; but for all that I will wait for your\n",
            "delight to provide relief in my distress, for whatever is the better\n",
            "than life in hell and in which the sufferings of the faithful will\n",
            "not give way to the happy ones; for God knows what will bestow good\n",
            "effect upon those who endure the hardships in which we are to live,\n",
            "and will bear witness that, if our good fortune allows it, this\n",
            "which is impossible will be the means which may bring about relief to those\n",
            "whose fortunes are at stake; for God knows the end of time itself, and\n",
            "the means most likely to save us is hope, when the best end should be\n",
            "nevertheless to hope and seek it, though we know nothing of\n",
            "it; for they who suffer suffer in hope, and that is the hope which\n",
            "comes to those who are in dread of it; for he who suffers is called the\n",
            "end of the world, as it has been written: for what seems as though it\n",
            "should exist now is not the end of the world; for the beginning is near and\n",
            "therefore a beginning is always requisite. And if in hope there be\n",
            "any, and if that be in hope it is the devil that has sent his death upon\n",
            "it, God, by the life he lives, has sent forth his will, and in its\n",
            "end and in its proper form they will one day be able to fulfil; for such can\n",
            "be the hopes that can hope; and God in his own mind directs the means by which\n",
            "he is to achieve it, and the means, as was the case with Don Francisco, is to\n",
            "befitting the need of such means. And so, señor, I entreat your worship to\n",
            "persevere and yield myself up and give yourself up to this danger now, and let\n",
            "us promise no other word or promise than this, that you shall be in\n",
            "peace and good health. And as these are things that will be expected to the\n",
            "exalted and the humble, which are two things for which you are grateful\n",
            "and considerate, I leave it to your worship to guide his will in the\n",
            "last second, because the end of our evil-will that he has for us in\n",
            "life is not one so far as will be to our good; for heaven’s will is\n",
            "not such that ends well with one that ends well with the will of an\n",
            "evil-minded person.”\n",
            "\n",
            "To which Sancho replied indignantly, “I give thanks for this, señor; but I\n",
            "am not to blame for not putting myself in a position to give you what you\n",
            "cannot give me myself, as I have told you already,” but to repeat to\n",
            "you, ‘A little trouble, and a little trouble is all you need,’ for to leave\n",
            "you he continued, ‘and if you will remember, Don Quixote was one of those men from\n",
            "the desert who promised never to engage in any more adventures, but only to\n",
            "have God give such a one for us in life as is just.’ “Aye aye,” he\n",
            "said, ‘that’s my story, and tell me how I was born;’ and at\n",
            "this point he made answer, “My mother died of a stomach bug some time ago;\n",
            "though, what a lot of good that’s not bad enough for a young squire.”\n",
            "\n",
            "With this and other such clarifications, and with a great deal more\n",
            "confident acknowledgment of his courtesy, he went off to beg his stay, and\n",
            "heaven forgive him in no wise.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[401 | 947.44] loss=2.51 avg=2.50\n",
            "[402 | 949.70] loss=2.44 avg=2.50\n",
            "[403 | 951.97] loss=2.39 avg=2.50\n",
            "[404 | 954.22] loss=2.45 avg=2.50\n",
            "[405 | 956.48] loss=2.09 avg=2.50\n",
            "[406 | 958.73] loss=2.52 avg=2.50\n",
            "[407 | 961.00] loss=2.37 avg=2.50\n",
            "[408 | 963.25] loss=2.23 avg=2.49\n",
            "[409 | 965.51] loss=2.27 avg=2.49\n",
            "[410 | 967.77] loss=2.23 avg=2.49\n",
            "[411 | 970.03] loss=2.61 avg=2.49\n",
            "[412 | 972.29] loss=2.32 avg=2.49\n",
            "[413 | 974.55] loss=2.32 avg=2.49\n",
            "[414 | 976.81] loss=2.11 avg=2.48\n",
            "[415 | 979.07] loss=2.30 avg=2.48\n",
            "[416 | 981.33] loss=2.17 avg=2.48\n",
            "[417 | 983.59] loss=2.11 avg=2.47\n",
            "[418 | 985.86] loss=1.98 avg=2.47\n",
            "[419 | 988.12] loss=2.46 avg=2.47\n",
            "[420 | 990.37] loss=2.48 avg=2.47\n",
            "[421 | 992.63] loss=2.25 avg=2.47\n",
            "[422 | 994.89] loss=2.39 avg=2.47\n",
            "[423 | 997.15] loss=1.69 avg=2.46\n",
            "[424 | 999.42] loss=2.69 avg=2.46\n",
            "[425 | 1001.68] loss=2.13 avg=2.46\n",
            "[426 | 1003.94] loss=2.11 avg=2.45\n",
            "[427 | 1006.21] loss=2.42 avg=2.45\n",
            "[428 | 1008.47] loss=2.22 avg=2.45\n",
            "[429 | 1010.74] loss=2.50 avg=2.45\n",
            "[430 | 1013.00] loss=2.29 avg=2.45\n",
            "[431 | 1015.27] loss=2.16 avg=2.45\n",
            "[432 | 1017.53] loss=2.42 avg=2.45\n",
            "[433 | 1019.80] loss=2.34 avg=2.44\n",
            "[434 | 1022.07] loss=2.38 avg=2.44\n",
            "[435 | 1024.32] loss=2.25 avg=2.44\n",
            "[436 | 1026.58] loss=2.66 avg=2.44\n",
            "[437 | 1028.85] loss=2.35 avg=2.44\n",
            "[438 | 1031.12] loss=2.45 avg=2.44\n",
            "[439 | 1033.39] loss=2.46 avg=2.44\n",
            "[440 | 1035.64] loss=2.41 avg=2.44\n",
            "[441 | 1037.89] loss=2.37 avg=2.44\n",
            "[442 | 1040.16] loss=2.03 avg=2.44\n",
            "[443 | 1042.43] loss=2.27 avg=2.44\n",
            "[444 | 1044.69] loss=2.39 avg=2.44\n",
            "[445 | 1046.95] loss=2.22 avg=2.43\n",
            "[446 | 1049.21] loss=2.06 avg=2.43\n",
            "[447 | 1051.46] loss=2.32 avg=2.43\n",
            "[448 | 1053.73] loss=2.30 avg=2.43\n",
            "[449 | 1055.99] loss=2.59 avg=2.43\n",
            "[450 | 1058.26] loss=2.08 avg=2.43\n",
            "[451 | 1060.52] loss=2.28 avg=2.42\n",
            "[452 | 1062.78] loss=2.11 avg=2.42\n",
            "[453 | 1065.04] loss=2.30 avg=2.42\n",
            "[454 | 1067.31] loss=2.24 avg=2.42\n",
            "[455 | 1069.57] loss=2.34 avg=2.42\n",
            "[456 | 1071.84] loss=2.23 avg=2.42\n",
            "[457 | 1074.10] loss=2.12 avg=2.41\n",
            "[458 | 1076.37] loss=2.22 avg=2.41\n",
            "[459 | 1078.63] loss=2.35 avg=2.41\n",
            "[460 | 1080.89] loss=2.02 avg=2.41\n",
            "[461 | 1083.15] loss=2.29 avg=2.41\n",
            "[462 | 1085.41] loss=2.44 avg=2.41\n",
            "[463 | 1087.67] loss=2.27 avg=2.40\n",
            "[464 | 1089.93] loss=2.60 avg=2.41\n",
            "[465 | 1092.19] loss=2.31 avg=2.40\n",
            "[466 | 1094.45] loss=2.16 avg=2.40\n",
            "[467 | 1096.71] loss=2.38 avg=2.40\n",
            "[468 | 1098.97] loss=2.02 avg=2.40\n",
            "[469 | 1101.23] loss=2.42 avg=2.40\n",
            "[470 | 1103.49] loss=2.27 avg=2.40\n",
            "[471 | 1105.74] loss=2.23 avg=2.40\n",
            "[472 | 1108.00] loss=2.32 avg=2.39\n",
            "[473 | 1110.26] loss=2.32 avg=2.39\n",
            "[474 | 1112.52] loss=2.35 avg=2.39\n",
            "[475 | 1114.78] loss=2.08 avg=2.39\n",
            "[476 | 1117.04] loss=2.14 avg=2.39\n",
            "[477 | 1119.30] loss=2.14 avg=2.39\n",
            "[478 | 1121.56] loss=2.20 avg=2.38\n",
            "[479 | 1123.82] loss=2.63 avg=2.39\n",
            "[480 | 1126.09] loss=2.26 avg=2.38\n",
            "[481 | 1128.35] loss=2.13 avg=2.38\n",
            "[482 | 1130.61] loss=2.58 avg=2.38\n",
            "[483 | 1132.87] loss=2.10 avg=2.38\n",
            "[484 | 1135.14] loss=2.34 avg=2.38\n",
            "[485 | 1137.39] loss=2.30 avg=2.38\n",
            "[486 | 1139.65] loss=2.25 avg=2.38\n",
            "[487 | 1141.92] loss=1.93 avg=2.37\n",
            "[488 | 1144.18] loss=2.40 avg=2.37\n",
            "[489 | 1146.44] loss=2.29 avg=2.37\n",
            "[490 | 1148.70] loss=2.49 avg=2.38\n",
            "[491 | 1150.96] loss=2.07 avg=2.37\n",
            "[492 | 1153.23] loss=2.52 avg=2.37\n",
            "[493 | 1155.49] loss=1.97 avg=2.37\n",
            "[494 | 1157.76] loss=2.05 avg=2.37\n",
            "[495 | 1160.01] loss=2.00 avg=2.36\n",
            "[496 | 1162.28] loss=2.08 avg=2.36\n",
            "[497 | 1164.54] loss=2.09 avg=2.36\n",
            "[498 | 1166.80] loss=2.30 avg=2.36\n",
            "[499 | 1169.06] loss=2.46 avg=2.36\n",
            "[500 | 1171.32] loss=2.17 avg=2.36\n",
            "Saving checkpoint/run1/model-500\n",
            "they arrest and take\n",
            "prisoners the same day, the same day they get out of\n",
            "the stable.”\n",
            "\n",
            "“I don’t understand that,” said Sancho.\n",
            "\n",
            "“I’ll get you straight to God,” said Don Quixote, “and I’ll go\n",
            "there myself.”\n",
            "\n",
            "“I don’t know how you know that, señor,” said Sancho.\n",
            "\n",
            "“The devil would understand what I do,” said Don Quixote, “but\n",
            "I don’t know how they are going to get to Heaven, for I don’t\n",
            "know when I will see myself there, and neither will Heaven itself,\n",
            "so I don’t know when either of us will come back;” and so saying he\n",
            "clasped the hands of Don Quixote before his eyes and, like a heifer\n",
            "on the bank of the San Lucar, delivered him down with such force\n",
            "that with a slap on the back he knocked him to the ground.\n",
            "\n",
            "The squire, however, could not bring himself to utter a word to\n",
            "Don Quixote about the man’s altercation with the keeper, and the\n",
            "master of the stable, who, he said, had told him of it and that it was\n",
            "not to be allowed to get out of hand, nor did he want any cause\n",
            "to put the keeper’s book into the stable, as he did not want any\n",
            "advantage in it; and so he said to him, “You have not been so good as\n",
            "to take away my life, master mine; let me take your life, as you\n",
            "were so valiant that you took away my life in the first place.”\n",
            "\n",
            "“I don’t understand that,” said Don Quixote.\n",
            "\n",
            "Don Quixote, who had read the letter, and who had not read the\n",
            "accounts, asked him what he meant by the name of Emperor of\n",
            "the Holies.\n",
            "\n",
            "“I don’t understand that,” replied Don Quixote.\n",
            "\n",
            "“It seems to me,” said Sancho, “that it struck him as a good name\n",
            "and likeness to be given the name of the Emperor of the Stars, so\n",
            "that he could give him more than two thousand crowns in return for the\n",
            "help he would give him in trying the trick. But I’d like to know why\n",
            "the name of the Emperor of the Stars is Holies, and not Stars.”\n",
            "\n",
            "“Ah! no!” said Don Quixote, “what starry, strange, or strange heaven\n",
            "did they have or heaven that they have not, nor heaven that cannot be\n",
            "infamous, nor any that is not magnificently named, nor any that is not\n",
            "infamous but ought to be named. Heaven has given them all, with a\n",
            "sceptre. Heaven has given names to them that are not worthy to be\n",
            "named, and names all that are not, and who can boast of having\n",
            "them; and I say that what I have read of them is not at all like\n",
            "those of the Stars, for there are a hundred that are not very\n",
            "famous, and so many that they are not in the lists of all the\n",
            "famous ones. Heaven has given them all, and I mean all that can be\n",
            "named, and names all that can be named; the only great star to\n",
            "be named is that of the Sun, which is a great star, but not a great\n",
            "star that is not very famous. Heaven has given them all, and the\n",
            "stars are not in the list of famous ones, but only of the dull\n",
            "lowest kind, which are dull ones that are few, but rather\n",
            "many; for the stars are dull and few are many, and the dull ones\n",
            "are many. Heaven has given them all, but he who is dullest is\n",
            "the dullest who are few. Heaven has given them all, but he who\n",
            "is many many many many many many many many, and he who is few\n",
            "many many many many many many many many many many many many many many many many\n",
            "many many many many many many, and so on and so forth; and Heaven’s\n",
            "good luck is in his giving names to stars because they don’t\n",
            "have names, though they are frequently named with their names, and\n",
            "they are called by a name that is very different from its proper\n",
            "meaning and proper name from which there can be no beginning, and so\n",
            "another name and a third name, and so on, and so on and so on\n",
            "until all the names are different in quality, and they all come to\n",
            "the same thing. Heaven has given\n"
          ]
        }
      ],
      "source": [
        "# This line is necessary to be able to run a new tf session if one has already been run\n",
        "tf.compat.v1.reset_default_graph()\n",
        "# Start a session\n",
        "sess = gpt2.start_tf_sess()\n",
        "# Fine tune `model_name` on `data`\n",
        "###################################\n",
        "# Swap out the `dataset` parameter with the path to your text dataset\n",
        "###################################\n",
        "gpt2.finetune(sess,\n",
        "              dataset='../data/text/DL_Lab8_Part_2.txt',\n",
        "              model_name=model_name,\n",
        "              restore_from='latest',\n",
        "              steps=500)   # steps is max number of training steps\n",
        "\n",
        "gpt2.generate(sess, run_name='run1')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}